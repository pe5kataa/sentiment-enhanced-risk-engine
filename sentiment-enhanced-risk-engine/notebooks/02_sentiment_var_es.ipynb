{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b948bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45161e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6ab868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      Price    Return\n",
      "0  2017-01-03  2257.8301       NaN\n",
      "1  2017-01-04  2270.7500  0.005706\n",
      "2  2017-01-05  2269.0000 -0.000771\n",
      "3  2017-01-06  2276.9800  0.003511\n",
      "4  2017-01-09  2268.8999 -0.003555\n",
      "        date      Price    Return  sent_mean  sent_median  sent_std  \\\n",
      "0 2017-01-03  2257.8301       NaN   0.249078     0.249078  0.000000   \n",
      "1 2017-01-04  2270.7500  0.005706   0.076594     0.052622  0.050993   \n",
      "2 2017-01-05  2269.0000 -0.000771        NaN          NaN       NaN   \n",
      "3 2017-01-06  2276.9800  0.003511        NaN          NaN       NaN   \n",
      "4 2017-01-09  2268.8999 -0.003555        NaN          NaN       NaN   \n",
      "\n",
      "   news_count  frac_neg  frac_pos  frac_neu   sent_se  \n",
      "0         1.0       0.0       1.0       0.0  0.000000  \n",
      "1         7.0       0.0       0.0       1.0  0.019274  \n",
      "2         NaN       NaN       NaN       NaN       NaN  \n",
      "3         NaN       NaN       NaN       NaN       NaN  \n",
      "4         NaN       NaN       NaN       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "sent_data = pd.read_csv(\"../data/processed/daily_sentiment_news_2017_2025.csv\")\n",
    "sp500_returns = pd.read_csv(\"../data/processed/SP500_returns.csv\")\n",
    "print(sp500_returns.head())\n",
    "\n",
    "sp500_returns[\"date\"] = pd.to_datetime(sp500_returns[\"date\"]).dt.normalize()\n",
    "\n",
    "if \"market_date\" in sent_data.columns: \n",
    "    sent_data = sent_data.rename(columns={\"market_date\": \"date\"}) \n",
    "elif \"date\" not in sent_data.columns: \n",
    "    raise ValueError(f\"Expected 'market_date' or 'date' in sent file. Found: {sent_data.columns.tolist()}\")\n",
    "\n",
    "sent_data[\"date\"] = pd.to_datetime(sent_data[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "mixed_data = sp500_returns.merge(sent_data, on=\"date\", how=\"left\").sort_values(\"date\")\n",
    "\n",
    "print(mixed_data.head())\n",
    "mixed_data.to_csv(\"../data/processed/mixed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a85463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Return</th>\n",
       "      <th>sent_mean</th>\n",
       "      <th>news_count</th>\n",
       "      <th>frac_neg</th>\n",
       "      <th>frac_pos</th>\n",
       "      <th>sent_se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.076594</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    Return  sent_mean  news_count  frac_neg  frac_pos   sent_se\n",
       "0 2017-01-03       NaN   0.249078         1.0       0.0       1.0  0.000000\n",
       "1 2017-01-04  0.005706   0.076594         7.0       0.0       0.0  0.019274\n",
       "2 2017-01-05 -0.000771   0.000000         0.0       0.0       0.0  0.000000\n",
       "3 2017-01-06  0.003511   0.000000         0.0       0.0       0.0  0.000000\n",
       "4 2017-01-09 -0.003555   0.000000         0.0       0.0       0.0  0.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_features = [\"sent_mean\", \"sent_median\", \"sent_std\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\"]\n",
    "\n",
    "# Fill missing sentiment with neutral-ish defaults\n",
    "# (so trading days without news don't become NaN and get dropped)\n",
    "# mixed_data[\"has_news\"] = (mixed_data[\"news_count\"].notna()).astype(int)\n",
    "\n",
    "mixed_data[\"news_count\"] = mixed_data[\"news_count\"].fillna(0)\n",
    "\n",
    "for value in [\"sent_mean\", \"sent_median\", \"sent_std\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\", \"sent_se\"]:\n",
    "    mixed_data[value] = mixed_data[value].fillna(0.0)\n",
    "    \n",
    "#lags = [1,2,5]\n",
    "#for col in [\"Return\", \"sent_mean\", \"sent_median\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\"]:\n",
    "#    for lag in lags:\n",
    "#        mixed_data[f\"{col}_lag{lag}\"] = mixed_data[col].shift(lag)\n",
    "\n",
    "X = mixed_data[[\"date\",\"Return\", \"sent_mean\", \"news_count\", \"frac_neg\", \"frac_pos\", \"sent_se\"]].copy()\n",
    "\n",
    "lags = []\n",
    "for col in [\"Return\", \"sent_mean\", \"news_count\", \"frac_neg\", \"sent_se\"]:\n",
    "    for lag in lags:\n",
    "        X[f\"{col}_lag{lag}\"] = X[col].shift(lag)\n",
    "   \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "950be1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return</th>\n",
       "      <th>sent_mean</th>\n",
       "      <th>news_count</th>\n",
       "      <th>frac_neg</th>\n",
       "      <th>frac_pos</th>\n",
       "      <th>sent_se</th>\n",
       "      <th>vol_30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.076594</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Return  sent_mean  news_count  frac_neg  frac_pos   sent_se  \\\n",
       "date                                                                        \n",
       "2017-01-03       NaN   0.249078         1.0       0.0       1.0  0.000000   \n",
       "2017-01-04  0.005706   0.076594         7.0       0.0       0.0  0.019274   \n",
       "2017-01-05 -0.000771   0.000000         0.0       0.0       0.0  0.000000   \n",
       "2017-01-06  0.003511   0.000000         0.0       0.0       0.0  0.000000   \n",
       "2017-01-09 -0.003555   0.000000         0.0       0.0       0.0  0.000000   \n",
       "\n",
       "            vol_30  \n",
       "date                \n",
       "2017-01-03     NaN  \n",
       "2017-01-04     NaN  \n",
       "2017-01-05     NaN  \n",
       "2017-01-06     NaN  \n",
       "2017-01-09     NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.set_index(\"date\", inplace=True)\n",
    "X[\"vol_30\"] = X[\"Return\"].rolling(20).std().shift(1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1dc4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Return  sent_mean  news_count  frac_neg  frac_pos  sent_se  \\\n",
      "date                                                                       \n",
      "2017-02-03  0.000570  -0.013828         1.0       0.0       0.0      0.0   \n",
      "2017-02-06  0.007238   0.000000         0.0       0.0       0.0      0.0   \n",
      "2017-02-07 -0.002118   0.000000         0.0       0.0       0.0      0.0   \n",
      "2017-02-08  0.000227   0.000000         0.0       0.0       0.0      0.0   \n",
      "2017-02-09  0.000693   0.000000         0.0       0.0       0.0      0.0   \n",
      "\n",
      "              vol_30  \n",
      "date                  \n",
      "2017-02-03  0.003687  \n",
      "2017-02-06  0.003477  \n",
      "2017-02-07  0.003803  \n",
      "2017-02-08  0.003786  \n",
      "2017-02-09  0.003674  \n",
      "date\n",
      "2017-02-03    0.007238\n",
      "2017-02-06   -0.002118\n",
      "2017-02-07    0.000227\n",
      "2017-02-08    0.000693\n",
      "2017-02-09    0.005736\n",
      "Name: Return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = X['Return']\n",
    "X = X.shift(1)\n",
    "\n",
    "X = X.dropna()\n",
    "y = y.reindex(X.index)\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5f3d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ngboost import NGBoost\n",
    "from ngboost.distns import T\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "\n",
    "def run_expanding_backtest_ngboost(X, y, start_test_idx, var_levels=(0.95, 0.99), es_mc_samples=3000):\n",
    "    model = NGBoost(\n",
    "        Dist=T,\n",
    "        Score=LogScore,\n",
    "        Base=DecisionTreeRegressor(max_depth=2, min_samples_leaf=50),\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=200,\n",
    "        natural_gradient=False,\n",
    "        verbose=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    rows = []\n",
    "\n",
    "    X_vals = X.values\n",
    "    y_vals = y.values\n",
    "    idx = X.index\n",
    "    n = len(X_vals)\n",
    "\n",
    "    for t in range(start_test_idx, n):\n",
    "        # Expanding window: train on everything up to t-1\n",
    "        window = 756   \n",
    "        start = max(0, t - window)\n",
    "        X_train = X.iloc[start:t].values\n",
    "        y_train = y.iloc[start:t].values \n",
    "\n",
    "        X_test = X_vals[t:t+1]\n",
    "        y_test = float(y_vals[t])\n",
    "\n",
    "        # Standardize X\n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_std = scaler_X.fit_transform(X_train)\n",
    "        X_test_std  = scaler_X.transform(X_test)\n",
    "\n",
    "        # Standardize y\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_std = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model.fit(X_train_std, y_train_std)\n",
    "        dist = model.pred_dist(X_test_std)\n",
    "\n",
    "        out = {\"Date\": idx[t], \"y_real\": y_test}\n",
    "\n",
    "        # Optional: store distribution mean/sigma in original units\n",
    "        mu_z = float(dist.mean()[0])\n",
    "        sig_z = float(np.sqrt(dist.var[0]))\n",
    "        out[\"mu_pred\"] = float(scaler_y.inverse_transform([[mu_z]])[0, 0])\n",
    "        out[\"sigma_pred\"] = float(scaler_y.scale_[0] * sig_z)\n",
    "\n",
    "        for a in var_levels:\n",
    "            q = 1 - a  # left-tail probability (e.g. 0.01 for 99% VaR)\n",
    "\n",
    "            var_z = float(np.asarray(dist.ppf(q)).reshape(-1)[0])\n",
    "\n",
    "            samples_z = dist.sample(es_mc_samples).reshape(-1)\n",
    "            es_z = float(samples_z[samples_z <= var_z].mean())\n",
    "\n",
    "            out[f\"VaR_{int(a*100)}\"] = float(scaler_y.inverse_transform([[var_z]])[0, 0])\n",
    "            out[f\"ES_{int(a*100)}\"]  = float(scaler_y.inverse_transform([[es_z]])[0, 0])\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9141a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.2893 val_loss=0.0000 scale=1.0000 norm=1.6219\n",
      "[iter 100] loss=1.1832 val_loss=0.0000 scale=0.5000 norm=0.8240\n",
      "[iter 0] loss=1.2887 val_loss=0.0000 scale=1.0000 norm=1.6230\n",
      "[iter 100] loss=1.1826 val_loss=0.0000 scale=2.0000 norm=3.2970\n",
      "[iter 0] loss=1.2893 val_loss=0.0000 scale=1.0000 norm=1.6218\n",
      "[iter 100] loss=1.1835 val_loss=0.0000 scale=1.0000 norm=1.6476\n",
      "[iter 0] loss=1.2900 val_loss=0.0000 scale=1.0000 norm=1.6205\n",
      "[iter 100] loss=1.1843 val_loss=0.0000 scale=1.0000 norm=1.6461\n",
      "[iter 0] loss=1.2900 val_loss=0.0000 scale=1.0000 norm=1.6206\n",
      "[iter 100] loss=1.1846 val_loss=0.0000 scale=1.0000 norm=1.6461\n",
      "[iter 0] loss=1.2901 val_loss=0.0000 scale=1.0000 norm=1.6202\n",
      "[iter 100] loss=1.1838 val_loss=0.0000 scale=1.0000 norm=1.6471\n",
      "[iter 0] loss=1.2902 val_loss=0.0000 scale=1.0000 norm=1.6201\n",
      "[iter 100] loss=1.1850 val_loss=0.0000 scale=1.0000 norm=1.6454\n",
      "[iter 0] loss=1.2898 val_loss=0.0000 scale=1.0000 norm=1.6209\n",
      "[iter 100] loss=1.1842 val_loss=0.0000 scale=1.0000 norm=1.6470\n",
      "[iter 0] loss=1.2900 val_loss=0.0000 scale=1.0000 norm=1.6207\n",
      "[iter 100] loss=1.1844 val_loss=0.0000 scale=1.0000 norm=1.6464\n",
      "[iter 0] loss=1.2900 val_loss=0.0000 scale=1.0000 norm=1.6208\n",
      "[iter 100] loss=1.1840 val_loss=0.0000 scale=1.0000 norm=1.6464\n",
      "[iter 0] loss=1.2904 val_loss=0.0000 scale=1.0000 norm=1.6198\n",
      "[iter 100] loss=1.1852 val_loss=0.0000 scale=1.0000 norm=1.6453\n",
      "[iter 0] loss=1.2919 val_loss=0.0000 scale=1.0000 norm=1.6175\n",
      "[iter 100] loss=1.1869 val_loss=0.0000 scale=1.0000 norm=1.6433\n",
      "[iter 0] loss=1.2874 val_loss=0.0000 scale=1.0000 norm=1.6251\n",
      "[iter 100] loss=1.1803 val_loss=0.0000 scale=1.0000 norm=1.6511\n",
      "[iter 0] loss=1.2851 val_loss=0.0000 scale=1.0000 norm=1.6298\n",
      "[iter 100] loss=1.1763 val_loss=0.0000 scale=1.0000 norm=1.6562\n",
      "[iter 0] loss=1.2856 val_loss=0.0000 scale=1.0000 norm=1.6287\n",
      "[iter 100] loss=1.1781 val_loss=0.0000 scale=1.0000 norm=1.6542\n",
      "[iter 0] loss=1.2752 val_loss=0.0000 scale=1.0000 norm=1.6432\n",
      "[iter 100] loss=1.1653 val_loss=0.0000 scale=1.0000 norm=1.6707\n",
      "[iter 0] loss=1.2766 val_loss=0.0000 scale=1.0000 norm=1.6411\n",
      "[iter 100] loss=1.1668 val_loss=0.0000 scale=1.0000 norm=1.6683\n",
      "[iter 0] loss=1.2670 val_loss=0.0000 scale=1.0000 norm=1.6549\n",
      "[iter 100] loss=1.1558 val_loss=0.0000 scale=1.0000 norm=1.6825\n",
      "[iter 0] loss=1.2649 val_loss=0.0000 scale=1.0000 norm=1.6595\n",
      "[iter 100] loss=1.1531 val_loss=0.0000 scale=1.0000 norm=1.6869\n",
      "[iter 0] loss=1.2578 val_loss=0.0000 scale=1.0000 norm=1.6711\n",
      "[iter 100] loss=1.1431 val_loss=0.0000 scale=1.0000 norm=1.7007\n",
      "[iter 0] loss=1.2552 val_loss=0.0000 scale=1.0000 norm=1.6766\n",
      "[iter 100] loss=1.1390 val_loss=0.0000 scale=1.0000 norm=1.7074\n",
      "[iter 0] loss=1.2566 val_loss=0.0000 scale=1.0000 norm=1.6750\n",
      "[iter 100] loss=1.1385 val_loss=0.0000 scale=1.0000 norm=1.7067\n",
      "[iter 0] loss=1.2185 val_loss=0.0000 scale=1.0000 norm=1.7232\n",
      "[iter 100] loss=1.0966 val_loss=0.0000 scale=1.0000 norm=1.7587\n",
      "[iter 0] loss=1.2103 val_loss=0.0000 scale=1.0000 norm=1.7369\n",
      "[iter 100] loss=1.0856 val_loss=0.0000 scale=1.0000 norm=1.7707\n",
      "[iter 0] loss=1.2016 val_loss=0.0000 scale=1.0000 norm=1.7514\n",
      "[iter 100] loss=1.0752 val_loss=0.0000 scale=1.0000 norm=1.7881\n",
      "[iter 0] loss=1.1489 val_loss=0.0000 scale=1.0000 norm=1.8218\n",
      "[iter 100] loss=1.0183 val_loss=0.0000 scale=1.0000 norm=1.8624\n",
      "[iter 0] loss=1.1150 val_loss=0.0000 scale=1.0000 norm=1.8722\n",
      "[iter 100] loss=0.9812 val_loss=0.0000 scale=1.0000 norm=1.9155\n",
      "[iter 0] loss=1.0448 val_loss=0.0000 scale=1.0000 norm=1.9759\n",
      "[iter 100] loss=0.9088 val_loss=0.0000 scale=0.5000 norm=1.0110\n",
      "[iter 0] loss=1.0379 val_loss=0.0000 scale=1.0000 norm=1.9922\n",
      "[iter 100] loss=0.8978 val_loss=0.0000 scale=1.0000 norm=2.0400\n",
      "[iter 0] loss=1.0343 val_loss=0.0000 scale=1.0000 norm=2.0029\n",
      "[iter 100] loss=0.8911 val_loss=0.0000 scale=1.0000 norm=2.0523\n",
      "[iter 0] loss=1.0345 val_loss=0.0000 scale=1.0000 norm=2.0028\n",
      "[iter 100] loss=0.8930 val_loss=0.0000 scale=0.5000 norm=1.0261\n",
      "[iter 0] loss=1.0341 val_loss=0.0000 scale=1.0000 norm=2.0074\n",
      "[iter 100] loss=0.8892 val_loss=0.0000 scale=0.5000 norm=1.0288\n",
      "[iter 0] loss=1.0345 val_loss=0.0000 scale=1.0000 norm=2.0096\n",
      "[iter 100] loss=0.8865 val_loss=0.0000 scale=1.0000 norm=2.0661\n",
      "[iter 0] loss=1.0125 val_loss=0.0000 scale=1.0000 norm=2.0505\n",
      "[iter 100] loss=0.8625 val_loss=0.0000 scale=1.0000 norm=2.1025\n",
      "[iter 0] loss=1.0141 val_loss=0.0000 scale=1.0000 norm=2.0471\n",
      "[iter 100] loss=0.8650 val_loss=0.0000 scale=1.0000 norm=2.0976\n",
      "[iter 0] loss=1.0092 val_loss=0.0000 scale=1.0000 norm=2.0604\n",
      "[iter 100] loss=0.8565 val_loss=0.0000 scale=0.5000 norm=1.0565\n",
      "[iter 0] loss=1.0116 val_loss=0.0000 scale=1.0000 norm=2.0591\n",
      "[iter 100] loss=0.8577 val_loss=0.0000 scale=1.0000 norm=2.1112\n",
      "[iter 0] loss=1.0133 val_loss=0.0000 scale=1.0000 norm=2.0586\n",
      "[iter 100] loss=0.8554 val_loss=0.0000 scale=0.5000 norm=1.0562\n",
      "[iter 0] loss=1.0161 val_loss=0.0000 scale=1.0000 norm=2.0531\n",
      "[iter 100] loss=0.8584 val_loss=0.0000 scale=1.0000 norm=2.1050\n",
      "[iter 0] loss=1.0165 val_loss=0.0000 scale=1.0000 norm=2.0559\n",
      "[iter 100] loss=0.8525 val_loss=0.0000 scale=0.5000 norm=1.0582\n",
      "[iter 0] loss=1.0191 val_loss=0.0000 scale=1.0000 norm=2.0518\n",
      "[iter 100] loss=0.8550 val_loss=0.0000 scale=0.5000 norm=1.0556\n",
      "[iter 0] loss=1.0216 val_loss=0.0000 scale=1.0000 norm=2.0469\n",
      "[iter 100] loss=0.8560 val_loss=0.0000 scale=1.0000 norm=2.1039\n",
      "[iter 0] loss=1.0147 val_loss=0.0000 scale=1.0000 norm=2.0635\n",
      "[iter 100] loss=0.8437 val_loss=0.0000 scale=0.5000 norm=1.0625\n",
      "[iter 0] loss=1.0145 val_loss=0.0000 scale=1.0000 norm=2.0638\n",
      "[iter 100] loss=0.8454 val_loss=0.0000 scale=1.0000 norm=2.1213\n",
      "[iter 0] loss=1.0170 val_loss=0.0000 scale=1.0000 norm=2.0617\n",
      "[iter 100] loss=0.8447 val_loss=0.0000 scale=1.0000 norm=2.1232\n",
      "[iter 0] loss=1.0190 val_loss=0.0000 scale=1.0000 norm=2.0577\n",
      "[iter 100] loss=0.8470 val_loss=0.0000 scale=1.0000 norm=2.1168\n",
      "[iter 0] loss=1.0208 val_loss=0.0000 scale=1.0000 norm=2.0536\n",
      "[iter 100] loss=0.8487 val_loss=0.0000 scale=0.5000 norm=1.0563\n",
      "[iter 0] loss=1.0235 val_loss=0.0000 scale=1.0000 norm=2.0500\n",
      "[iter 100] loss=0.8489 val_loss=0.0000 scale=0.5000 norm=1.0558\n",
      "[iter 0] loss=1.0261 val_loss=0.0000 scale=1.0000 norm=2.0460\n",
      "[iter 100] loss=0.8480 val_loss=0.0000 scale=1.0000 norm=2.1132\n",
      "[iter 0] loss=1.0255 val_loss=0.0000 scale=1.0000 norm=2.0478\n",
      "[iter 100] loss=0.8510 val_loss=0.0000 scale=0.5000 norm=1.0540\n",
      "[iter 0] loss=1.0274 val_loss=0.0000 scale=1.0000 norm=2.0456\n",
      "[iter 100] loss=0.8491 val_loss=0.0000 scale=0.5000 norm=1.0559\n",
      "[iter 0] loss=1.0299 val_loss=0.0000 scale=1.0000 norm=2.0404\n",
      "[iter 100] loss=0.8487 val_loss=0.0000 scale=0.5000 norm=1.0561\n",
      "[iter 0] loss=1.0327 val_loss=0.0000 scale=1.0000 norm=2.0374\n",
      "[iter 100] loss=0.8489 val_loss=0.0000 scale=1.0000 norm=2.1090\n",
      "[iter 0] loss=1.0347 val_loss=0.0000 scale=1.0000 norm=2.0339\n",
      "[iter 100] loss=0.8472 val_loss=0.0000 scale=0.5000 norm=1.0544\n",
      "[iter 0] loss=1.0344 val_loss=0.0000 scale=1.0000 norm=2.0346\n",
      "[iter 100] loss=0.8476 val_loss=0.0000 scale=0.5000 norm=1.0557\n",
      "[iter 0] loss=1.0349 val_loss=0.0000 scale=1.0000 norm=2.0333\n",
      "[iter 100] loss=0.8466 val_loss=0.0000 scale=0.5000 norm=1.0562\n",
      "[iter 0] loss=1.0365 val_loss=0.0000 scale=1.0000 norm=2.0298\n",
      "[iter 100] loss=0.8494 val_loss=0.0000 scale=0.5000 norm=1.0514\n",
      "[iter 0] loss=1.0372 val_loss=0.0000 scale=1.0000 norm=2.0285\n",
      "[iter 100] loss=0.8486 val_loss=0.0000 scale=0.5000 norm=1.0534\n",
      "[iter 0] loss=1.0400 val_loss=0.0000 scale=1.0000 norm=2.0243\n",
      "[iter 100] loss=0.8528 val_loss=0.0000 scale=1.0000 norm=2.0958\n",
      "[iter 0] loss=1.0414 val_loss=0.0000 scale=1.0000 norm=2.0209\n",
      "[iter 100] loss=0.8527 val_loss=0.0000 scale=0.5000 norm=1.0489\n",
      "[iter 0] loss=1.0444 val_loss=0.0000 scale=1.0000 norm=2.0171\n",
      "[iter 100] loss=0.8544 val_loss=0.0000 scale=1.0000 norm=2.0943\n",
      "[iter 0] loss=1.0446 val_loss=0.0000 scale=1.0000 norm=2.0167\n",
      "[iter 100] loss=0.8555 val_loss=0.0000 scale=0.5000 norm=1.0482\n",
      "[iter 0] loss=1.0455 val_loss=0.0000 scale=1.0000 norm=2.0152\n",
      "[iter 100] loss=0.8565 val_loss=0.0000 scale=0.5000 norm=1.0476\n",
      "[iter 0] loss=1.0465 val_loss=0.0000 scale=1.0000 norm=2.0128\n",
      "[iter 100] loss=0.8595 val_loss=0.0000 scale=1.0000 norm=2.0880\n",
      "[iter 0] loss=1.0478 val_loss=0.0000 scale=1.0000 norm=2.0097\n",
      "[iter 100] loss=0.8592 val_loss=0.0000 scale=1.0000 norm=2.0852\n",
      "[iter 0] loss=1.0500 val_loss=0.0000 scale=1.0000 norm=2.0053\n",
      "[iter 100] loss=0.8605 val_loss=0.0000 scale=0.5000 norm=1.0400\n",
      "[iter 0] loss=1.0499 val_loss=0.0000 scale=1.0000 norm=2.0055\n",
      "[iter 100] loss=0.8604 val_loss=0.0000 scale=0.5000 norm=1.0411\n",
      "[iter 0] loss=1.0528 val_loss=0.0000 scale=1.0000 norm=2.0003\n",
      "[iter 100] loss=0.8630 val_loss=0.0000 scale=0.5000 norm=1.0394\n",
      "[iter 0] loss=1.0553 val_loss=0.0000 scale=1.0000 norm=1.9957\n",
      "[iter 100] loss=0.8654 val_loss=0.0000 scale=1.0000 norm=2.0741\n",
      "[iter 0] loss=1.0565 val_loss=0.0000 scale=1.0000 norm=1.9929\n",
      "[iter 100] loss=0.8652 val_loss=0.0000 scale=1.0000 norm=2.0751\n",
      "[iter 0] loss=1.0564 val_loss=0.0000 scale=1.0000 norm=1.9929\n",
      "[iter 100] loss=0.8663 val_loss=0.0000 scale=1.0000 norm=2.0713\n",
      "[iter 0] loss=1.0590 val_loss=0.0000 scale=1.0000 norm=1.9899\n",
      "[iter 100] loss=0.8697 val_loss=0.0000 scale=1.0000 norm=2.0665\n",
      "[iter 0] loss=1.0580 val_loss=0.0000 scale=1.0000 norm=1.9913\n",
      "[iter 100] loss=0.8657 val_loss=0.0000 scale=1.0000 norm=2.0703\n",
      "[iter 0] loss=1.0600 val_loss=0.0000 scale=1.0000 norm=1.9870\n",
      "[iter 100] loss=0.8680 val_loss=0.0000 scale=1.0000 norm=2.0640\n",
      "[iter 0] loss=1.0606 val_loss=0.0000 scale=1.0000 norm=1.9852\n",
      "[iter 100] loss=0.8662 val_loss=0.0000 scale=1.0000 norm=2.0656\n",
      "[iter 0] loss=1.0603 val_loss=0.0000 scale=1.0000 norm=1.9857\n",
      "[iter 100] loss=0.8674 val_loss=0.0000 scale=1.0000 norm=2.0634\n",
      "[iter 0] loss=1.0618 val_loss=0.0000 scale=1.0000 norm=1.9823\n",
      "[iter 100] loss=0.8688 val_loss=0.0000 scale=0.5000 norm=1.0306\n",
      "[iter 0] loss=1.0637 val_loss=0.0000 scale=1.0000 norm=1.9786\n",
      "[iter 100] loss=0.8703 val_loss=0.0000 scale=1.0000 norm=2.0566\n",
      "[iter 0] loss=1.0637 val_loss=0.0000 scale=1.0000 norm=1.9785\n",
      "[iter 100] loss=0.8703 val_loss=0.0000 scale=1.0000 norm=2.0551\n",
      "[iter 0] loss=1.0639 val_loss=0.0000 scale=1.0000 norm=1.9783\n",
      "[iter 100] loss=0.8721 val_loss=0.0000 scale=0.5000 norm=1.0267\n",
      "[iter 0] loss=1.0639 val_loss=0.0000 scale=1.0000 norm=1.9786\n",
      "[iter 100] loss=0.8729 val_loss=0.0000 scale=0.5000 norm=1.0263\n",
      "[iter 0] loss=1.0647 val_loss=0.0000 scale=1.0000 norm=1.9771\n",
      "[iter 100] loss=0.8730 val_loss=0.0000 scale=0.5000 norm=1.0277\n",
      "[iter 0] loss=1.0657 val_loss=0.0000 scale=1.0000 norm=1.9748\n",
      "[iter 100] loss=0.8728 val_loss=0.0000 scale=0.5000 norm=1.0272\n",
      "[iter 0] loss=1.0659 val_loss=0.0000 scale=1.0000 norm=1.9744\n",
      "[iter 100] loss=0.8749 val_loss=0.0000 scale=0.5000 norm=1.0242\n",
      "[iter 0] loss=1.0685 val_loss=0.0000 scale=1.0000 norm=1.9703\n",
      "[iter 100] loss=0.8768 val_loss=0.0000 scale=0.5000 norm=1.0221\n",
      "[iter 0] loss=1.0696 val_loss=0.0000 scale=1.0000 norm=1.9677\n",
      "[iter 100] loss=0.8792 val_loss=0.0000 scale=1.0000 norm=2.0399\n",
      "[iter 0] loss=1.0708 val_loss=0.0000 scale=1.0000 norm=1.9654\n",
      "[iter 100] loss=0.8795 val_loss=0.0000 scale=0.5000 norm=1.0191\n",
      "[iter 0] loss=1.0714 val_loss=0.0000 scale=1.0000 norm=1.9640\n",
      "[iter 100] loss=0.8819 val_loss=0.0000 scale=0.5000 norm=1.0173\n",
      "[iter 0] loss=1.0681 val_loss=0.0000 scale=1.0000 norm=1.9734\n",
      "[iter 100] loss=0.8788 val_loss=0.0000 scale=0.5000 norm=1.0207\n",
      "[iter 0] loss=1.0696 val_loss=0.0000 scale=1.0000 norm=1.9702\n",
      "[iter 100] loss=0.8819 val_loss=0.0000 scale=1.0000 norm=2.0365\n",
      "[iter 0] loss=1.0702 val_loss=0.0000 scale=1.0000 norm=1.9690\n",
      "[iter 100] loss=0.8819 val_loss=0.0000 scale=0.5000 norm=1.0178\n",
      "[iter 0] loss=1.0724 val_loss=0.0000 scale=1.0000 norm=1.9648\n",
      "[iter 100] loss=0.8832 val_loss=0.0000 scale=1.0000 norm=2.0317\n",
      "[iter 0] loss=1.0725 val_loss=0.0000 scale=1.0000 norm=1.9643\n",
      "[iter 100] loss=0.8837 val_loss=0.0000 scale=1.0000 norm=2.0319\n",
      "[iter 0] loss=1.0725 val_loss=0.0000 scale=1.0000 norm=1.9644\n",
      "[iter 100] loss=0.8858 val_loss=0.0000 scale=1.0000 norm=2.0297\n",
      "[iter 0] loss=1.0725 val_loss=0.0000 scale=1.0000 norm=1.9643\n",
      "[iter 100] loss=0.8845 val_loss=0.0000 scale=0.5000 norm=1.0156\n",
      "[iter 0] loss=1.0721 val_loss=0.0000 scale=1.0000 norm=1.9656\n",
      "[iter 100] loss=0.8839 val_loss=0.0000 scale=1.0000 norm=2.0321\n",
      "[iter 0] loss=1.0722 val_loss=0.0000 scale=1.0000 norm=1.9657\n",
      "[iter 100] loss=0.8823 val_loss=0.0000 scale=0.5000 norm=1.0172\n",
      "[iter 0] loss=1.0750 val_loss=0.0000 scale=1.0000 norm=1.9616\n",
      "[iter 100] loss=0.8864 val_loss=0.0000 scale=1.0000 norm=2.0291\n",
      "[iter 0] loss=1.0762 val_loss=0.0000 scale=1.0000 norm=1.9592\n",
      "[iter 100] loss=0.8903 val_loss=0.0000 scale=0.5000 norm=1.0129\n",
      "[iter 0] loss=1.0790 val_loss=0.0000 scale=1.0000 norm=1.9548\n",
      "[iter 100] loss=0.8944 val_loss=0.0000 scale=1.0000 norm=2.0205\n",
      "[iter 0] loss=1.0796 val_loss=0.0000 scale=1.0000 norm=1.9536\n",
      "[iter 100] loss=0.8925 val_loss=0.0000 scale=1.0000 norm=2.0218\n",
      "[iter 0] loss=1.0806 val_loss=0.0000 scale=1.0000 norm=1.9514\n",
      "[iter 100] loss=0.8913 val_loss=0.0000 scale=0.5000 norm=1.0121\n",
      "[iter 0] loss=1.0796 val_loss=0.0000 scale=1.0000 norm=1.9539\n",
      "[iter 100] loss=0.8911 val_loss=0.0000 scale=0.5000 norm=1.0125\n",
      "[iter 0] loss=1.0798 val_loss=0.0000 scale=1.0000 norm=1.9537\n",
      "[iter 100] loss=0.8928 val_loss=0.0000 scale=0.5000 norm=1.0106\n",
      "[iter 0] loss=1.0817 val_loss=0.0000 scale=1.0000 norm=1.9501\n",
      "[iter 100] loss=0.8941 val_loss=0.0000 scale=0.5000 norm=1.0083\n",
      "[iter 0] loss=1.0833 val_loss=0.0000 scale=1.0000 norm=1.9467\n",
      "[iter 100] loss=0.8940 val_loss=0.0000 scale=0.5000 norm=1.0080\n",
      "[iter 0] loss=1.0825 val_loss=0.0000 scale=1.0000 norm=1.9485\n",
      "[iter 100] loss=0.8914 val_loss=0.0000 scale=0.5000 norm=1.0117\n",
      "[iter 0] loss=1.0829 val_loss=0.0000 scale=1.0000 norm=1.9477\n",
      "[iter 100] loss=0.8923 val_loss=0.0000 scale=1.0000 norm=2.0197\n",
      "[iter 0] loss=1.0839 val_loss=0.0000 scale=1.0000 norm=1.9455\n",
      "[iter 100] loss=0.8917 val_loss=0.0000 scale=0.5000 norm=1.0103\n",
      "[iter 0] loss=1.0852 val_loss=0.0000 scale=1.0000 norm=1.9426\n",
      "[iter 100] loss=0.8943 val_loss=0.0000 scale=0.5000 norm=1.0073\n",
      "[iter 0] loss=1.0862 val_loss=0.0000 scale=1.0000 norm=1.9407\n",
      "[iter 100] loss=0.8951 val_loss=0.0000 scale=1.0000 norm=2.0102\n",
      "[iter 0] loss=1.0870 val_loss=0.0000 scale=1.0000 norm=1.9390\n",
      "[iter 100] loss=0.8959 val_loss=0.0000 scale=0.5000 norm=1.0048\n",
      "[iter 0] loss=1.0871 val_loss=0.0000 scale=1.0000 norm=1.9387\n",
      "[iter 100] loss=0.8962 val_loss=0.0000 scale=1.0000 norm=2.0079\n",
      "[iter 0] loss=1.0872 val_loss=0.0000 scale=1.0000 norm=1.9386\n",
      "[iter 100] loss=0.8971 val_loss=0.0000 scale=0.5000 norm=1.0038\n",
      "[iter 0] loss=1.0878 val_loss=0.0000 scale=1.0000 norm=1.9375\n",
      "[iter 100] loss=0.8984 val_loss=0.0000 scale=0.5000 norm=1.0039\n",
      "[iter 0] loss=1.0876 val_loss=0.0000 scale=1.0000 norm=1.9378\n",
      "[iter 100] loss=0.8987 val_loss=0.0000 scale=0.5000 norm=1.0026\n",
      "[iter 0] loss=1.0879 val_loss=0.0000 scale=1.0000 norm=1.9373\n",
      "[iter 100] loss=0.8983 val_loss=0.0000 scale=1.0000 norm=2.0075\n",
      "[iter 0] loss=1.0896 val_loss=0.0000 scale=1.0000 norm=1.9337\n",
      "[iter 100] loss=0.9012 val_loss=0.0000 scale=1.0000 norm=2.0024\n",
      "[iter 0] loss=1.0903 val_loss=0.0000 scale=1.0000 norm=1.9320\n",
      "[iter 100] loss=0.9039 val_loss=0.0000 scale=1.0000 norm=1.9969\n",
      "[iter 0] loss=1.0908 val_loss=0.0000 scale=1.0000 norm=1.9313\n",
      "[iter 100] loss=0.9043 val_loss=0.0000 scale=0.5000 norm=0.9971\n",
      "[iter 0] loss=1.0916 val_loss=0.0000 scale=1.0000 norm=1.9296\n",
      "[iter 100] loss=0.9044 val_loss=0.0000 scale=1.0000 norm=1.9968\n",
      "[iter 0] loss=1.0929 val_loss=0.0000 scale=1.0000 norm=1.9271\n",
      "[iter 100] loss=0.9064 val_loss=0.0000 scale=0.5000 norm=0.9948\n",
      "[iter 0] loss=1.0932 val_loss=0.0000 scale=1.0000 norm=1.9266\n",
      "[iter 100] loss=0.9067 val_loss=0.0000 scale=1.0000 norm=1.9916\n",
      "[iter 0] loss=1.0936 val_loss=0.0000 scale=1.0000 norm=1.9259\n",
      "[iter 100] loss=0.9068 val_loss=0.0000 scale=1.0000 norm=1.9916\n",
      "[iter 0] loss=1.0941 val_loss=0.0000 scale=1.0000 norm=1.9251\n",
      "[iter 100] loss=0.9074 val_loss=0.0000 scale=1.0000 norm=1.9927\n",
      "[iter 0] loss=1.0942 val_loss=0.0000 scale=1.0000 norm=1.9252\n",
      "[iter 100] loss=0.9081 val_loss=0.0000 scale=1.0000 norm=1.9925\n",
      "[iter 0] loss=1.0944 val_loss=0.0000 scale=1.0000 norm=1.9247\n",
      "[iter 100] loss=0.9088 val_loss=0.0000 scale=0.5000 norm=0.9957\n",
      "[iter 0] loss=1.0948 val_loss=0.0000 scale=1.0000 norm=1.9242\n",
      "[iter 100] loss=0.9088 val_loss=0.0000 scale=1.0000 norm=1.9928\n",
      "[iter 0] loss=1.0948 val_loss=0.0000 scale=1.0000 norm=1.9241\n",
      "[iter 100] loss=0.9102 val_loss=0.0000 scale=1.0000 norm=1.9914\n",
      "[iter 0] loss=1.0946 val_loss=0.0000 scale=1.0000 norm=1.9247\n",
      "[iter 100] loss=0.9103 val_loss=0.0000 scale=0.5000 norm=0.9955\n",
      "[iter 0] loss=1.0956 val_loss=0.0000 scale=1.0000 norm=1.9224\n",
      "[iter 100] loss=0.9134 val_loss=0.0000 scale=1.0000 norm=1.9858\n",
      "[iter 0] loss=1.0950 val_loss=0.0000 scale=1.0000 norm=1.9236\n",
      "[iter 100] loss=0.9096 val_loss=0.0000 scale=1.0000 norm=1.9918\n",
      "[iter 0] loss=1.0952 val_loss=0.0000 scale=1.0000 norm=1.9233\n",
      "[iter 100] loss=0.9078 val_loss=0.0000 scale=1.0000 norm=1.9953\n",
      "[iter 0] loss=1.0943 val_loss=0.0000 scale=1.0000 norm=1.9249\n",
      "[iter 100] loss=0.9057 val_loss=0.0000 scale=1.0000 norm=1.9977\n",
      "[iter 0] loss=1.0943 val_loss=0.0000 scale=1.0000 norm=1.9251\n",
      "[iter 100] loss=0.9047 val_loss=0.0000 scale=1.0000 norm=1.9996\n",
      "[iter 0] loss=1.0943 val_loss=0.0000 scale=1.0000 norm=1.9251\n",
      "[iter 100] loss=0.9060 val_loss=0.0000 scale=1.0000 norm=1.9974\n",
      "[iter 0] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=1.9282\n",
      "[iter 100] loss=0.9023 val_loss=0.0000 scale=0.5000 norm=1.0025\n",
      "[iter 0] loss=1.0925 val_loss=0.0000 scale=1.0000 norm=1.9287\n",
      "[iter 100] loss=0.9029 val_loss=0.0000 scale=1.0000 norm=2.0019\n",
      "[iter 0] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=1.9287\n",
      "[iter 100] loss=0.9031 val_loss=0.0000 scale=1.0000 norm=2.0030\n",
      "[iter 0] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=1.9287\n",
      "[iter 100] loss=0.9008 val_loss=0.0000 scale=1.0000 norm=2.0083\n",
      "[iter 0] loss=1.0923 val_loss=0.0000 scale=1.0000 norm=1.9292\n",
      "[iter 100] loss=0.9004 val_loss=0.0000 scale=1.0000 norm=2.0102\n",
      "[iter 0] loss=1.0931 val_loss=0.0000 scale=1.0000 norm=1.9276\n",
      "[iter 100] loss=0.9023 val_loss=0.0000 scale=0.5000 norm=1.0051\n",
      "[iter 0] loss=1.0931 val_loss=0.0000 scale=1.0000 norm=1.9276\n",
      "[iter 100] loss=0.9032 val_loss=0.0000 scale=1.0000 norm=2.0077\n",
      "[iter 0] loss=1.0934 val_loss=0.0000 scale=1.0000 norm=1.9271\n",
      "[iter 100] loss=0.9037 val_loss=0.0000 scale=1.0000 norm=2.0095\n",
      "[iter 0] loss=1.0937 val_loss=0.0000 scale=1.0000 norm=1.9270\n",
      "[iter 100] loss=0.9051 val_loss=0.0000 scale=1.0000 norm=2.0065\n",
      "[iter 0] loss=1.0940 val_loss=0.0000 scale=1.0000 norm=1.9262\n",
      "[iter 100] loss=0.9051 val_loss=0.0000 scale=0.5000 norm=1.0039\n",
      "[iter 0] loss=1.0954 val_loss=0.0000 scale=1.0000 norm=1.9229\n",
      "[iter 100] loss=0.9090 val_loss=0.0000 scale=0.5000 norm=1.0001\n",
      "[iter 0] loss=1.0976 val_loss=0.0000 scale=1.0000 norm=1.9214\n",
      "[iter 100] loss=0.9134 val_loss=0.0000 scale=0.5000 norm=0.9989\n",
      "[iter 0] loss=1.0977 val_loss=0.0000 scale=1.0000 norm=1.9212\n",
      "[iter 100] loss=0.9125 val_loss=0.0000 scale=1.0000 norm=1.9990\n",
      "[iter 0] loss=1.1004 val_loss=0.0000 scale=1.0000 norm=1.9175\n",
      "[iter 100] loss=0.9157 val_loss=0.0000 scale=0.5000 norm=0.9968\n",
      "[iter 0] loss=1.1025 val_loss=0.0000 scale=1.0000 norm=1.9134\n",
      "[iter 100] loss=0.9169 val_loss=0.0000 scale=1.0000 norm=1.9900\n",
      "[iter 0] loss=1.1047 val_loss=0.0000 scale=1.0000 norm=1.9094\n",
      "[iter 100] loss=0.9203 val_loss=0.0000 scale=1.0000 norm=1.9820\n",
      "[iter 0] loss=1.1037 val_loss=0.0000 scale=1.0000 norm=1.9111\n",
      "[iter 100] loss=0.9199 val_loss=0.0000 scale=1.0000 norm=1.9828\n",
      "[iter 0] loss=1.1050 val_loss=0.0000 scale=1.0000 norm=1.9086\n",
      "[iter 100] loss=0.9204 val_loss=0.0000 scale=1.0000 norm=1.9819\n",
      "[iter 0] loss=1.1052 val_loss=0.0000 scale=1.0000 norm=1.9084\n",
      "[iter 100] loss=0.9213 val_loss=0.0000 scale=1.0000 norm=1.9826\n",
      "[iter 0] loss=1.1056 val_loss=0.0000 scale=1.0000 norm=1.9077\n",
      "[iter 100] loss=0.9224 val_loss=0.0000 scale=1.0000 norm=1.9804\n",
      "[iter 0] loss=1.1067 val_loss=0.0000 scale=1.0000 norm=1.9055\n",
      "[iter 100] loss=0.9232 val_loss=0.0000 scale=1.0000 norm=1.9816\n",
      "[iter 0] loss=1.1083 val_loss=0.0000 scale=1.0000 norm=1.9020\n",
      "[iter 100] loss=0.9231 val_loss=0.0000 scale=0.5000 norm=0.9915\n",
      "[iter 0] loss=1.1099 val_loss=0.0000 scale=1.0000 norm=1.8988\n",
      "[iter 100] loss=0.9281 val_loss=0.0000 scale=0.5000 norm=0.9860\n",
      "[iter 0] loss=1.1108 val_loss=0.0000 scale=1.0000 norm=1.8971\n",
      "[iter 100] loss=0.9298 val_loss=0.0000 scale=1.0000 norm=1.9708\n",
      "[iter 0] loss=1.1131 val_loss=0.0000 scale=1.0000 norm=1.8933\n",
      "[iter 100] loss=0.9315 val_loss=0.0000 scale=1.0000 norm=1.9688\n",
      "[iter 0] loss=1.1132 val_loss=0.0000 scale=1.0000 norm=1.8935\n",
      "[iter 100] loss=0.9339 val_loss=0.0000 scale=1.0000 norm=1.9666\n",
      "[iter 0] loss=1.1146 val_loss=0.0000 scale=1.0000 norm=1.8905\n",
      "[iter 100] loss=0.9359 val_loss=0.0000 scale=1.0000 norm=1.9605\n",
      "[iter 0] loss=1.1163 val_loss=0.0000 scale=1.0000 norm=1.8873\n",
      "[iter 100] loss=0.9362 val_loss=0.0000 scale=0.5000 norm=0.9814\n",
      "[iter 0] loss=1.1167 val_loss=0.0000 scale=1.0000 norm=1.8864\n",
      "[iter 100] loss=0.9382 val_loss=0.0000 scale=1.0000 norm=1.9583\n",
      "[iter 0] loss=1.1173 val_loss=0.0000 scale=1.0000 norm=1.8853\n",
      "[iter 100] loss=0.9390 val_loss=0.0000 scale=0.5000 norm=0.9789\n",
      "[iter 0] loss=1.1174 val_loss=0.0000 scale=1.0000 norm=1.8851\n",
      "[iter 100] loss=0.9394 val_loss=0.0000 scale=0.5000 norm=0.9783\n",
      "[iter 0] loss=1.1186 val_loss=0.0000 scale=1.0000 norm=1.8825\n",
      "[iter 100] loss=0.9417 val_loss=0.0000 scale=0.5000 norm=0.9766\n",
      "[iter 0] loss=1.1204 val_loss=0.0000 scale=1.0000 norm=1.8791\n",
      "[iter 100] loss=0.9450 val_loss=0.0000 scale=1.0000 norm=1.9447\n",
      "[iter 0] loss=1.1223 val_loss=0.0000 scale=1.0000 norm=1.8755\n",
      "[iter 100] loss=0.9446 val_loss=0.0000 scale=1.0000 norm=1.9471\n",
      "[iter 0] loss=1.1238 val_loss=0.0000 scale=1.0000 norm=1.8723\n",
      "[iter 100] loss=0.9459 val_loss=0.0000 scale=1.0000 norm=1.9418\n",
      "[iter 0] loss=1.1242 val_loss=0.0000 scale=1.0000 norm=1.8716\n",
      "[iter 100] loss=0.9472 val_loss=0.0000 scale=0.5000 norm=0.9711\n",
      "[iter 0] loss=1.1247 val_loss=0.0000 scale=1.0000 norm=1.8708\n",
      "[iter 100] loss=0.9477 val_loss=0.0000 scale=1.0000 norm=1.9416\n",
      "[iter 0] loss=1.1263 val_loss=0.0000 scale=1.0000 norm=1.8677\n",
      "[iter 100] loss=0.9493 val_loss=0.0000 scale=0.5000 norm=0.9698\n",
      "[iter 0] loss=1.1271 val_loss=0.0000 scale=1.0000 norm=1.8661\n",
      "[iter 100] loss=0.9505 val_loss=0.0000 scale=1.0000 norm=1.9394\n",
      "[iter 0] loss=1.1277 val_loss=0.0000 scale=1.0000 norm=1.8648\n",
      "[iter 100] loss=0.9537 val_loss=0.0000 scale=1.0000 norm=1.9339\n",
      "[iter 0] loss=1.1278 val_loss=0.0000 scale=1.0000 norm=1.8645\n",
      "[iter 100] loss=0.9546 val_loss=0.0000 scale=1.0000 norm=1.9350\n",
      "[iter 0] loss=1.1278 val_loss=0.0000 scale=1.0000 norm=1.8646\n",
      "[iter 100] loss=0.9551 val_loss=0.0000 scale=1.0000 norm=1.9359\n",
      "[iter 0] loss=1.1299 val_loss=0.0000 scale=1.0000 norm=1.8608\n",
      "[iter 100] loss=0.9581 val_loss=0.0000 scale=1.0000 norm=1.9318\n",
      "[iter 0] loss=1.1301 val_loss=0.0000 scale=1.0000 norm=1.8606\n",
      "[iter 100] loss=0.9591 val_loss=0.0000 scale=1.0000 norm=1.9344\n",
      "[iter 0] loss=1.1302 val_loss=0.0000 scale=1.0000 norm=1.8605\n",
      "[iter 100] loss=0.9606 val_loss=0.0000 scale=1.0000 norm=1.9326\n",
      "[iter 0] loss=1.1302 val_loss=0.0000 scale=1.0000 norm=1.8605\n",
      "[iter 100] loss=0.9599 val_loss=0.0000 scale=0.5000 norm=0.9669\n",
      "[iter 0] loss=1.1299 val_loss=0.0000 scale=1.0000 norm=1.8609\n",
      "[iter 100] loss=0.9616 val_loss=0.0000 scale=0.5000 norm=0.9647\n",
      "[iter 0] loss=1.1322 val_loss=0.0000 scale=1.0000 norm=1.8569\n",
      "[iter 100] loss=0.9637 val_loss=0.0000 scale=0.5000 norm=0.9639\n",
      "[iter 0] loss=1.1320 val_loss=0.0000 scale=1.0000 norm=1.8573\n",
      "[iter 100] loss=0.9634 val_loss=0.0000 scale=1.0000 norm=1.9297\n",
      "[iter 0] loss=1.1340 val_loss=0.0000 scale=1.0000 norm=1.8559\n",
      "[iter 100] loss=0.9657 val_loss=0.0000 scale=1.0000 norm=1.9303\n",
      "[iter 0] loss=1.1345 val_loss=0.0000 scale=1.0000 norm=1.8546\n",
      "[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=1.9272\n",
      "[iter 0] loss=1.1358 val_loss=0.0000 scale=1.0000 norm=1.8519\n",
      "[iter 100] loss=0.9668 val_loss=0.0000 scale=1.0000 norm=1.9229\n",
      "[iter 0] loss=1.1369 val_loss=0.0000 scale=1.0000 norm=1.8499\n",
      "[iter 100] loss=0.9687 val_loss=0.0000 scale=0.5000 norm=0.9603\n",
      "[iter 0] loss=1.1386 val_loss=0.0000 scale=1.0000 norm=1.8468\n",
      "[iter 100] loss=0.9716 val_loss=0.0000 scale=0.5000 norm=0.9583\n",
      "[iter 0] loss=1.1406 val_loss=0.0000 scale=1.0000 norm=1.8433\n",
      "[iter 100] loss=0.9729 val_loss=0.0000 scale=1.0000 norm=1.9160\n",
      "[iter 0] loss=1.1424 val_loss=0.0000 scale=1.0000 norm=1.8400\n",
      "[iter 100] loss=0.9748 val_loss=0.0000 scale=1.0000 norm=1.9145\n",
      "[iter 0] loss=1.1424 val_loss=0.0000 scale=1.0000 norm=1.8399\n",
      "[iter 100] loss=0.9760 val_loss=0.0000 scale=1.0000 norm=1.9132\n",
      "[iter 0] loss=1.1433 val_loss=0.0000 scale=1.0000 norm=1.8383\n",
      "[iter 100] loss=0.9770 val_loss=0.0000 scale=0.5000 norm=0.9549\n",
      "[iter 0] loss=1.1434 val_loss=0.0000 scale=1.0000 norm=1.8380\n",
      "[iter 100] loss=0.9775 val_loss=0.0000 scale=1.0000 norm=1.9117\n",
      "[iter 0] loss=1.1435 val_loss=0.0000 scale=1.0000 norm=1.8378\n",
      "[iter 100] loss=0.9774 val_loss=0.0000 scale=1.0000 norm=1.9123\n",
      "[iter 0] loss=1.1447 val_loss=0.0000 scale=1.0000 norm=1.8355\n",
      "[iter 100] loss=0.9800 val_loss=0.0000 scale=1.0000 norm=1.9072\n",
      "[iter 0] loss=1.1458 val_loss=0.0000 scale=1.0000 norm=1.8333\n",
      "[iter 100] loss=0.9823 val_loss=0.0000 scale=1.0000 norm=1.9033\n",
      "[iter 0] loss=1.1466 val_loss=0.0000 scale=1.0000 norm=1.8316\n",
      "[iter 100] loss=0.9826 val_loss=0.0000 scale=1.0000 norm=1.9012\n",
      "[iter 0] loss=1.1465 val_loss=0.0000 scale=1.0000 norm=1.8319\n",
      "[iter 100] loss=0.9831 val_loss=0.0000 scale=1.0000 norm=1.9021\n",
      "[iter 0] loss=1.1474 val_loss=0.0000 scale=1.0000 norm=1.8298\n",
      "[iter 100] loss=0.9841 val_loss=0.0000 scale=1.0000 norm=1.8996\n",
      "[iter 0] loss=1.1473 val_loss=0.0000 scale=1.0000 norm=1.8304\n",
      "[iter 100] loss=0.9845 val_loss=0.0000 scale=0.5000 norm=0.9506\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8287\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=0.5000 norm=0.9491\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8290\n",
      "[iter 100] loss=0.9857 val_loss=0.0000 scale=0.5000 norm=0.9496\n",
      "[iter 0] loss=1.1494 val_loss=0.0000 scale=1.0000 norm=1.8263\n",
      "[iter 100] loss=0.9860 val_loss=0.0000 scale=0.5000 norm=0.9484\n",
      "[iter 0] loss=1.1495 val_loss=0.0000 scale=1.0000 norm=1.8261\n",
      "[iter 100] loss=0.9875 val_loss=0.0000 scale=1.0000 norm=1.8952\n",
      "[iter 0] loss=1.1494 val_loss=0.0000 scale=1.0000 norm=1.8261\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=1.0000 norm=1.8933\n",
      "[iter 0] loss=1.1492 val_loss=0.0000 scale=1.0000 norm=1.8266\n",
      "[iter 100] loss=0.9876 val_loss=0.0000 scale=1.0000 norm=1.8960\n",
      "[iter 0] loss=1.1500 val_loss=0.0000 scale=1.0000 norm=1.8250\n",
      "[iter 100] loss=0.9895 val_loss=0.0000 scale=0.5000 norm=0.9459\n",
      "[iter 0] loss=1.1495 val_loss=0.0000 scale=1.0000 norm=1.8259\n",
      "[iter 100] loss=0.9883 val_loss=0.0000 scale=0.5000 norm=0.9472\n",
      "[iter 0] loss=1.1494 val_loss=0.0000 scale=1.0000 norm=1.8260\n",
      "[iter 100] loss=0.9884 val_loss=0.0000 scale=1.0000 norm=1.8934\n",
      "[iter 0] loss=1.1499 val_loss=0.0000 scale=1.0000 norm=1.8252\n",
      "[iter 100] loss=0.9898 val_loss=0.0000 scale=1.0000 norm=1.8911\n",
      "[iter 0] loss=1.1497 val_loss=0.0000 scale=1.0000 norm=1.8255\n",
      "[iter 100] loss=0.9906 val_loss=0.0000 scale=1.0000 norm=1.8876\n",
      "[iter 0] loss=1.1497 val_loss=0.0000 scale=1.0000 norm=1.8254\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=0.5000 norm=0.9457\n",
      "[iter 0] loss=1.1506 val_loss=0.0000 scale=1.0000 norm=1.8237\n",
      "[iter 100] loss=0.9919 val_loss=0.0000 scale=1.0000 norm=1.8858\n",
      "[iter 0] loss=1.1505 val_loss=0.0000 scale=1.0000 norm=1.8237\n",
      "[iter 100] loss=0.9918 val_loss=0.0000 scale=0.5000 norm=0.9435\n",
      "[iter 0] loss=1.1506 val_loss=0.0000 scale=1.0000 norm=1.8236\n",
      "[iter 100] loss=0.9924 val_loss=0.0000 scale=1.0000 norm=1.8858\n",
      "[iter 0] loss=1.1510 val_loss=0.0000 scale=1.0000 norm=1.8229\n",
      "[iter 100] loss=0.9932 val_loss=0.0000 scale=1.0000 norm=1.8850\n",
      "[iter 0] loss=1.1520 val_loss=0.0000 scale=1.0000 norm=1.8209\n",
      "[iter 100] loss=0.9949 val_loss=0.0000 scale=0.5000 norm=0.9422\n",
      "[iter 0] loss=1.1516 val_loss=0.0000 scale=1.0000 norm=1.8215\n",
      "[iter 100] loss=0.9929 val_loss=0.0000 scale=1.0000 norm=1.8889\n",
      "[iter 0] loss=1.1513 val_loss=0.0000 scale=1.0000 norm=1.8222\n",
      "[iter 100] loss=0.9939 val_loss=0.0000 scale=1.0000 norm=1.8837\n",
      "[iter 0] loss=1.1514 val_loss=0.0000 scale=1.0000 norm=1.8221\n",
      "[iter 100] loss=0.9925 val_loss=0.0000 scale=0.5000 norm=0.9426\n",
      "[iter 0] loss=1.1515 val_loss=0.0000 scale=1.0000 norm=1.8219\n",
      "[iter 100] loss=0.9938 val_loss=0.0000 scale=1.0000 norm=1.8831\n",
      "[iter 0] loss=1.1516 val_loss=0.0000 scale=1.0000 norm=1.8216\n",
      "[iter 100] loss=0.9938 val_loss=0.0000 scale=1.0000 norm=1.8852\n",
      "[iter 0] loss=1.1516 val_loss=0.0000 scale=1.0000 norm=1.8216\n",
      "[iter 100] loss=0.9943 val_loss=0.0000 scale=0.5000 norm=0.9416\n",
      "[iter 0] loss=1.1516 val_loss=0.0000 scale=1.0000 norm=1.8217\n",
      "[iter 100] loss=0.9950 val_loss=0.0000 scale=1.0000 norm=1.8818\n",
      "[iter 0] loss=1.1520 val_loss=0.0000 scale=1.0000 norm=1.8209\n",
      "[iter 100] loss=0.9954 val_loss=0.0000 scale=1.0000 norm=1.8854\n",
      "[iter 0] loss=1.1522 val_loss=0.0000 scale=1.0000 norm=1.8207\n",
      "[iter 100] loss=0.9948 val_loss=0.0000 scale=1.0000 norm=1.8878\n",
      "[iter 0] loss=1.1522 val_loss=0.0000 scale=1.0000 norm=1.8208\n",
      "[iter 100] loss=0.9953 val_loss=0.0000 scale=1.0000 norm=1.8864\n",
      "[iter 0] loss=1.1520 val_loss=0.0000 scale=1.0000 norm=1.8214\n",
      "[iter 100] loss=0.9946 val_loss=0.0000 scale=1.0000 norm=1.8886\n",
      "[iter 0] loss=1.1533 val_loss=0.0000 scale=1.0000 norm=1.8186\n",
      "[iter 100] loss=0.9967 val_loss=0.0000 scale=1.0000 norm=1.8855\n",
      "[iter 0] loss=1.1533 val_loss=0.0000 scale=1.0000 norm=1.8185\n",
      "[iter 100] loss=0.9975 val_loss=0.0000 scale=1.0000 norm=1.8844\n",
      "[iter 0] loss=1.1535 val_loss=0.0000 scale=1.0000 norm=1.8181\n",
      "[iter 100] loss=0.9976 val_loss=0.0000 scale=1.0000 norm=1.8839\n",
      "[iter 0] loss=1.1544 val_loss=0.0000 scale=1.0000 norm=1.8164\n",
      "[iter 100] loss=0.9991 val_loss=0.0000 scale=0.5000 norm=0.9407\n",
      "[iter 0] loss=1.1546 val_loss=0.0000 scale=1.0000 norm=1.8161\n",
      "[iter 100] loss=1.0002 val_loss=0.0000 scale=0.5000 norm=0.9402\n",
      "[iter 0] loss=1.1553 val_loss=0.0000 scale=1.0000 norm=1.8148\n",
      "[iter 100] loss=1.0014 val_loss=0.0000 scale=0.5000 norm=0.9400\n",
      "[iter 0] loss=1.1552 val_loss=0.0000 scale=1.0000 norm=1.8150\n",
      "[iter 100] loss=1.0014 val_loss=0.0000 scale=1.0000 norm=1.8810\n",
      "[iter 0] loss=1.1549 val_loss=0.0000 scale=1.0000 norm=1.8155\n",
      "[iter 100] loss=1.0005 val_loss=0.0000 scale=1.0000 norm=1.8810\n",
      "[iter 0] loss=1.1549 val_loss=0.0000 scale=1.0000 norm=1.8153\n",
      "[iter 100] loss=1.0006 val_loss=0.0000 scale=1.0000 norm=1.8803\n",
      "[iter 0] loss=1.1554 val_loss=0.0000 scale=1.0000 norm=1.8143\n",
      "[iter 100] loss=1.0021 val_loss=0.0000 scale=0.5000 norm=0.9391\n",
      "[iter 0] loss=1.1552 val_loss=0.0000 scale=1.0000 norm=1.8146\n",
      "[iter 100] loss=1.0010 val_loss=0.0000 scale=0.5000 norm=0.9389\n",
      "[iter 0] loss=1.1563 val_loss=0.0000 scale=1.0000 norm=1.8126\n",
      "[iter 100] loss=1.0040 val_loss=0.0000 scale=1.0000 norm=1.8723\n",
      "[iter 0] loss=1.1562 val_loss=0.0000 scale=1.0000 norm=1.8125\n",
      "[iter 100] loss=1.0037 val_loss=0.0000 scale=1.0000 norm=1.8721\n",
      "[iter 0] loss=1.1560 val_loss=0.0000 scale=1.0000 norm=1.8127\n",
      "[iter 100] loss=1.0022 val_loss=0.0000 scale=1.0000 norm=1.8747\n",
      "[iter 0] loss=1.1561 val_loss=0.0000 scale=1.0000 norm=1.8127\n",
      "[iter 100] loss=1.0016 val_loss=0.0000 scale=0.5000 norm=0.9379\n",
      "[iter 0] loss=1.1561 val_loss=0.0000 scale=1.0000 norm=1.8128\n",
      "[iter 100] loss=1.0024 val_loss=0.0000 scale=0.5000 norm=0.9372\n",
      "[iter 0] loss=1.1584 val_loss=0.0000 scale=1.0000 norm=1.8092\n",
      "[iter 100] loss=1.0061 val_loss=0.0000 scale=0.5000 norm=0.9351\n",
      "[iter 0] loss=1.1582 val_loss=0.0000 scale=1.0000 norm=1.8101\n",
      "[iter 100] loss=1.0066 val_loss=0.0000 scale=0.5000 norm=0.9341\n",
      "[iter 0] loss=1.1596 val_loss=0.0000 scale=1.0000 norm=1.8077\n",
      "[iter 100] loss=1.0071 val_loss=0.0000 scale=0.5000 norm=0.9344\n",
      "[iter 0] loss=1.1598 val_loss=0.0000 scale=1.0000 norm=1.8077\n",
      "[iter 100] loss=1.0069 val_loss=0.0000 scale=0.5000 norm=0.9342\n",
      "[iter 0] loss=1.1609 val_loss=0.0000 scale=1.0000 norm=1.8054\n",
      "[iter 100] loss=1.0086 val_loss=0.0000 scale=0.5000 norm=0.9320\n",
      "[iter 0] loss=1.1608 val_loss=0.0000 scale=1.0000 norm=1.8054\n",
      "[iter 100] loss=1.0096 val_loss=0.0000 scale=1.0000 norm=1.8635\n",
      "[iter 0] loss=1.1594 val_loss=0.0000 scale=1.0000 norm=1.8077\n",
      "[iter 100] loss=1.0055 val_loss=0.0000 scale=0.5000 norm=0.9345\n",
      "[iter 0] loss=1.1583 val_loss=0.0000 scale=1.0000 norm=1.8075\n",
      "[iter 100] loss=1.0047 val_loss=0.0000 scale=1.0000 norm=1.8675\n",
      "[iter 0] loss=1.1571 val_loss=0.0000 scale=1.0000 norm=1.8099\n",
      "[iter 100] loss=1.0040 val_loss=0.0000 scale=1.0000 norm=1.8692\n",
      "[iter 0] loss=1.1567 val_loss=0.0000 scale=1.0000 norm=1.8105\n",
      "[iter 100] loss=1.0037 val_loss=0.0000 scale=1.0000 norm=1.8699\n",
      "[iter 0] loss=1.1552 val_loss=0.0000 scale=1.0000 norm=1.8112\n",
      "[iter 100] loss=1.0022 val_loss=0.0000 scale=1.0000 norm=1.8711\n",
      "[iter 0] loss=1.1539 val_loss=0.0000 scale=1.0000 norm=1.8138\n",
      "[iter 100] loss=1.0013 val_loss=0.0000 scale=1.0000 norm=1.8744\n",
      "[iter 0] loss=1.1528 val_loss=0.0000 scale=1.0000 norm=1.8160\n",
      "[iter 100] loss=1.0000 val_loss=0.0000 scale=1.0000 norm=1.8757\n",
      "[iter 0] loss=1.1529 val_loss=0.0000 scale=1.0000 norm=1.8158\n",
      "[iter 100] loss=0.9999 val_loss=0.0000 scale=2.0000 norm=3.7542\n",
      "[iter 0] loss=1.1518 val_loss=0.0000 scale=1.0000 norm=1.8178\n",
      "[iter 100] loss=1.0008 val_loss=0.0000 scale=1.0000 norm=1.8768\n",
      "[iter 0] loss=1.1512 val_loss=0.0000 scale=1.0000 norm=1.8191\n",
      "[iter 100] loss=1.0004 val_loss=0.0000 scale=1.0000 norm=1.8759\n",
      "[iter 0] loss=1.1513 val_loss=0.0000 scale=1.0000 norm=1.8189\n",
      "[iter 100] loss=1.0000 val_loss=0.0000 scale=1.0000 norm=1.8757\n",
      "[iter 0] loss=1.1516 val_loss=0.0000 scale=1.0000 norm=1.8183\n",
      "[iter 100] loss=0.9997 val_loss=0.0000 scale=1.0000 norm=1.8759\n",
      "[iter 0] loss=1.1511 val_loss=0.0000 scale=1.0000 norm=1.8192\n",
      "[iter 100] loss=0.9981 val_loss=0.0000 scale=0.5000 norm=0.9389\n",
      "[iter 0] loss=1.1519 val_loss=0.0000 scale=1.0000 norm=1.8177\n",
      "[iter 100] loss=0.9989 val_loss=0.0000 scale=1.0000 norm=1.8754\n",
      "[iter 0] loss=1.1528 val_loss=0.0000 scale=1.0000 norm=1.8168\n",
      "[iter 100] loss=1.0008 val_loss=0.0000 scale=1.0000 norm=1.8747\n",
      "[iter 0] loss=1.1523 val_loss=0.0000 scale=1.0000 norm=1.8178\n",
      "[iter 100] loss=0.9999 val_loss=0.0000 scale=1.0000 norm=1.8767\n",
      "[iter 0] loss=1.1528 val_loss=0.0000 scale=1.0000 norm=1.8176\n",
      "[iter 100] loss=0.9999 val_loss=0.0000 scale=0.5000 norm=0.9387\n",
      "[iter 0] loss=1.1524 val_loss=0.0000 scale=1.0000 norm=1.8184\n",
      "[iter 100] loss=0.9986 val_loss=0.0000 scale=1.0000 norm=1.8790\n",
      "[iter 0] loss=1.1524 val_loss=0.0000 scale=1.0000 norm=1.8184\n",
      "[iter 100] loss=0.9976 val_loss=0.0000 scale=0.5000 norm=0.9403\n",
      "[iter 0] loss=1.1538 val_loss=0.0000 scale=1.0000 norm=1.8154\n",
      "[iter 100] loss=0.9993 val_loss=0.0000 scale=1.0000 norm=1.8767\n",
      "[iter 0] loss=1.1548 val_loss=0.0000 scale=1.0000 norm=1.8139\n",
      "[iter 100] loss=1.0005 val_loss=0.0000 scale=1.0000 norm=1.8749\n",
      "[iter 0] loss=1.1553 val_loss=0.0000 scale=1.0000 norm=1.8131\n",
      "[iter 100] loss=1.0011 val_loss=0.0000 scale=1.0000 norm=1.8724\n",
      "[iter 0] loss=1.1564 val_loss=0.0000 scale=1.0000 norm=1.8107\n",
      "[iter 100] loss=1.0011 val_loss=0.0000 scale=1.0000 norm=1.8705\n",
      "[iter 0] loss=1.1566 val_loss=0.0000 scale=1.0000 norm=1.8105\n",
      "[iter 100] loss=1.0020 val_loss=0.0000 scale=0.5000 norm=0.9351\n",
      "[iter 0] loss=1.1558 val_loss=0.0000 scale=1.0000 norm=1.8119\n",
      "[iter 100] loss=1.0016 val_loss=0.0000 scale=1.0000 norm=1.8698\n",
      "[iter 0] loss=1.1557 val_loss=0.0000 scale=1.0000 norm=1.8121\n",
      "[iter 100] loss=0.9997 val_loss=0.0000 scale=0.5000 norm=0.9364\n",
      "[iter 0] loss=1.1553 val_loss=0.0000 scale=1.0000 norm=1.8131\n",
      "[iter 100] loss=1.0003 val_loss=0.0000 scale=1.0000 norm=1.8728\n",
      "[iter 0] loss=1.1548 val_loss=0.0000 scale=1.0000 norm=1.8140\n",
      "[iter 100] loss=0.9984 val_loss=0.0000 scale=1.0000 norm=1.8750\n",
      "[iter 0] loss=1.1548 val_loss=0.0000 scale=1.0000 norm=1.8139\n",
      "[iter 100] loss=0.9992 val_loss=0.0000 scale=1.0000 norm=1.8727\n",
      "[iter 0] loss=1.1566 val_loss=0.0000 scale=1.0000 norm=1.8108\n",
      "[iter 100] loss=1.0009 val_loss=0.0000 scale=1.0000 norm=1.8710\n",
      "[iter 0] loss=1.1549 val_loss=0.0000 scale=1.0000 norm=1.8140\n",
      "[iter 100] loss=1.0001 val_loss=0.0000 scale=1.0000 norm=1.8733\n",
      "[iter 0] loss=1.1552 val_loss=0.0000 scale=1.0000 norm=1.8134\n",
      "[iter 100] loss=1.0003 val_loss=0.0000 scale=1.0000 norm=1.8725\n",
      "[iter 0] loss=1.1559 val_loss=0.0000 scale=1.0000 norm=1.8119\n",
      "[iter 100] loss=1.0008 val_loss=0.0000 scale=1.0000 norm=1.8714\n",
      "[iter 0] loss=1.1541 val_loss=0.0000 scale=1.0000 norm=1.8142\n",
      "[iter 100] loss=0.9994 val_loss=0.0000 scale=1.0000 norm=1.8721\n",
      "[iter 0] loss=1.1521 val_loss=0.0000 scale=1.0000 norm=1.8177\n",
      "[iter 100] loss=0.9983 val_loss=0.0000 scale=1.0000 norm=1.8735\n",
      "[iter 0] loss=1.1515 val_loss=0.0000 scale=1.0000 norm=1.8179\n",
      "[iter 100] loss=0.9987 val_loss=0.0000 scale=1.0000 norm=1.8726\n",
      "[iter 0] loss=1.1495 val_loss=0.0000 scale=1.0000 norm=1.8213\n",
      "[iter 100] loss=0.9967 val_loss=0.0000 scale=0.5000 norm=0.9383\n",
      "[iter 0] loss=1.1496 val_loss=0.0000 scale=1.0000 norm=1.8215\n",
      "[iter 100] loss=0.9967 val_loss=0.0000 scale=1.0000 norm=1.8759\n",
      "[iter 0] loss=1.1484 val_loss=0.0000 scale=1.0000 norm=1.8238\n",
      "[iter 100] loss=0.9953 val_loss=0.0000 scale=1.0000 norm=1.8776\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8254\n",
      "[iter 100] loss=0.9951 val_loss=0.0000 scale=0.5000 norm=0.9389\n",
      "[iter 0] loss=1.1473 val_loss=0.0000 scale=1.0000 norm=1.8250\n",
      "[iter 100] loss=0.9948 val_loss=0.0000 scale=1.0000 norm=1.8782\n",
      "[iter 0] loss=1.1464 val_loss=0.0000 scale=1.0000 norm=1.8268\n",
      "[iter 100] loss=0.9941 val_loss=0.0000 scale=1.0000 norm=1.8785\n",
      "[iter 0] loss=1.1461 val_loss=0.0000 scale=1.0000 norm=1.8271\n",
      "[iter 100] loss=0.9940 val_loss=0.0000 scale=1.0000 norm=1.8797\n",
      "[iter 0] loss=1.1439 val_loss=0.0000 scale=1.0000 norm=1.8308\n",
      "[iter 100] loss=0.9911 val_loss=0.0000 scale=1.0000 norm=1.8842\n",
      "[iter 0] loss=1.1443 val_loss=0.0000 scale=1.0000 norm=1.8300\n",
      "[iter 100] loss=0.9917 val_loss=0.0000 scale=0.5000 norm=0.9413\n",
      "[iter 0] loss=1.1427 val_loss=0.0000 scale=1.0000 norm=1.8331\n",
      "[iter 100] loss=0.9895 val_loss=0.0000 scale=1.0000 norm=1.8864\n",
      "[iter 0] loss=1.1422 val_loss=0.0000 scale=1.0000 norm=1.8340\n",
      "[iter 100] loss=0.9882 val_loss=0.0000 scale=1.0000 norm=1.8872\n",
      "[iter 0] loss=1.1421 val_loss=0.0000 scale=1.0000 norm=1.8343\n",
      "[iter 100] loss=0.9881 val_loss=0.0000 scale=1.0000 norm=1.8902\n",
      "[iter 0] loss=1.1427 val_loss=0.0000 scale=1.0000 norm=1.8328\n",
      "[iter 100] loss=0.9879 val_loss=0.0000 scale=0.5000 norm=0.9454\n",
      "[iter 0] loss=1.1423 val_loss=0.0000 scale=1.0000 norm=1.8339\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=0.5000 norm=0.9454\n",
      "[iter 0] loss=1.1420 val_loss=0.0000 scale=1.0000 norm=1.8343\n",
      "[iter 100] loss=0.9897 val_loss=0.0000 scale=1.0000 norm=1.8892\n",
      "[iter 0] loss=1.1428 val_loss=0.0000 scale=1.0000 norm=1.8328\n",
      "[iter 100] loss=0.9896 val_loss=0.0000 scale=1.0000 norm=1.8931\n",
      "[iter 0] loss=1.1428 val_loss=0.0000 scale=1.0000 norm=1.8328\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=1.0000 norm=1.8922\n",
      "[iter 0] loss=1.1429 val_loss=0.0000 scale=1.0000 norm=1.8325\n",
      "[iter 100] loss=0.9890 val_loss=0.0000 scale=1.0000 norm=1.8907\n",
      "[iter 0] loss=1.1437 val_loss=0.0000 scale=1.0000 norm=1.8310\n",
      "[iter 100] loss=0.9884 val_loss=0.0000 scale=1.0000 norm=1.8891\n",
      "[iter 0] loss=1.1420 val_loss=0.0000 scale=1.0000 norm=1.8342\n",
      "[iter 100] loss=0.9864 val_loss=0.0000 scale=1.0000 norm=1.8926\n",
      "[iter 0] loss=1.1421 val_loss=0.0000 scale=1.0000 norm=1.8341\n",
      "[iter 100] loss=0.9846 val_loss=0.0000 scale=1.0000 norm=1.8938\n",
      "[iter 0] loss=1.1414 val_loss=0.0000 scale=1.0000 norm=1.8355\n",
      "[iter 100] loss=0.9837 val_loss=0.0000 scale=1.0000 norm=1.8947\n",
      "[iter 0] loss=1.1417 val_loss=0.0000 scale=1.0000 norm=1.8353\n",
      "[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=1.8960\n",
      "[iter 0] loss=1.1415 val_loss=0.0000 scale=1.0000 norm=1.8354\n",
      "[iter 100] loss=0.9836 val_loss=0.0000 scale=0.5000 norm=0.9479\n",
      "[iter 0] loss=1.1416 val_loss=0.0000 scale=1.0000 norm=1.8354\n",
      "[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=1.8936\n",
      "[iter 0] loss=1.1415 val_loss=0.0000 scale=1.0000 norm=1.8355\n",
      "[iter 100] loss=0.9832 val_loss=0.0000 scale=1.0000 norm=1.8938\n",
      "[iter 0] loss=1.1413 val_loss=0.0000 scale=1.0000 norm=1.8360\n",
      "[iter 100] loss=0.9838 val_loss=0.0000 scale=1.0000 norm=1.8912\n",
      "[iter 0] loss=1.1407 val_loss=0.0000 scale=1.0000 norm=1.8373\n",
      "[iter 100] loss=0.9830 val_loss=0.0000 scale=0.5000 norm=0.9466\n",
      "[iter 0] loss=1.1410 val_loss=0.0000 scale=1.0000 norm=1.8364\n",
      "[iter 100] loss=0.9830 val_loss=0.0000 scale=1.0000 norm=1.8944\n",
      "[iter 0] loss=1.1423 val_loss=0.0000 scale=1.0000 norm=1.8340\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=0.5000 norm=0.9444\n",
      "[iter 0] loss=1.1426 val_loss=0.0000 scale=1.0000 norm=1.8334\n",
      "[iter 100] loss=0.9865 val_loss=0.0000 scale=1.0000 norm=1.8886\n",
      "[iter 0] loss=1.1443 val_loss=0.0000 scale=1.0000 norm=1.8306\n",
      "[iter 100] loss=0.9898 val_loss=0.0000 scale=1.0000 norm=1.8812\n",
      "[iter 0] loss=1.1453 val_loss=0.0000 scale=1.0000 norm=1.8288\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=1.0000 norm=1.8858\n",
      "[iter 0] loss=1.1467 val_loss=0.0000 scale=1.0000 norm=1.8260\n",
      "[iter 100] loss=0.9933 val_loss=0.0000 scale=1.0000 norm=1.8779\n",
      "[iter 0] loss=1.1461 val_loss=0.0000 scale=1.0000 norm=1.8273\n",
      "[iter 100] loss=0.9924 val_loss=0.0000 scale=1.0000 norm=1.8812\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8254\n",
      "[iter 100] loss=0.9938 val_loss=0.0000 scale=1.0000 norm=1.8791\n",
      "[iter 0] loss=1.1472 val_loss=0.0000 scale=1.0000 norm=1.8250\n",
      "[iter 100] loss=0.9945 val_loss=0.0000 scale=0.5000 norm=0.9401\n",
      "[iter 0] loss=1.1478 val_loss=0.0000 scale=1.0000 norm=1.8238\n",
      "[iter 100] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=1.8776\n",
      "[iter 0] loss=1.1475 val_loss=0.0000 scale=1.0000 norm=1.8243\n",
      "[iter 100] loss=0.9948 val_loss=0.0000 scale=0.5000 norm=0.9398\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8234\n",
      "[iter 100] loss=0.9960 val_loss=0.0000 scale=1.0000 norm=1.8769\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8232\n",
      "[iter 100] loss=0.9953 val_loss=0.0000 scale=1.0000 norm=1.8791\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8236\n",
      "[iter 100] loss=0.9960 val_loss=0.0000 scale=1.0000 norm=1.8776\n",
      "[iter 0] loss=1.1477 val_loss=0.0000 scale=1.0000 norm=1.8238\n",
      "[iter 100] loss=0.9950 val_loss=0.0000 scale=0.5000 norm=0.9391\n",
      "[iter 0] loss=1.1463 val_loss=0.0000 scale=1.0000 norm=1.8267\n",
      "[iter 100] loss=0.9920 val_loss=0.0000 scale=0.5000 norm=0.9417\n",
      "[iter 0] loss=1.1452 val_loss=0.0000 scale=1.0000 norm=1.8287\n",
      "[iter 100] loss=0.9916 val_loss=0.0000 scale=1.0000 norm=1.8830\n",
      "[iter 0] loss=1.1445 val_loss=0.0000 scale=1.0000 norm=1.8302\n",
      "[iter 100] loss=0.9900 val_loss=0.0000 scale=1.0000 norm=1.8845\n",
      "[iter 0] loss=1.1440 val_loss=0.0000 scale=1.0000 norm=1.8312\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=0.5000 norm=0.9431\n",
      "[iter 0] loss=1.1444 val_loss=0.0000 scale=1.0000 norm=1.8301\n",
      "[iter 100] loss=0.9891 val_loss=0.0000 scale=0.5000 norm=0.9419\n",
      "[iter 0] loss=1.1445 val_loss=0.0000 scale=1.0000 norm=1.8300\n",
      "[iter 100] loss=0.9897 val_loss=0.0000 scale=1.0000 norm=1.8823\n",
      "[iter 0] loss=1.1440 val_loss=0.0000 scale=1.0000 norm=1.8309\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=1.0000 norm=1.8819\n",
      "[iter 0] loss=1.1441 val_loss=0.0000 scale=1.0000 norm=1.8309\n",
      "[iter 100] loss=0.9892 val_loss=0.0000 scale=0.5000 norm=0.9421\n",
      "[iter 0] loss=1.1441 val_loss=0.0000 scale=1.0000 norm=1.8308\n",
      "[iter 100] loss=0.9896 val_loss=0.0000 scale=1.0000 norm=1.8821\n",
      "[iter 0] loss=1.1441 val_loss=0.0000 scale=1.0000 norm=1.8307\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=1.0000 norm=1.8830\n",
      "[iter 0] loss=1.1441 val_loss=0.0000 scale=1.0000 norm=1.8307\n",
      "[iter 100] loss=0.9900 val_loss=0.0000 scale=1.0000 norm=1.8813\n",
      "[iter 0] loss=1.1439 val_loss=0.0000 scale=1.0000 norm=1.8314\n",
      "[iter 100] loss=0.9898 val_loss=0.0000 scale=1.0000 norm=1.8799\n",
      "[iter 0] loss=1.1445 val_loss=0.0000 scale=1.0000 norm=1.8303\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=1.0000 norm=1.8815\n",
      "[iter 0] loss=1.1444 val_loss=0.0000 scale=1.0000 norm=1.8301\n",
      "[iter 100] loss=0.9897 val_loss=0.0000 scale=1.0000 norm=1.8807\n",
      "[iter 0] loss=1.1459 val_loss=0.0000 scale=1.0000 norm=1.8274\n",
      "[iter 100] loss=0.9923 val_loss=0.0000 scale=1.0000 norm=1.8760\n",
      "[iter 0] loss=1.1468 val_loss=0.0000 scale=1.0000 norm=1.8255\n",
      "[iter 100] loss=0.9932 val_loss=0.0000 scale=0.5000 norm=0.9371\n",
      "[iter 0] loss=1.1469 val_loss=0.0000 scale=1.0000 norm=1.8253\n",
      "[iter 100] loss=0.9947 val_loss=0.0000 scale=1.0000 norm=1.8738\n",
      "[iter 0] loss=1.1463 val_loss=0.0000 scale=1.0000 norm=1.8267\n",
      "[iter 100] loss=0.9930 val_loss=0.0000 scale=1.0000 norm=1.8756\n",
      "[iter 0] loss=1.1465 val_loss=0.0000 scale=1.0000 norm=1.8262\n",
      "[iter 100] loss=0.9931 val_loss=0.0000 scale=0.5000 norm=0.9379\n",
      "[iter 0] loss=1.1449 val_loss=0.0000 scale=1.0000 norm=1.8296\n",
      "[iter 100] loss=0.9900 val_loss=0.0000 scale=1.0000 norm=1.8798\n",
      "[iter 0] loss=1.1449 val_loss=0.0000 scale=1.0000 norm=1.8296\n",
      "[iter 100] loss=0.9892 val_loss=0.0000 scale=1.0000 norm=1.8798\n",
      "[iter 0] loss=1.1439 val_loss=0.0000 scale=1.0000 norm=1.8318\n",
      "[iter 100] loss=0.9860 val_loss=0.0000 scale=0.5000 norm=0.9434\n",
      "[iter 0] loss=1.1436 val_loss=0.0000 scale=1.0000 norm=1.8320\n",
      "[iter 100] loss=0.9863 val_loss=0.0000 scale=0.5000 norm=0.9421\n",
      "[iter 0] loss=1.1438 val_loss=0.0000 scale=1.0000 norm=1.8318\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=0.5000 norm=0.9429\n",
      "[iter 0] loss=1.1442 val_loss=0.0000 scale=1.0000 norm=1.8314\n",
      "[iter 100] loss=0.9871 val_loss=0.0000 scale=1.0000 norm=1.8839\n",
      "[iter 0] loss=1.1438 val_loss=0.0000 scale=1.0000 norm=1.8319\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=1.0000 norm=1.8828\n",
      "[iter 0] loss=1.1433 val_loss=0.0000 scale=1.0000 norm=1.8330\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9434\n",
      "[iter 0] loss=1.1438 val_loss=0.0000 scale=1.0000 norm=1.8317\n",
      "[iter 100] loss=0.9857 val_loss=0.0000 scale=0.5000 norm=0.9425\n",
      "[iter 0] loss=1.1442 val_loss=0.0000 scale=1.0000 norm=1.8310\n",
      "[iter 100] loss=0.9851 val_loss=0.0000 scale=1.0000 norm=1.8828\n",
      "[iter 0] loss=1.1442 val_loss=0.0000 scale=1.0000 norm=1.8309\n",
      "[iter 100] loss=0.9852 val_loss=0.0000 scale=1.0000 norm=1.8808\n",
      "[iter 0] loss=1.1437 val_loss=0.0000 scale=1.0000 norm=1.8321\n",
      "[iter 100] loss=0.9835 val_loss=0.0000 scale=1.0000 norm=1.8842\n",
      "[iter 0] loss=1.1431 val_loss=0.0000 scale=1.0000 norm=1.8331\n",
      "[iter 100] loss=0.9817 val_loss=0.0000 scale=1.0000 norm=1.8870\n",
      "[iter 0] loss=1.1434 val_loss=0.0000 scale=1.0000 norm=1.8324\n",
      "[iter 100] loss=0.9837 val_loss=0.0000 scale=1.0000 norm=1.8816\n",
      "[iter 0] loss=1.1442 val_loss=0.0000 scale=1.0000 norm=1.8308\n",
      "[iter 100] loss=0.9846 val_loss=0.0000 scale=0.5000 norm=0.9410\n",
      "[iter 0] loss=1.1460 val_loss=0.0000 scale=1.0000 norm=1.8273\n",
      "[iter 100] loss=0.9862 val_loss=0.0000 scale=0.5000 norm=0.9402\n",
      "[iter 0] loss=1.1474 val_loss=0.0000 scale=1.0000 norm=1.8247\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=1.0000 norm=1.8821\n",
      "[iter 0] loss=1.1475 val_loss=0.0000 scale=1.0000 norm=1.8245\n",
      "[iter 100] loss=0.9881 val_loss=0.0000 scale=0.5000 norm=0.9400\n",
      "[iter 0] loss=1.1475 val_loss=0.0000 scale=1.0000 norm=1.8247\n",
      "[iter 100] loss=0.9876 val_loss=0.0000 scale=0.5000 norm=0.9393\n",
      "[iter 0] loss=1.1482 val_loss=0.0000 scale=1.0000 norm=1.8232\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=1.0000 norm=1.8779\n",
      "[iter 0] loss=1.1481 val_loss=0.0000 scale=1.0000 norm=1.8234\n",
      "[iter 100] loss=0.9887 val_loss=0.0000 scale=1.0000 norm=1.8783\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8238\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=1.0000 norm=1.8808\n",
      "[iter 0] loss=1.1477 val_loss=0.0000 scale=1.0000 norm=1.8240\n",
      "[iter 100] loss=0.9876 val_loss=0.0000 scale=1.0000 norm=1.8787\n",
      "[iter 0] loss=1.1471 val_loss=0.0000 scale=1.0000 norm=1.8253\n",
      "[iter 100] loss=0.9876 val_loss=0.0000 scale=0.5000 norm=0.9384\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8255\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=1.0000 norm=1.8767\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8255\n",
      "[iter 100] loss=0.9882 val_loss=0.0000 scale=0.5000 norm=0.9366\n",
      "[iter 0] loss=1.1474 val_loss=0.0000 scale=1.0000 norm=1.8248\n",
      "[iter 100] loss=0.9880 val_loss=0.0000 scale=0.5000 norm=0.9382\n",
      "[iter 0] loss=1.1478 val_loss=0.0000 scale=1.0000 norm=1.8240\n",
      "[iter 100] loss=0.9894 val_loss=0.0000 scale=0.5000 norm=0.9362\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8239\n",
      "[iter 100] loss=0.9890 val_loss=0.0000 scale=0.5000 norm=0.9373\n",
      "[iter 0] loss=1.1478 val_loss=0.0000 scale=1.0000 norm=1.8237\n",
      "[iter 100] loss=0.9888 val_loss=0.0000 scale=1.0000 norm=1.8723\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8237\n",
      "[iter 100] loss=0.9903 val_loss=0.0000 scale=0.5000 norm=0.9353\n",
      "[iter 0] loss=1.1478 val_loss=0.0000 scale=1.0000 norm=1.8237\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=0.5000 norm=0.9360\n",
      "[iter 0] loss=1.1477 val_loss=0.0000 scale=1.0000 norm=1.8238\n",
      "[iter 100] loss=0.9878 val_loss=0.0000 scale=1.0000 norm=1.8724\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8254\n",
      "[iter 100] loss=0.9869 val_loss=0.0000 scale=0.5000 norm=0.9372\n",
      "[iter 0] loss=1.1466 val_loss=0.0000 scale=1.0000 norm=1.8262\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=0.5000 norm=0.9372\n",
      "[iter 0] loss=1.1463 val_loss=0.0000 scale=1.0000 norm=1.8265\n",
      "[iter 100] loss=0.9847 val_loss=0.0000 scale=1.0000 norm=1.8795\n",
      "[iter 0] loss=1.1463 val_loss=0.0000 scale=1.0000 norm=1.8266\n",
      "[iter 100] loss=0.9855 val_loss=0.0000 scale=1.0000 norm=1.8757\n",
      "[iter 0] loss=1.1471 val_loss=0.0000 scale=1.0000 norm=1.8249\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=1.0000 norm=1.8726\n",
      "[iter 0] loss=1.1471 val_loss=0.0000 scale=1.0000 norm=1.8248\n",
      "[iter 100] loss=0.9870 val_loss=0.0000 scale=0.5000 norm=0.9364\n",
      "[iter 0] loss=1.1476 val_loss=0.0000 scale=1.0000 norm=1.8241\n",
      "[iter 100] loss=0.9878 val_loss=0.0000 scale=0.5000 norm=0.9360\n",
      "[iter 0] loss=1.1481 val_loss=0.0000 scale=1.0000 norm=1.8232\n",
      "[iter 100] loss=0.9910 val_loss=0.0000 scale=0.5000 norm=0.9342\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8230\n",
      "[iter 100] loss=0.9895 val_loss=0.0000 scale=1.0000 norm=1.8695\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8235\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=0.5000 norm=0.9361\n",
      "[iter 0] loss=1.1482 val_loss=0.0000 scale=1.0000 norm=1.8225\n",
      "[iter 100] loss=0.9900 val_loss=0.0000 scale=0.5000 norm=0.9349\n",
      "[iter 0] loss=1.1484 val_loss=0.0000 scale=1.0000 norm=1.8222\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9385\n",
      "[iter 0] loss=1.1485 val_loss=0.0000 scale=1.0000 norm=1.8223\n",
      "[iter 100] loss=0.9859 val_loss=0.0000 scale=1.0000 norm=1.8792\n",
      "[iter 0] loss=1.1484 val_loss=0.0000 scale=1.0000 norm=1.8222\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9386\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8230\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=1.0000 norm=1.8777\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8231\n",
      "[iter 100] loss=0.9851 val_loss=0.0000 scale=0.5000 norm=0.9390\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8231\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=1.0000 norm=1.8756\n",
      "[iter 0] loss=1.1479 val_loss=0.0000 scale=1.0000 norm=1.8230\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=1.0000 norm=1.8746\n",
      "[iter 0] loss=1.1477 val_loss=0.0000 scale=1.0000 norm=1.8234\n",
      "[iter 100] loss=0.9867 val_loss=0.0000 scale=1.0000 norm=1.8750\n",
      "[iter 0] loss=1.1480 val_loss=0.0000 scale=1.0000 norm=1.8229\n",
      "[iter 100] loss=0.9859 val_loss=0.0000 scale=1.0000 norm=1.8777\n",
      "[iter 0] loss=1.1488 val_loss=0.0000 scale=1.0000 norm=1.8210\n",
      "[iter 100] loss=0.9914 val_loss=0.0000 scale=1.0000 norm=1.8669\n",
      "[iter 0] loss=1.1488 val_loss=0.0000 scale=1.0000 norm=1.8211\n",
      "[iter 100] loss=0.9913 val_loss=0.0000 scale=0.5000 norm=0.9332\n",
      "[iter 0] loss=1.1494 val_loss=0.0000 scale=1.0000 norm=1.8200\n",
      "[iter 100] loss=0.9925 val_loss=0.0000 scale=0.5000 norm=0.9323\n",
      "[iter 0] loss=1.1497 val_loss=0.0000 scale=1.0000 norm=1.8195\n",
      "[iter 100] loss=0.9926 val_loss=0.0000 scale=0.5000 norm=0.9337\n",
      "[iter 0] loss=1.1498 val_loss=0.0000 scale=1.0000 norm=1.8194\n",
      "[iter 100] loss=0.9939 val_loss=0.0000 scale=0.5000 norm=0.9316\n",
      "[iter 0] loss=1.1503 val_loss=0.0000 scale=1.0000 norm=1.8182\n",
      "[iter 100] loss=0.9908 val_loss=0.0000 scale=0.5000 norm=0.9339\n",
      "[iter 0] loss=1.1521 val_loss=0.0000 scale=1.0000 norm=1.8148\n",
      "[iter 100] loss=0.9960 val_loss=0.0000 scale=0.5000 norm=0.9292\n",
      "[iter 0] loss=1.1522 val_loss=0.0000 scale=1.0000 norm=1.8148\n",
      "[iter 100] loss=0.9954 val_loss=0.0000 scale=0.5000 norm=0.9302\n",
      "[iter 0] loss=1.1524 val_loss=0.0000 scale=1.0000 norm=1.8143\n",
      "[iter 100] loss=0.9966 val_loss=0.0000 scale=0.5000 norm=0.9309\n",
      "[iter 0] loss=1.1533 val_loss=0.0000 scale=1.0000 norm=1.8126\n",
      "[iter 100] loss=0.9969 val_loss=0.0000 scale=0.5000 norm=0.9303\n",
      "[iter 0] loss=1.1530 val_loss=0.0000 scale=1.0000 norm=1.8130\n",
      "[iter 100] loss=0.9974 val_loss=0.0000 scale=0.5000 norm=0.9292\n",
      "[iter 0] loss=1.1532 val_loss=0.0000 scale=1.0000 norm=1.8128\n",
      "[iter 100] loss=0.9972 val_loss=0.0000 scale=0.5000 norm=0.9295\n",
      "[iter 0] loss=1.1551 val_loss=0.0000 scale=1.0000 norm=1.8095\n",
      "[iter 100] loss=0.9986 val_loss=0.0000 scale=0.5000 norm=0.9294\n",
      "[iter 0] loss=1.1551 val_loss=0.0000 scale=1.0000 norm=1.8097\n",
      "[iter 100] loss=0.9998 val_loss=0.0000 scale=1.0000 norm=1.8575\n",
      "[iter 0] loss=1.1565 val_loss=0.0000 scale=1.0000 norm=1.8069\n",
      "[iter 100] loss=1.0029 val_loss=0.0000 scale=0.5000 norm=0.9277\n",
      "[iter 0] loss=1.1573 val_loss=0.0000 scale=1.0000 norm=1.8054\n",
      "[iter 100] loss=1.0041 val_loss=0.0000 scale=0.5000 norm=0.9260\n",
      "[iter 0] loss=1.1588 val_loss=0.0000 scale=1.0000 norm=1.8025\n",
      "[iter 100] loss=1.0049 val_loss=0.0000 scale=0.5000 norm=0.9261\n",
      "[iter 0] loss=1.1595 val_loss=0.0000 scale=1.0000 norm=1.8010\n",
      "[iter 100] loss=1.0069 val_loss=0.0000 scale=1.0000 norm=1.8471\n",
      "[iter 0] loss=1.1587 val_loss=0.0000 scale=1.0000 norm=1.8027\n",
      "[iter 100] loss=1.0068 val_loss=0.0000 scale=0.5000 norm=0.9243\n",
      "[iter 0] loss=1.1586 val_loss=0.0000 scale=1.0000 norm=1.8032\n",
      "[iter 100] loss=1.0082 val_loss=0.0000 scale=1.0000 norm=1.8483\n",
      "[iter 0] loss=1.1587 val_loss=0.0000 scale=1.0000 norm=1.8029\n",
      "[iter 100] loss=1.0091 val_loss=0.0000 scale=1.0000 norm=1.8470\n",
      "[iter 0] loss=1.1593 val_loss=0.0000 scale=1.0000 norm=1.8017\n",
      "[iter 100] loss=1.0092 val_loss=0.0000 scale=0.5000 norm=0.9244\n",
      "[iter 0] loss=1.1575 val_loss=0.0000 scale=1.0000 norm=1.8031\n",
      "[iter 100] loss=1.0065 val_loss=0.0000 scale=0.5000 norm=0.9236\n",
      "[iter 0] loss=1.1554 val_loss=0.0000 scale=1.0000 norm=1.8069\n",
      "[iter 100] loss=1.0044 val_loss=0.0000 scale=1.0000 norm=1.8551\n",
      "[iter 0] loss=1.1557 val_loss=0.0000 scale=1.0000 norm=1.8063\n",
      "[iter 100] loss=1.0049 val_loss=0.0000 scale=0.5000 norm=0.9261\n",
      "[iter 0] loss=1.1555 val_loss=0.0000 scale=1.0000 norm=1.8068\n",
      "[iter 100] loss=1.0042 val_loss=0.0000 scale=0.5000 norm=0.9272\n",
      "[iter 0] loss=1.1536 val_loss=0.0000 scale=1.0000 norm=1.8101\n",
      "[iter 100] loss=1.0029 val_loss=0.0000 scale=0.5000 norm=0.9283\n",
      "[iter 0] loss=1.1539 val_loss=0.0000 scale=1.0000 norm=1.8095\n",
      "[iter 100] loss=1.0032 val_loss=0.0000 scale=0.5000 norm=0.9285\n",
      "[iter 0] loss=1.1523 val_loss=0.0000 scale=1.0000 norm=1.8127\n",
      "[iter 100] loss=1.0022 val_loss=0.0000 scale=1.0000 norm=1.8563\n",
      "[iter 0] loss=1.1522 val_loss=0.0000 scale=1.0000 norm=1.8130\n",
      "[iter 100] loss=1.0018 val_loss=0.0000 scale=1.0000 norm=1.8601\n",
      "[iter 0] loss=1.1519 val_loss=0.0000 scale=1.0000 norm=1.8134\n",
      "[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=1.8610\n",
      "[iter 0] loss=1.1514 val_loss=0.0000 scale=1.0000 norm=1.8144\n",
      "[iter 100] loss=1.0011 val_loss=0.0000 scale=1.0000 norm=1.8597\n",
      "[iter 0] loss=1.1493 val_loss=0.0000 scale=1.0000 norm=1.8167\n",
      "[iter 100] loss=0.9984 val_loss=0.0000 scale=0.5000 norm=0.9310\n",
      "[iter 0] loss=1.1481 val_loss=0.0000 scale=1.0000 norm=1.8188\n",
      "[iter 100] loss=0.9973 val_loss=0.0000 scale=1.0000 norm=1.8661\n",
      "[iter 0] loss=1.1467 val_loss=0.0000 scale=1.0000 norm=1.8213\n",
      "[iter 100] loss=0.9967 val_loss=0.0000 scale=1.0000 norm=1.8641\n",
      "[iter 0] loss=1.1460 val_loss=0.0000 scale=1.0000 norm=1.8226\n",
      "[iter 100] loss=0.9954 val_loss=0.0000 scale=0.5000 norm=0.9341\n",
      "[iter 0] loss=1.1446 val_loss=0.0000 scale=1.0000 norm=1.8254\n",
      "[iter 100] loss=0.9935 val_loss=0.0000 scale=0.5000 norm=0.9358\n",
      "[iter 0] loss=1.1438 val_loss=0.0000 scale=1.0000 norm=1.8269\n",
      "[iter 100] loss=0.9945 val_loss=0.0000 scale=0.5000 norm=0.9343\n",
      "[iter 0] loss=1.1433 val_loss=0.0000 scale=1.0000 norm=1.8278\n",
      "[iter 100] loss=0.9939 val_loss=0.0000 scale=0.5000 norm=0.9346\n",
      "[iter 0] loss=1.1427 val_loss=0.0000 scale=1.0000 norm=1.8293\n",
      "[iter 100] loss=0.9912 val_loss=0.0000 scale=0.5000 norm=0.9379\n",
      "[iter 0] loss=1.1426 val_loss=0.0000 scale=1.0000 norm=1.8294\n",
      "[iter 100] loss=0.9929 val_loss=0.0000 scale=1.0000 norm=1.8680\n",
      "[iter 0] loss=1.1423 val_loss=0.0000 scale=1.0000 norm=1.8297\n",
      "[iter 100] loss=0.9930 val_loss=0.0000 scale=1.0000 norm=1.8674\n",
      "[iter 0] loss=1.1407 val_loss=0.0000 scale=1.0000 norm=1.8324\n",
      "[iter 100] loss=0.9913 val_loss=0.0000 scale=0.5000 norm=0.9380\n",
      "[iter 0] loss=1.1415 val_loss=0.0000 scale=1.0000 norm=1.8309\n",
      "[iter 100] loss=0.9936 val_loss=0.0000 scale=1.0000 norm=1.8701\n",
      "[iter 0] loss=1.1404 val_loss=0.0000 scale=1.0000 norm=1.8330\n",
      "[iter 100] loss=0.9927 val_loss=0.0000 scale=1.0000 norm=1.8726\n",
      "[iter 0] loss=1.1385 val_loss=0.0000 scale=1.0000 norm=1.8361\n",
      "[iter 100] loss=0.9890 val_loss=0.0000 scale=1.0000 norm=1.8759\n",
      "[iter 0] loss=1.1384 val_loss=0.0000 scale=1.0000 norm=1.8363\n",
      "[iter 100] loss=0.9914 val_loss=0.0000 scale=0.5000 norm=0.9357\n",
      "[iter 0] loss=1.1375 val_loss=0.0000 scale=1.0000 norm=1.8382\n",
      "[iter 100] loss=0.9902 val_loss=0.0000 scale=0.5000 norm=0.9361\n",
      "[iter 0] loss=1.1370 val_loss=0.0000 scale=1.0000 norm=1.8393\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=0.5000 norm=0.9388\n",
      "[iter 0] loss=1.1370 val_loss=0.0000 scale=1.0000 norm=1.8392\n",
      "[iter 100] loss=0.9887 val_loss=0.0000 scale=0.5000 norm=0.9387\n",
      "[iter 0] loss=1.1351 val_loss=0.0000 scale=1.0000 norm=1.8429\n",
      "[iter 100] loss=0.9863 val_loss=0.0000 scale=0.5000 norm=0.9403\n",
      "[iter 0] loss=1.1332 val_loss=0.0000 scale=1.0000 norm=1.8460\n",
      "[iter 100] loss=0.9854 val_loss=0.0000 scale=0.5000 norm=0.9422\n",
      "[iter 0] loss=1.1331 val_loss=0.0000 scale=1.0000 norm=1.8462\n",
      "[iter 100] loss=0.9838 val_loss=0.0000 scale=0.5000 norm=0.9432\n",
      "[iter 0] loss=1.1324 val_loss=0.0000 scale=1.0000 norm=1.8475\n",
      "[iter 100] loss=0.9838 val_loss=0.0000 scale=0.5000 norm=0.9420\n",
      "[iter 0] loss=1.1333 val_loss=0.0000 scale=1.0000 norm=1.8467\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9417\n",
      "[iter 0] loss=1.1345 val_loss=0.0000 scale=1.0000 norm=1.8445\n",
      "[iter 100] loss=0.9869 val_loss=0.0000 scale=0.5000 norm=0.9401\n",
      "[iter 0] loss=1.1347 val_loss=0.0000 scale=1.0000 norm=1.8439\n",
      "[iter 100] loss=0.9879 val_loss=0.0000 scale=0.5000 norm=0.9391\n",
      "[iter 0] loss=1.1360 val_loss=0.0000 scale=1.0000 norm=1.8412\n",
      "[iter 100] loss=0.9875 val_loss=0.0000 scale=0.5000 norm=0.9394\n",
      "[iter 0] loss=1.1368 val_loss=0.0000 scale=1.0000 norm=1.8396\n",
      "[iter 100] loss=0.9882 val_loss=0.0000 scale=0.5000 norm=0.9387\n",
      "[iter 0] loss=1.1370 val_loss=0.0000 scale=1.0000 norm=1.8390\n",
      "[iter 100] loss=0.9896 val_loss=0.0000 scale=1.0000 norm=1.8735\n",
      "[iter 0] loss=1.1358 val_loss=0.0000 scale=1.0000 norm=1.8392\n",
      "[iter 100] loss=0.9836 val_loss=0.0000 scale=0.5000 norm=0.9422\n",
      "[iter 0] loss=1.1376 val_loss=0.0000 scale=1.0000 norm=1.8361\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9412\n",
      "[iter 0] loss=1.1353 val_loss=0.0000 scale=1.0000 norm=1.8396\n",
      "[iter 100] loss=0.9840 val_loss=0.0000 scale=0.5000 norm=0.9420\n",
      "[iter 0] loss=1.1361 val_loss=0.0000 scale=1.0000 norm=1.8378\n",
      "[iter 100] loss=0.9833 val_loss=0.0000 scale=0.5000 norm=0.9421\n",
      "[iter 0] loss=1.1367 val_loss=0.0000 scale=1.0000 norm=1.8369\n",
      "[iter 100] loss=0.9865 val_loss=0.0000 scale=0.5000 norm=0.9375\n",
      "[iter 0] loss=1.1377 val_loss=0.0000 scale=1.0000 norm=1.8348\n",
      "[iter 100] loss=0.9854 val_loss=0.0000 scale=0.5000 norm=0.9393\n",
      "[iter 0] loss=1.1385 val_loss=0.0000 scale=1.0000 norm=1.8330\n",
      "[iter 100] loss=0.9848 val_loss=0.0000 scale=1.0000 norm=1.8782\n",
      "[iter 0] loss=1.1378 val_loss=0.0000 scale=1.0000 norm=1.8341\n",
      "[iter 100] loss=0.9836 val_loss=0.0000 scale=0.5000 norm=0.9396\n",
      "[iter 0] loss=1.1366 val_loss=0.0000 scale=1.0000 norm=1.8355\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=1.0000 norm=1.8703\n",
      "[iter 0] loss=1.1379 val_loss=0.0000 scale=1.0000 norm=1.8328\n",
      "[iter 100] loss=0.9871 val_loss=0.0000 scale=0.5000 norm=0.9344\n",
      "[iter 0] loss=1.1374 val_loss=0.0000 scale=1.0000 norm=1.8334\n",
      "[iter 100] loss=0.9867 val_loss=0.0000 scale=0.5000 norm=0.9349\n",
      "[iter 0] loss=1.1371 val_loss=0.0000 scale=1.0000 norm=1.8340\n",
      "[iter 100] loss=0.9869 val_loss=0.0000 scale=0.5000 norm=0.9361\n",
      "[iter 0] loss=1.1356 val_loss=0.0000 scale=1.0000 norm=1.8364\n",
      "[iter 100] loss=0.9883 val_loss=0.0000 scale=0.5000 norm=0.9353\n",
      "[iter 0] loss=1.1335 val_loss=0.0000 scale=1.0000 norm=1.8390\n",
      "[iter 100] loss=0.9867 val_loss=0.0000 scale=0.5000 norm=0.9371\n",
      "[iter 0] loss=1.1348 val_loss=0.0000 scale=1.0000 norm=1.8335\n",
      "[iter 100] loss=0.9875 val_loss=0.0000 scale=0.5000 norm=0.9369\n",
      "[iter 0] loss=1.1344 val_loss=0.0000 scale=1.0000 norm=1.8344\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=0.5000 norm=0.9370\n",
      "[iter 0] loss=1.1343 val_loss=0.0000 scale=1.0000 norm=1.8346\n",
      "[iter 100] loss=0.9875 val_loss=0.0000 scale=0.5000 norm=0.9374\n",
      "[iter 0] loss=1.1341 val_loss=0.0000 scale=1.0000 norm=1.8350\n",
      "[iter 100] loss=0.9878 val_loss=0.0000 scale=0.5000 norm=0.9381\n",
      "[iter 0] loss=1.1343 val_loss=0.0000 scale=1.0000 norm=1.8346\n",
      "[iter 100] loss=0.9881 val_loss=0.0000 scale=0.5000 norm=0.9376\n",
      "[iter 0] loss=1.1322 val_loss=0.0000 scale=1.0000 norm=1.8374\n",
      "[iter 100] loss=0.9850 val_loss=0.0000 scale=1.0000 norm=1.8786\n",
      "[iter 0] loss=1.1304 val_loss=0.0000 scale=1.0000 norm=1.8391\n",
      "[iter 100] loss=0.9848 val_loss=0.0000 scale=1.0000 norm=1.8780\n",
      "[iter 0] loss=1.1323 val_loss=0.0000 scale=1.0000 norm=1.8359\n",
      "[iter 100] loss=0.9871 val_loss=0.0000 scale=1.0000 norm=1.8754\n",
      "[iter 0] loss=1.1317 val_loss=0.0000 scale=1.0000 norm=1.8371\n",
      "[iter 100] loss=0.9855 val_loss=0.0000 scale=1.0000 norm=1.8796\n",
      "[iter 0] loss=1.1321 val_loss=0.0000 scale=1.0000 norm=1.8363\n",
      "[iter 100] loss=0.9861 val_loss=0.0000 scale=1.0000 norm=1.8763\n",
      "[iter 0] loss=1.1321 val_loss=0.0000 scale=1.0000 norm=1.8363\n",
      "[iter 100] loss=0.9851 val_loss=0.0000 scale=1.0000 norm=1.8781\n",
      "[iter 0] loss=1.1327 val_loss=0.0000 scale=1.0000 norm=1.8353\n",
      "[iter 100] loss=0.9856 val_loss=0.0000 scale=0.5000 norm=0.9387\n",
      "[iter 0] loss=1.1321 val_loss=0.0000 scale=1.0000 norm=1.8362\n",
      "[iter 100] loss=0.9852 val_loss=0.0000 scale=0.5000 norm=0.9390\n",
      "[iter 0] loss=1.1331 val_loss=0.0000 scale=1.0000 norm=1.8345\n",
      "[iter 100] loss=0.9864 val_loss=0.0000 scale=1.0000 norm=1.8766\n",
      "[iter 0] loss=1.1331 val_loss=0.0000 scale=1.0000 norm=1.8345\n",
      "[iter 100] loss=0.9855 val_loss=0.0000 scale=0.5000 norm=0.9373\n",
      "[iter 0] loss=1.1349 val_loss=0.0000 scale=1.0000 norm=1.8314\n",
      "[iter 100] loss=0.9878 val_loss=0.0000 scale=1.0000 norm=1.8726\n",
      "[iter 0] loss=1.1349 val_loss=0.0000 scale=1.0000 norm=1.8314\n",
      "[iter 100] loss=0.9863 val_loss=0.0000 scale=0.5000 norm=0.9387\n",
      "[iter 0] loss=1.1345 val_loss=0.0000 scale=1.0000 norm=1.8320\n",
      "[iter 100] loss=0.9868 val_loss=0.0000 scale=0.5000 norm=0.9373\n",
      "[iter 0] loss=1.1367 val_loss=0.0000 scale=1.0000 norm=1.8282\n",
      "[iter 100] loss=0.9871 val_loss=0.0000 scale=1.0000 norm=1.8701\n",
      "[iter 0] loss=1.1367 val_loss=0.0000 scale=1.0000 norm=1.8284\n",
      "[iter 100] loss=0.9878 val_loss=0.0000 scale=1.0000 norm=1.8691\n",
      "[iter 0] loss=1.1377 val_loss=0.0000 scale=1.0000 norm=1.8262\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=1.0000 norm=1.8649\n",
      "[iter 0] loss=1.1369 val_loss=0.0000 scale=1.0000 norm=1.8279\n",
      "[iter 100] loss=0.9882 val_loss=0.0000 scale=0.5000 norm=0.9347\n",
      "[iter 0] loss=1.1374 val_loss=0.0000 scale=1.0000 norm=1.8271\n",
      "[iter 100] loss=0.9883 val_loss=0.0000 scale=0.5000 norm=0.9353\n",
      "[iter 0] loss=1.1380 val_loss=0.0000 scale=1.0000 norm=1.8264\n",
      "[iter 100] loss=0.9902 val_loss=0.0000 scale=0.5000 norm=0.9343\n",
      "[iter 0] loss=1.1392 val_loss=0.0000 scale=1.0000 norm=1.8245\n",
      "[iter 100] loss=0.9922 val_loss=0.0000 scale=0.5000 norm=0.9334\n",
      "[iter 0] loss=1.1396 val_loss=0.0000 scale=1.0000 norm=1.8239\n",
      "[iter 100] loss=0.9920 val_loss=0.0000 scale=0.5000 norm=0.9335\n",
      "[iter 0] loss=1.1399 val_loss=0.0000 scale=1.0000 norm=1.8233\n",
      "[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=1.8677\n",
      "[iter 0] loss=1.1421 val_loss=0.0000 scale=1.0000 norm=1.8201\n",
      "[iter 100] loss=0.9935 val_loss=0.0000 scale=1.0000 norm=1.8648\n",
      "[iter 0] loss=1.1421 val_loss=0.0000 scale=1.0000 norm=1.8202\n",
      "[iter 100] loss=0.9946 val_loss=0.0000 scale=1.0000 norm=1.8655\n",
      "[iter 0] loss=1.1413 val_loss=0.0000 scale=1.0000 norm=1.8219\n",
      "[iter 100] loss=0.9925 val_loss=0.0000 scale=0.5000 norm=0.9336\n",
      "[iter 0] loss=1.1418 val_loss=0.0000 scale=1.0000 norm=1.8210\n",
      "[iter 100] loss=0.9929 val_loss=0.0000 scale=1.0000 norm=1.8661\n",
      "[iter 0] loss=1.1431 val_loss=0.0000 scale=1.0000 norm=1.8187\n",
      "[iter 100] loss=0.9953 val_loss=0.0000 scale=0.5000 norm=0.9317\n",
      "[iter 0] loss=1.1441 val_loss=0.0000 scale=1.0000 norm=1.8173\n",
      "[iter 100] loss=0.9932 val_loss=0.0000 scale=1.0000 norm=1.8669\n",
      "[iter 0] loss=1.1462 val_loss=0.0000 scale=1.0000 norm=1.8136\n",
      "[iter 100] loss=0.9957 val_loss=0.0000 scale=0.5000 norm=0.9306\n",
      "[iter 0] loss=1.1464 val_loss=0.0000 scale=1.0000 norm=1.8135\n",
      "[iter 100] loss=0.9969 val_loss=0.0000 scale=1.0000 norm=1.8587\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8122\n",
      "[iter 100] loss=0.9986 val_loss=0.0000 scale=0.5000 norm=0.9268\n",
      "[iter 0] loss=1.1470 val_loss=0.0000 scale=1.0000 norm=1.8124\n",
      "[iter 100] loss=0.9986 val_loss=0.0000 scale=0.5000 norm=0.9275\n",
      "[iter 0] loss=1.1492 val_loss=0.0000 scale=1.0000 norm=1.8087\n",
      "[iter 100] loss=1.0015 val_loss=0.0000 scale=1.0000 norm=1.8493\n",
      "[iter 0] loss=1.1497 val_loss=0.0000 scale=1.0000 norm=1.8078\n",
      "[iter 100] loss=1.0025 val_loss=0.0000 scale=1.0000 norm=1.8463\n",
      "[iter 0] loss=1.1506 val_loss=0.0000 scale=1.0000 norm=1.8059\n",
      "[iter 100] loss=1.0054 val_loss=0.0000 scale=1.0000 norm=1.8396\n",
      "[iter 0] loss=1.1527 val_loss=0.0000 scale=1.0000 norm=1.8023\n",
      "[iter 100] loss=1.0079 val_loss=0.0000 scale=0.5000 norm=0.9174\n",
      "[iter 0] loss=1.1540 val_loss=0.0000 scale=1.0000 norm=1.7999\n",
      "[iter 100] loss=1.0064 val_loss=0.0000 scale=0.5000 norm=0.9193\n",
      "[iter 0] loss=1.1558 val_loss=0.0000 scale=1.0000 norm=1.7970\n",
      "[iter 100] loss=1.0087 val_loss=0.0000 scale=1.0000 norm=1.8347\n",
      "[iter 0] loss=1.1558 val_loss=0.0000 scale=1.0000 norm=1.7970\n",
      "[iter 100] loss=1.0092 val_loss=0.0000 scale=1.0000 norm=1.8339\n",
      "[iter 0] loss=1.1573 val_loss=0.0000 scale=1.0000 norm=1.7944\n",
      "[iter 100] loss=1.0104 val_loss=0.0000 scale=1.0000 norm=1.8326\n",
      "[iter 0] loss=1.1586 val_loss=0.0000 scale=1.0000 norm=1.7920\n",
      "[iter 100] loss=1.0121 val_loss=0.0000 scale=0.5000 norm=0.9152\n",
      "[iter 0] loss=1.1590 val_loss=0.0000 scale=1.0000 norm=1.7911\n",
      "[iter 100] loss=1.0139 val_loss=0.0000 scale=1.0000 norm=1.8260\n",
      "[iter 0] loss=1.1592 val_loss=0.0000 scale=1.0000 norm=1.7908\n",
      "[iter 100] loss=1.0152 val_loss=0.0000 scale=0.5000 norm=0.9108\n",
      "[iter 0] loss=1.1604 val_loss=0.0000 scale=1.0000 norm=1.7902\n",
      "[iter 100] loss=1.0158 val_loss=0.0000 scale=0.5000 norm=0.9092\n",
      "[iter 0] loss=1.1610 val_loss=0.0000 scale=1.0000 norm=1.7890\n",
      "[iter 100] loss=1.0123 val_loss=0.0000 scale=0.5000 norm=0.9118\n",
      "[iter 0] loss=1.1618 val_loss=0.0000 scale=1.0000 norm=1.7884\n",
      "[iter 100] loss=1.0141 val_loss=0.0000 scale=1.0000 norm=1.8187\n",
      "[iter 0] loss=1.1621 val_loss=0.0000 scale=1.0000 norm=1.7879\n",
      "[iter 100] loss=1.0148 val_loss=0.0000 scale=0.5000 norm=0.9092\n",
      "[iter 0] loss=1.1633 val_loss=0.0000 scale=1.0000 norm=1.7856\n",
      "[iter 100] loss=1.0149 val_loss=0.0000 scale=0.5000 norm=0.9080\n",
      "[iter 0] loss=1.1640 val_loss=0.0000 scale=1.0000 norm=1.7840\n",
      "[iter 100] loss=1.0126 val_loss=0.0000 scale=0.5000 norm=0.9095\n",
      "[iter 0] loss=1.1657 val_loss=0.0000 scale=1.0000 norm=1.7812\n",
      "[iter 100] loss=1.0164 val_loss=0.0000 scale=0.5000 norm=0.9075\n",
      "[iter 0] loss=1.1675 val_loss=0.0000 scale=1.0000 norm=1.7785\n",
      "[iter 100] loss=1.0163 val_loss=0.0000 scale=0.5000 norm=0.9075\n",
      "[iter 0] loss=1.1684 val_loss=0.0000 scale=1.0000 norm=1.7767\n",
      "[iter 100] loss=1.0181 val_loss=0.0000 scale=1.0000 norm=1.8126\n",
      "[iter 0] loss=1.1690 val_loss=0.0000 scale=1.0000 norm=1.7755\n",
      "[iter 100] loss=1.0176 val_loss=0.0000 scale=0.5000 norm=0.9072\n",
      "[iter 0] loss=1.1683 val_loss=0.0000 scale=1.0000 norm=1.7766\n",
      "[iter 100] loss=1.0174 val_loss=0.0000 scale=0.5000 norm=0.9074\n",
      "[iter 0] loss=1.1671 val_loss=0.0000 scale=1.0000 norm=1.7786\n",
      "[iter 100] loss=1.0132 val_loss=0.0000 scale=0.5000 norm=0.9100\n",
      "[iter 0] loss=1.1685 val_loss=0.0000 scale=1.0000 norm=1.7761\n",
      "[iter 100] loss=1.0201 val_loss=0.0000 scale=0.5000 norm=0.9043\n",
      "[iter 0] loss=1.1693 val_loss=0.0000 scale=1.0000 norm=1.7744\n",
      "[iter 100] loss=1.0210 val_loss=0.0000 scale=0.5000 norm=0.9027\n",
      "[iter 0] loss=1.1690 val_loss=0.0000 scale=1.0000 norm=1.7749\n",
      "[iter 100] loss=1.0208 val_loss=0.0000 scale=0.5000 norm=0.9021\n",
      "[iter 0] loss=1.1693 val_loss=0.0000 scale=1.0000 norm=1.7743\n",
      "[iter 100] loss=1.0153 val_loss=0.0000 scale=1.0000 norm=1.8166\n",
      "[iter 0] loss=1.1700 val_loss=0.0000 scale=1.0000 norm=1.7732\n",
      "[iter 100] loss=1.0207 val_loss=0.0000 scale=0.5000 norm=0.9033\n",
      "[iter 0] loss=1.1697 val_loss=0.0000 scale=1.0000 norm=1.7735\n",
      "[iter 100] loss=1.0214 val_loss=0.0000 scale=1.0000 norm=1.8045\n",
      "[iter 0] loss=1.1715 val_loss=0.0000 scale=1.0000 norm=1.7705\n",
      "[iter 100] loss=1.0203 val_loss=0.0000 scale=0.5000 norm=0.9044\n",
      "[iter 0] loss=1.1715 val_loss=0.0000 scale=1.0000 norm=1.7705\n",
      "[iter 100] loss=1.0233 val_loss=0.0000 scale=1.0000 norm=1.8048\n",
      "[iter 0] loss=1.1719 val_loss=0.0000 scale=1.0000 norm=1.7699\n",
      "[iter 100] loss=1.0199 val_loss=0.0000 scale=1.0000 norm=1.8118\n",
      "[iter 0] loss=1.1732 val_loss=0.0000 scale=1.0000 norm=1.7674\n",
      "[iter 100] loss=1.0226 val_loss=0.0000 scale=1.0000 norm=1.8078\n",
      "[iter 0] loss=1.1743 val_loss=0.0000 scale=1.0000 norm=1.7651\n",
      "[iter 100] loss=1.0233 val_loss=0.0000 scale=1.0000 norm=1.8076\n",
      "[iter 0] loss=1.1738 val_loss=0.0000 scale=1.0000 norm=1.7661\n",
      "[iter 100] loss=1.0290 val_loss=0.0000 scale=0.5000 norm=0.8985\n",
      "[iter 0] loss=1.1739 val_loss=0.0000 scale=1.0000 norm=1.7660\n",
      "[iter 100] loss=1.0293 val_loss=0.0000 scale=1.0000 norm=1.7962\n",
      "[iter 0] loss=1.1757 val_loss=0.0000 scale=1.0000 norm=1.7629\n",
      "[iter 100] loss=1.0318 val_loss=0.0000 scale=1.0000 norm=1.7933\n",
      "[iter 0] loss=1.1757 val_loss=0.0000 scale=1.0000 norm=1.7627\n",
      "[iter 100] loss=1.0284 val_loss=0.0000 scale=1.0000 norm=1.8034\n",
      "[iter 0] loss=1.1764 val_loss=0.0000 scale=1.0000 norm=1.7615\n",
      "[iter 100] loss=1.0305 val_loss=0.0000 scale=1.0000 norm=1.7988\n",
      "[iter 0] loss=1.1778 val_loss=0.0000 scale=1.0000 norm=1.7590\n",
      "[iter 100] loss=1.0332 val_loss=0.0000 scale=1.0000 norm=1.7937\n",
      "[iter 0] loss=1.1776 val_loss=0.0000 scale=1.0000 norm=1.7591\n",
      "[iter 100] loss=1.0343 val_loss=0.0000 scale=0.5000 norm=0.8959\n",
      "[iter 0] loss=1.1790 val_loss=0.0000 scale=1.0000 norm=1.7567\n",
      "[iter 100] loss=1.0331 val_loss=0.0000 scale=0.5000 norm=0.8979\n",
      "[iter 0] loss=1.1790 val_loss=0.0000 scale=1.0000 norm=1.7568\n",
      "[iter 100] loss=1.0340 val_loss=0.0000 scale=1.0000 norm=1.7949\n",
      "[iter 0] loss=1.1801 val_loss=0.0000 scale=1.0000 norm=1.7547\n",
      "[iter 100] loss=1.0356 val_loss=0.0000 scale=0.5000 norm=0.8976\n",
      "[iter 0] loss=1.1820 val_loss=0.0000 scale=1.0000 norm=1.7524\n",
      "[iter 100] loss=1.0404 val_loss=0.0000 scale=0.5000 norm=0.8945\n",
      "[iter 0] loss=1.1822 val_loss=0.0000 scale=1.0000 norm=1.7522\n",
      "[iter 100] loss=1.0423 val_loss=0.0000 scale=1.0000 norm=1.7863\n",
      "[iter 0] loss=1.1841 val_loss=0.0000 scale=1.0000 norm=1.7499\n",
      "[iter 100] loss=1.0444 val_loss=0.0000 scale=1.0000 norm=1.7861\n",
      "[iter 0] loss=1.1841 val_loss=0.0000 scale=1.0000 norm=1.7500\n",
      "[iter 100] loss=1.0446 val_loss=0.0000 scale=1.0000 norm=1.7882\n",
      "[iter 0] loss=1.1859 val_loss=0.0000 scale=1.0000 norm=1.7473\n",
      "[iter 100] loss=1.0469 val_loss=0.0000 scale=1.0000 norm=1.7833\n",
      "[iter 0] loss=1.1867 val_loss=0.0000 scale=1.0000 norm=1.7478\n",
      "[iter 100] loss=1.0464 val_loss=0.0000 scale=0.5000 norm=0.8924\n",
      "[iter 0] loss=1.1867 val_loss=0.0000 scale=1.0000 norm=1.7479\n",
      "[iter 100] loss=1.0496 val_loss=0.0000 scale=1.0000 norm=1.7808\n",
      "[iter 0] loss=1.1862 val_loss=0.0000 scale=1.0000 norm=1.7488\n",
      "[iter 100] loss=1.0488 val_loss=0.0000 scale=1.0000 norm=1.7810\n",
      "[iter 0] loss=1.1877 val_loss=0.0000 scale=1.0000 norm=1.7472\n",
      "[iter 100] loss=1.0491 val_loss=0.0000 scale=0.5000 norm=0.8883\n",
      "[iter 0] loss=1.1875 val_loss=0.0000 scale=1.0000 norm=1.7493\n",
      "[iter 100] loss=1.0462 val_loss=0.0000 scale=1.0000 norm=1.7809\n",
      "[iter 0] loss=1.1879 val_loss=0.0000 scale=1.0000 norm=1.7486\n",
      "[iter 100] loss=1.0463 val_loss=0.0000 scale=1.0000 norm=1.7802\n",
      "[iter 0] loss=1.1895 val_loss=0.0000 scale=1.0000 norm=1.7471\n",
      "[iter 100] loss=1.0475 val_loss=0.0000 scale=1.0000 norm=1.7795\n",
      "[iter 0] loss=1.1895 val_loss=0.0000 scale=1.0000 norm=1.7471\n",
      "[iter 100] loss=1.0474 val_loss=0.0000 scale=0.5000 norm=0.8908\n",
      "[iter 0] loss=1.1891 val_loss=0.0000 scale=1.0000 norm=1.7471\n",
      "[iter 100] loss=1.0456 val_loss=0.0000 scale=1.0000 norm=1.7821\n",
      "[iter 0] loss=1.1888 val_loss=0.0000 scale=1.0000 norm=1.7474\n",
      "[iter 100] loss=1.0450 val_loss=0.0000 scale=0.5000 norm=0.8913\n",
      "[iter 0] loss=1.1904 val_loss=0.0000 scale=1.0000 norm=1.7450\n",
      "[iter 100] loss=1.0450 val_loss=0.0000 scale=1.0000 norm=1.7816\n",
      "[iter 0] loss=1.1902 val_loss=0.0000 scale=1.0000 norm=1.7453\n",
      "[iter 100] loss=1.0459 val_loss=0.0000 scale=1.0000 norm=1.7796\n",
      "[iter 0] loss=1.1913 val_loss=0.0000 scale=1.0000 norm=1.7436\n",
      "[iter 100] loss=1.0464 val_loss=0.0000 scale=1.0000 norm=1.7762\n",
      "[iter 0] loss=1.1917 val_loss=0.0000 scale=1.0000 norm=1.7449\n",
      "[iter 100] loss=1.0416 val_loss=0.0000 scale=0.5000 norm=0.8932\n",
      "[iter 0] loss=1.1918 val_loss=0.0000 scale=1.0000 norm=1.7447\n",
      "[iter 100] loss=1.0449 val_loss=0.0000 scale=1.0000 norm=1.7774\n",
      "[iter 0] loss=1.1916 val_loss=0.0000 scale=1.0000 norm=1.7450\n",
      "[iter 100] loss=1.0466 val_loss=0.0000 scale=0.5000 norm=0.8887\n",
      "[iter 0] loss=1.1919 val_loss=0.0000 scale=1.0000 norm=1.7447\n",
      "[iter 100] loss=1.0453 val_loss=0.0000 scale=1.0000 norm=1.7774\n",
      "[iter 0] loss=1.1926 val_loss=0.0000 scale=1.0000 norm=1.7431\n",
      "[iter 100] loss=1.0460 val_loss=0.0000 scale=1.0000 norm=1.7760\n",
      "[iter 0] loss=1.1924 val_loss=0.0000 scale=1.0000 norm=1.7439\n",
      "[iter 100] loss=1.0459 val_loss=0.0000 scale=1.0000 norm=1.7768\n",
      "[iter 0] loss=1.1933 val_loss=0.0000 scale=1.0000 norm=1.7424\n",
      "[iter 100] loss=1.0473 val_loss=0.0000 scale=0.5000 norm=0.8872\n",
      "[iter 0] loss=1.1950 val_loss=0.0000 scale=1.0000 norm=1.7396\n",
      "[iter 100] loss=1.0476 val_loss=0.0000 scale=0.5000 norm=0.8868\n",
      "[iter 0] loss=1.1942 val_loss=0.0000 scale=1.0000 norm=1.7414\n",
      "[iter 100] loss=1.0466 val_loss=0.0000 scale=1.0000 norm=1.7750\n",
      "[iter 0] loss=1.1947 val_loss=0.0000 scale=1.0000 norm=1.7402\n",
      "[iter 100] loss=1.0468 val_loss=0.0000 scale=1.0000 norm=1.7757\n",
      "[iter 0] loss=1.1945 val_loss=0.0000 scale=1.0000 norm=1.7403\n",
      "[iter 100] loss=1.0460 val_loss=0.0000 scale=1.0000 norm=1.7754\n",
      "[iter 0] loss=1.1958 val_loss=0.0000 scale=1.0000 norm=1.7381\n",
      "[iter 100] loss=1.0471 val_loss=0.0000 scale=0.5000 norm=0.8869\n",
      "[iter 0] loss=1.1956 val_loss=0.0000 scale=1.0000 norm=1.7382\n",
      "[iter 100] loss=1.0470 val_loss=0.0000 scale=1.0000 norm=1.7740\n",
      "[iter 0] loss=1.1955 val_loss=0.0000 scale=1.0000 norm=1.7384\n",
      "[iter 100] loss=1.0466 val_loss=0.0000 scale=0.5000 norm=0.8874\n",
      "[iter 0] loss=1.1965 val_loss=0.0000 scale=1.0000 norm=1.7365\n",
      "[iter 100] loss=1.0467 val_loss=0.0000 scale=0.5000 norm=0.8876\n",
      "[iter 0] loss=1.1985 val_loss=0.0000 scale=1.0000 norm=1.7338\n",
      "[iter 100] loss=1.0494 val_loss=0.0000 scale=1.0000 norm=1.7704\n",
      "[iter 0] loss=1.2004 val_loss=0.0000 scale=1.0000 norm=1.7316\n",
      "[iter 100] loss=1.0499 val_loss=0.0000 scale=0.5000 norm=0.8846\n",
      "[iter 0] loss=1.2016 val_loss=0.0000 scale=1.0000 norm=1.7312\n",
      "[iter 100] loss=1.0523 val_loss=0.0000 scale=0.5000 norm=0.8842\n",
      "[iter 0] loss=1.2018 val_loss=0.0000 scale=1.0000 norm=1.7308\n",
      "[iter 100] loss=1.0544 val_loss=0.0000 scale=0.5000 norm=0.8828\n",
      "[iter 0] loss=1.2028 val_loss=0.0000 scale=1.0000 norm=1.7289\n",
      "[iter 100] loss=1.0548 val_loss=0.0000 scale=1.0000 norm=1.7651\n",
      "[iter 0] loss=1.2041 val_loss=0.0000 scale=1.0000 norm=1.7280\n",
      "[iter 100] loss=1.0544 val_loss=0.0000 scale=1.0000 norm=1.7661\n",
      "[iter 0] loss=1.2041 val_loss=0.0000 scale=1.0000 norm=1.7280\n",
      "[iter 100] loss=1.0565 val_loss=0.0000 scale=1.0000 norm=1.7640\n",
      "[iter 0] loss=1.2053 val_loss=0.0000 scale=1.0000 norm=1.7262\n",
      "[iter 100] loss=1.0564 val_loss=0.0000 scale=1.0000 norm=1.7630\n",
      "[iter 0] loss=1.2053 val_loss=0.0000 scale=1.0000 norm=1.7261\n",
      "[iter 100] loss=1.0569 val_loss=0.0000 scale=1.0000 norm=1.7625\n",
      "[iter 0] loss=1.2057 val_loss=0.0000 scale=1.0000 norm=1.7255\n",
      "[iter 100] loss=1.0568 val_loss=0.0000 scale=1.0000 norm=1.7628\n",
      "[iter 0] loss=1.2065 val_loss=0.0000 scale=1.0000 norm=1.7250\n",
      "[iter 100] loss=1.0547 val_loss=0.0000 scale=1.0000 norm=1.7682\n",
      "[iter 0] loss=1.2067 val_loss=0.0000 scale=1.0000 norm=1.7248\n",
      "[iter 100] loss=1.0542 val_loss=0.0000 scale=1.0000 norm=1.7695\n",
      "[iter 0] loss=1.2085 val_loss=0.0000 scale=1.0000 norm=1.7218\n",
      "[iter 100] loss=1.0574 val_loss=0.0000 scale=1.0000 norm=1.7633\n",
      "[iter 0] loss=1.2084 val_loss=0.0000 scale=1.0000 norm=1.7220\n",
      "[iter 100] loss=1.0588 val_loss=0.0000 scale=1.0000 norm=1.7628\n",
      "[iter 0] loss=1.2089 val_loss=0.0000 scale=1.0000 norm=1.7209\n",
      "[iter 100] loss=1.0606 val_loss=0.0000 scale=1.0000 norm=1.7584\n",
      "[iter 0] loss=1.2094 val_loss=0.0000 scale=1.0000 norm=1.7198\n",
      "[iter 100] loss=1.0605 val_loss=0.0000 scale=1.0000 norm=1.7591\n",
      "[iter 0] loss=1.2091 val_loss=0.0000 scale=1.0000 norm=1.7204\n",
      "[iter 100] loss=1.0614 val_loss=0.0000 scale=0.5000 norm=0.8787\n",
      "[iter 0] loss=1.2091 val_loss=0.0000 scale=1.0000 norm=1.7204\n",
      "[iter 100] loss=1.0621 val_loss=0.0000 scale=1.0000 norm=1.7575\n",
      "[iter 0] loss=1.2098 val_loss=0.0000 scale=1.0000 norm=1.7191\n",
      "[iter 100] loss=1.0629 val_loss=0.0000 scale=1.0000 norm=1.7545\n",
      "[iter 0] loss=1.2098 val_loss=0.0000 scale=1.0000 norm=1.7191\n",
      "[iter 100] loss=1.0633 val_loss=0.0000 scale=0.5000 norm=0.8771\n",
      "[iter 0] loss=1.2108 val_loss=0.0000 scale=1.0000 norm=1.7169\n",
      "[iter 100] loss=1.0659 val_loss=0.0000 scale=1.0000 norm=1.7516\n",
      "[iter 0] loss=1.2117 val_loss=0.0000 scale=1.0000 norm=1.7154\n",
      "[iter 100] loss=1.0681 val_loss=0.0000 scale=0.5000 norm=0.8737\n",
      "[iter 0] loss=1.2119 val_loss=0.0000 scale=1.0000 norm=1.7150\n",
      "[iter 100] loss=1.0681 val_loss=0.0000 scale=1.0000 norm=1.7475\n",
      "[iter 0] loss=1.2121 val_loss=0.0000 scale=1.0000 norm=1.7147\n",
      "[iter 100] loss=1.0693 val_loss=0.0000 scale=0.5000 norm=0.8740\n",
      "[iter 0] loss=1.2133 val_loss=0.0000 scale=1.0000 norm=1.7125\n",
      "[iter 100] loss=1.0706 val_loss=0.0000 scale=1.0000 norm=1.7438\n",
      "[iter 0] loss=1.2135 val_loss=0.0000 scale=1.0000 norm=1.7121\n",
      "[iter 100] loss=1.0668 val_loss=0.0000 scale=1.0000 norm=1.7478\n",
      "[iter 0] loss=1.2151 val_loss=0.0000 scale=1.0000 norm=1.7097\n",
      "[iter 100] loss=1.0678 val_loss=0.0000 scale=1.0000 norm=1.7470\n",
      "[iter 0] loss=1.2148 val_loss=0.0000 scale=1.0000 norm=1.7104\n",
      "[iter 100] loss=1.0682 val_loss=0.0000 scale=0.5000 norm=0.8745\n",
      "[iter 0] loss=1.2153 val_loss=0.0000 scale=1.0000 norm=1.7094\n",
      "[iter 100] loss=1.0693 val_loss=0.0000 scale=1.0000 norm=1.7461\n",
      "[iter 0] loss=1.2159 val_loss=0.0000 scale=1.0000 norm=1.7084\n",
      "[iter 100] loss=1.0683 val_loss=0.0000 scale=1.0000 norm=1.7447\n",
      "[iter 0] loss=1.2158 val_loss=0.0000 scale=1.0000 norm=1.7084\n",
      "[iter 100] loss=1.0696 val_loss=0.0000 scale=1.0000 norm=1.7444\n",
      "[iter 0] loss=1.2165 val_loss=0.0000 scale=1.0000 norm=1.7069\n",
      "[iter 100] loss=1.0706 val_loss=0.0000 scale=1.0000 norm=1.7424\n",
      "[iter 0] loss=1.2179 val_loss=0.0000 scale=1.0000 norm=1.7050\n",
      "[iter 100] loss=1.0731 val_loss=0.0000 scale=1.0000 norm=1.7379\n",
      "[iter 0] loss=1.2186 val_loss=0.0000 scale=1.0000 norm=1.7037\n",
      "[iter 100] loss=1.0755 val_loss=0.0000 scale=1.0000 norm=1.7368\n",
      "[iter 0] loss=1.2194 val_loss=0.0000 scale=1.0000 norm=1.7023\n",
      "[iter 100] loss=1.0773 val_loss=0.0000 scale=1.0000 norm=1.7342\n",
      "[iter 0] loss=1.2186 val_loss=0.0000 scale=1.0000 norm=1.7039\n",
      "[iter 100] loss=1.0748 val_loss=0.0000 scale=1.0000 norm=1.7327\n",
      "[iter 0] loss=1.2183 val_loss=0.0000 scale=1.0000 norm=1.7045\n",
      "[iter 100] loss=1.0772 val_loss=0.0000 scale=0.5000 norm=0.8659\n",
      "[iter 0] loss=1.2188 val_loss=0.0000 scale=1.0000 norm=1.7035\n",
      "[iter 100] loss=1.0779 val_loss=0.0000 scale=1.0000 norm=1.7296\n",
      "[iter 0] loss=1.2170 val_loss=0.0000 scale=1.0000 norm=1.7059\n",
      "[iter 100] loss=1.0742 val_loss=0.0000 scale=1.0000 norm=1.7307\n",
      "[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=1.7072\n",
      "[iter 100] loss=1.0730 val_loss=0.0000 scale=1.0000 norm=1.7311\n",
      "[iter 0] loss=1.2163 val_loss=0.0000 scale=1.0000 norm=1.7070\n",
      "[iter 100] loss=1.0726 val_loss=0.0000 scale=0.5000 norm=0.8661\n",
      "[iter 0] loss=1.2152 val_loss=0.0000 scale=1.0000 norm=1.7089\n",
      "[iter 100] loss=1.0712 val_loss=0.0000 scale=1.0000 norm=1.7357\n",
      "[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=1.7073\n",
      "[iter 100] loss=1.0711 val_loss=0.0000 scale=0.5000 norm=0.8672\n",
      "[iter 0] loss=1.2151 val_loss=0.0000 scale=1.0000 norm=1.7095\n",
      "[iter 100] loss=1.0668 val_loss=0.0000 scale=0.5000 norm=0.8699\n",
      "[iter 0] loss=1.2153 val_loss=0.0000 scale=1.0000 norm=1.7091\n",
      "[iter 100] loss=1.0678 val_loss=0.0000 scale=0.5000 norm=0.8692\n",
      "[iter 0] loss=1.2134 val_loss=0.0000 scale=1.0000 norm=1.7114\n",
      "[iter 100] loss=1.0678 val_loss=0.0000 scale=1.0000 norm=1.7371\n",
      "[iter 0] loss=1.2134 val_loss=0.0000 scale=1.0000 norm=1.7113\n",
      "[iter 100] loss=1.0683 val_loss=0.0000 scale=1.0000 norm=1.7395\n",
      "[iter 0] loss=1.2130 val_loss=0.0000 scale=1.0000 norm=1.7121\n",
      "[iter 100] loss=1.0664 val_loss=0.0000 scale=1.0000 norm=1.7407\n",
      "[iter 0] loss=1.2123 val_loss=0.0000 scale=1.0000 norm=1.7135\n",
      "[iter 100] loss=1.0636 val_loss=0.0000 scale=1.0000 norm=1.7446\n",
      "[iter 0] loss=1.2128 val_loss=0.0000 scale=1.0000 norm=1.7125\n",
      "[iter 100] loss=1.0649 val_loss=0.0000 scale=1.0000 norm=1.7390\n",
      "[iter 0] loss=1.2144 val_loss=0.0000 scale=1.0000 norm=1.7102\n",
      "[iter 100] loss=1.0680 val_loss=0.0000 scale=0.5000 norm=0.8686\n",
      "[iter 0] loss=1.2144 val_loss=0.0000 scale=1.0000 norm=1.7100\n",
      "[iter 100] loss=1.0677 val_loss=0.0000 scale=1.0000 norm=1.7407\n",
      "[iter 0] loss=1.2125 val_loss=0.0000 scale=1.0000 norm=1.7128\n",
      "[iter 100] loss=1.0672 val_loss=0.0000 scale=1.0000 norm=1.7402\n",
      "[iter 0] loss=1.2128 val_loss=0.0000 scale=1.0000 norm=1.7120\n",
      "[iter 100] loss=1.0693 val_loss=0.0000 scale=1.0000 norm=1.7382\n",
      "[iter 0] loss=1.2143 val_loss=0.0000 scale=1.0000 norm=1.7109\n",
      "[iter 100] loss=1.0714 val_loss=0.0000 scale=1.0000 norm=1.7362\n",
      "[iter 0] loss=1.2146 val_loss=0.0000 scale=1.0000 norm=1.7100\n",
      "[iter 100] loss=1.0711 val_loss=0.0000 scale=1.0000 norm=1.7354\n",
      "[iter 0] loss=1.2148 val_loss=0.0000 scale=1.0000 norm=1.7097\n",
      "[iter 100] loss=1.0690 val_loss=0.0000 scale=1.0000 norm=1.7404\n",
      "[iter 0] loss=1.2155 val_loss=0.0000 scale=1.0000 norm=1.7086\n",
      "[iter 100] loss=1.0694 val_loss=0.0000 scale=1.0000 norm=1.7387\n",
      "[iter 0] loss=1.2149 val_loss=0.0000 scale=1.0000 norm=1.7094\n",
      "[iter 100] loss=1.0694 val_loss=0.0000 scale=1.0000 norm=1.7391\n",
      "[iter 0] loss=1.2153 val_loss=0.0000 scale=1.0000 norm=1.7087\n",
      "[iter 100] loss=1.0707 val_loss=0.0000 scale=1.0000 norm=1.7358\n",
      "[iter 0] loss=1.2147 val_loss=0.0000 scale=1.0000 norm=1.7099\n",
      "[iter 100] loss=1.0706 val_loss=0.0000 scale=1.0000 norm=1.7370\n",
      "[iter 0] loss=1.2160 val_loss=0.0000 scale=1.0000 norm=1.7075\n",
      "[iter 100] loss=1.0708 val_loss=0.0000 scale=1.0000 norm=1.7356\n",
      "[iter 0] loss=1.2163 val_loss=0.0000 scale=1.0000 norm=1.7072\n",
      "[iter 100] loss=1.0710 val_loss=0.0000 scale=0.5000 norm=0.8671\n",
      "[iter 0] loss=1.2173 val_loss=0.0000 scale=1.0000 norm=1.7054\n",
      "[iter 100] loss=1.0721 val_loss=0.0000 scale=1.0000 norm=1.7314\n",
      "[iter 0] loss=1.2176 val_loss=0.0000 scale=1.0000 norm=1.7046\n",
      "[iter 100] loss=1.0718 val_loss=0.0000 scale=1.0000 norm=1.7307\n",
      "[iter 0] loss=1.2183 val_loss=0.0000 scale=1.0000 norm=1.7053\n",
      "[iter 100] loss=1.0707 val_loss=0.0000 scale=1.0000 norm=1.7335\n",
      "[iter 0] loss=1.2182 val_loss=0.0000 scale=1.0000 norm=1.7054\n",
      "[iter 100] loss=1.0719 val_loss=0.0000 scale=0.5000 norm=0.8663\n",
      "[iter 0] loss=1.2191 val_loss=0.0000 scale=1.0000 norm=1.7038\n",
      "[iter 100] loss=1.0718 val_loss=0.0000 scale=1.0000 norm=1.7322\n",
      "[iter 0] loss=1.2197 val_loss=0.0000 scale=1.0000 norm=1.7026\n",
      "[iter 100] loss=1.0734 val_loss=0.0000 scale=1.0000 norm=1.7293\n",
      "[iter 0] loss=1.2199 val_loss=0.0000 scale=1.0000 norm=1.7024\n",
      "[iter 100] loss=1.0731 val_loss=0.0000 scale=1.0000 norm=1.7301\n",
      "[iter 0] loss=1.2209 val_loss=0.0000 scale=1.0000 norm=1.7005\n",
      "[iter 100] loss=1.0747 val_loss=0.0000 scale=0.5000 norm=0.8644\n",
      "[iter 0] loss=1.2221 val_loss=0.0000 scale=1.0000 norm=1.6983\n",
      "[iter 100] loss=1.0760 val_loss=0.0000 scale=1.0000 norm=1.7254\n",
      "[iter 0] loss=1.2228 val_loss=0.0000 scale=1.0000 norm=1.6970\n",
      "[iter 100] loss=1.0765 val_loss=0.0000 scale=1.0000 norm=1.7253\n",
      "[iter 0] loss=1.2237 val_loss=0.0000 scale=1.0000 norm=1.6956\n",
      "[iter 100] loss=1.0774 val_loss=0.0000 scale=0.5000 norm=0.8612\n",
      "[iter 0] loss=1.2244 val_loss=0.0000 scale=1.0000 norm=1.6940\n",
      "[iter 100] loss=1.0771 val_loss=0.0000 scale=1.0000 norm=1.7203\n",
      "[iter 0] loss=1.2244 val_loss=0.0000 scale=1.0000 norm=1.6942\n",
      "[iter 100] loss=1.0773 val_loss=0.0000 scale=1.0000 norm=1.7212\n",
      "[iter 0] loss=1.2254 val_loss=0.0000 scale=1.0000 norm=1.6925\n",
      "[iter 100] loss=1.0792 val_loss=0.0000 scale=1.0000 norm=1.7181\n",
      "[iter 0] loss=1.2271 val_loss=0.0000 scale=1.0000 norm=1.6898\n",
      "[iter 100] loss=1.0819 val_loss=0.0000 scale=1.0000 norm=1.7133\n",
      "[iter 0] loss=1.2273 val_loss=0.0000 scale=1.0000 norm=1.6894\n",
      "[iter 100] loss=1.0808 val_loss=0.0000 scale=1.0000 norm=1.7132\n",
      "[iter 0] loss=1.2274 val_loss=0.0000 scale=1.0000 norm=1.6898\n",
      "[iter 100] loss=1.0812 val_loss=0.0000 scale=1.0000 norm=1.7142\n",
      "[iter 0] loss=1.2287 val_loss=0.0000 scale=1.0000 norm=1.6881\n",
      "[iter 100] loss=1.0806 val_loss=0.0000 scale=1.0000 norm=1.7137\n",
      "[iter 0] loss=1.2278 val_loss=0.0000 scale=1.0000 norm=1.6896\n",
      "[iter 100] loss=1.0792 val_loss=0.0000 scale=0.5000 norm=0.8575\n",
      "[iter 0] loss=1.2284 val_loss=0.0000 scale=1.0000 norm=1.6885\n",
      "[iter 100] loss=1.0816 val_loss=0.0000 scale=0.5000 norm=0.8551\n",
      "[iter 0] loss=1.2289 val_loss=0.0000 scale=1.0000 norm=1.6886\n",
      "[iter 100] loss=1.0813 val_loss=0.0000 scale=0.5000 norm=0.8549\n",
      "[iter 0] loss=1.2291 val_loss=0.0000 scale=1.0000 norm=1.6882\n",
      "[iter 100] loss=1.0811 val_loss=0.0000 scale=1.0000 norm=1.7111\n",
      "[iter 0] loss=1.2293 val_loss=0.0000 scale=1.0000 norm=1.6877\n",
      "[iter 100] loss=1.0830 val_loss=0.0000 scale=0.5000 norm=0.8540\n",
      "[iter 0] loss=1.2289 val_loss=0.0000 scale=1.0000 norm=1.6886\n",
      "[iter 100] loss=1.0805 val_loss=0.0000 scale=1.0000 norm=1.7120\n",
      "[iter 0] loss=1.2304 val_loss=0.0000 scale=1.0000 norm=1.6862\n",
      "[iter 100] loss=1.0816 val_loss=0.0000 scale=1.0000 norm=1.7099\n",
      "[iter 0] loss=1.2318 val_loss=0.0000 scale=1.0000 norm=1.6843\n",
      "[iter 100] loss=1.0791 val_loss=0.0000 scale=1.0000 norm=1.7139\n",
      "[iter 0] loss=1.2333 val_loss=0.0000 scale=1.0000 norm=1.6821\n",
      "[iter 100] loss=1.0801 val_loss=0.0000 scale=0.5000 norm=0.8566\n",
      "[iter 0] loss=1.2339 val_loss=0.0000 scale=1.0000 norm=1.6809\n",
      "[iter 100] loss=1.0838 val_loss=0.0000 scale=0.5000 norm=0.8534\n",
      "[iter 0] loss=1.2342 val_loss=0.0000 scale=1.0000 norm=1.6804\n",
      "[iter 100] loss=1.0830 val_loss=0.0000 scale=0.5000 norm=0.8543\n",
      "[iter 0] loss=1.2345 val_loss=0.0000 scale=1.0000 norm=1.6797\n",
      "[iter 100] loss=1.0858 val_loss=0.0000 scale=1.0000 norm=1.7043\n",
      "[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=1.6777\n",
      "[iter 100] loss=1.0882 val_loss=0.0000 scale=0.5000 norm=0.8506\n",
      "[iter 0] loss=1.2365 val_loss=0.0000 scale=1.0000 norm=1.6765\n",
      "[iter 100] loss=1.0873 val_loss=0.0000 scale=0.5000 norm=0.8517\n",
      "[iter 0] loss=1.2376 val_loss=0.0000 scale=1.0000 norm=1.6745\n",
      "[iter 100] loss=1.0897 val_loss=0.0000 scale=1.0000 norm=1.7000\n",
      "[iter 0] loss=1.2381 val_loss=0.0000 scale=1.0000 norm=1.6735\n",
      "[iter 100] loss=1.0919 val_loss=0.0000 scale=0.5000 norm=0.8485\n",
      "[iter 0] loss=1.2383 val_loss=0.0000 scale=1.0000 norm=1.6730\n",
      "[iter 100] loss=1.0923 val_loss=0.0000 scale=1.0000 norm=1.6961\n",
      "[iter 0] loss=1.2398 val_loss=0.0000 scale=1.0000 norm=1.6705\n",
      "[iter 100] loss=1.0935 val_loss=0.0000 scale=1.0000 norm=1.6939\n",
      "[iter 0] loss=1.2403 val_loss=0.0000 scale=1.0000 norm=1.6698\n",
      "[iter 100] loss=1.0948 val_loss=0.0000 scale=0.5000 norm=0.8457\n",
      "[iter 0] loss=1.2404 val_loss=0.0000 scale=1.0000 norm=1.6695\n",
      "[iter 100] loss=1.0968 val_loss=0.0000 scale=1.0000 norm=1.6909\n",
      "[iter 0] loss=1.2417 val_loss=0.0000 scale=1.0000 norm=1.6680\n",
      "[iter 100] loss=1.0991 val_loss=0.0000 scale=1.0000 norm=1.6869\n",
      "[iter 0] loss=1.2425 val_loss=0.0000 scale=1.0000 norm=1.6664\n",
      "[iter 100] loss=1.0986 val_loss=0.0000 scale=0.5000 norm=0.8443\n",
      "[iter 0] loss=1.2433 val_loss=0.0000 scale=1.0000 norm=1.6650\n",
      "[iter 100] loss=1.1007 val_loss=0.0000 scale=1.0000 norm=1.6856\n",
      "[iter 0] loss=1.2437 val_loss=0.0000 scale=1.0000 norm=1.6640\n",
      "[iter 100] loss=1.1023 val_loss=0.0000 scale=1.0000 norm=1.6830\n",
      "[iter 0] loss=1.2439 val_loss=0.0000 scale=1.0000 norm=1.6637\n",
      "[iter 100] loss=1.1029 val_loss=0.0000 scale=1.0000 norm=1.6835\n",
      "[iter 0] loss=1.2455 val_loss=0.0000 scale=1.0000 norm=1.6612\n",
      "[iter 100] loss=1.1030 val_loss=0.0000 scale=1.0000 norm=1.6830\n",
      "[iter 0] loss=1.2444 val_loss=0.0000 scale=1.0000 norm=1.6644\n",
      "[iter 100] loss=1.1027 val_loss=0.0000 scale=0.5000 norm=0.8427\n",
      "[iter 0] loss=1.2449 val_loss=0.0000 scale=1.0000 norm=1.6636\n",
      "[iter 100] loss=1.1035 val_loss=0.0000 scale=1.0000 norm=1.6856\n",
      "[iter 0] loss=1.2455 val_loss=0.0000 scale=1.0000 norm=1.6624\n",
      "[iter 100] loss=1.1049 val_loss=0.0000 scale=0.5000 norm=0.8416\n",
      "[iter 0] loss=1.2459 val_loss=0.0000 scale=1.0000 norm=1.6618\n",
      "[iter 100] loss=1.1057 val_loss=0.0000 scale=1.0000 norm=1.6835\n",
      "[iter 0] loss=1.2462 val_loss=0.0000 scale=1.0000 norm=1.6611\n",
      "[iter 100] loss=1.1046 val_loss=0.0000 scale=1.0000 norm=1.6826\n",
      "[iter 0] loss=1.2463 val_loss=0.0000 scale=1.0000 norm=1.6609\n",
      "[iter 100] loss=1.1050 val_loss=0.0000 scale=1.0000 norm=1.6831\n",
      "[iter 0] loss=1.2464 val_loss=0.0000 scale=1.0000 norm=1.6608\n",
      "[iter 100] loss=1.1069 val_loss=0.0000 scale=0.5000 norm=0.8409\n",
      "[iter 0] loss=1.2464 val_loss=0.0000 scale=1.0000 norm=1.6608\n",
      "[iter 100] loss=1.1114 val_loss=0.0000 scale=0.5000 norm=0.8378\n",
      "[iter 0] loss=1.2471 val_loss=0.0000 scale=1.0000 norm=1.6594\n",
      "[iter 100] loss=1.1098 val_loss=0.0000 scale=0.5000 norm=0.8378\n",
      "[iter 0] loss=1.2472 val_loss=0.0000 scale=1.0000 norm=1.6591\n",
      "[iter 100] loss=1.1085 val_loss=0.0000 scale=1.0000 norm=1.6780\n",
      "[iter 0] loss=1.2470 val_loss=0.0000 scale=1.0000 norm=1.6597\n",
      "[iter 100] loss=1.1076 val_loss=0.0000 scale=1.0000 norm=1.6803\n",
      "[iter 0] loss=1.2482 val_loss=0.0000 scale=1.0000 norm=1.6572\n",
      "[iter 100] loss=1.1090 val_loss=0.0000 scale=1.0000 norm=1.6789\n",
      "[iter 0] loss=1.2482 val_loss=0.0000 scale=1.0000 norm=1.6572\n",
      "[iter 100] loss=1.1109 val_loss=0.0000 scale=0.5000 norm=0.8393\n",
      "[iter 0] loss=1.2496 val_loss=0.0000 scale=1.0000 norm=1.6558\n",
      "[iter 100] loss=1.1142 val_loss=0.0000 scale=1.0000 norm=1.6741\n",
      "[iter 0] loss=1.2490 val_loss=0.0000 scale=1.0000 norm=1.6567\n",
      "[iter 100] loss=1.1129 val_loss=0.0000 scale=0.5000 norm=0.8376\n",
      "[iter 0] loss=1.2486 val_loss=0.0000 scale=1.0000 norm=1.6575\n",
      "[iter 100] loss=1.1138 val_loss=0.0000 scale=1.0000 norm=1.6783\n",
      "[iter 0] loss=1.2499 val_loss=0.0000 scale=1.0000 norm=1.6553\n",
      "[iter 100] loss=1.1166 val_loss=0.0000 scale=0.5000 norm=0.8371\n",
      "[iter 0] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.6531\n",
      "[iter 100] loss=1.1195 val_loss=0.0000 scale=0.5000 norm=0.8349\n",
      "[iter 0] loss=1.2507 val_loss=0.0000 scale=1.0000 norm=1.6539\n",
      "[iter 100] loss=1.1190 val_loss=0.0000 scale=0.5000 norm=0.8353\n",
      "[iter 0] loss=1.2508 val_loss=0.0000 scale=1.0000 norm=1.6537\n",
      "[iter 100] loss=1.1191 val_loss=0.0000 scale=0.5000 norm=0.8354\n",
      "[iter 0] loss=1.2513 val_loss=0.0000 scale=1.0000 norm=1.6527\n",
      "[iter 100] loss=1.1183 val_loss=0.0000 scale=0.5000 norm=0.8357\n",
      "[iter 0] loss=1.2521 val_loss=0.0000 scale=1.0000 norm=1.6511\n",
      "[iter 100] loss=1.1181 val_loss=0.0000 scale=1.0000 norm=1.6690\n",
      "[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=1.6515\n",
      "[iter 100] loss=1.1167 val_loss=0.0000 scale=1.0000 norm=1.6700\n",
      "[iter 0] loss=1.2523 val_loss=0.0000 scale=1.0000 norm=1.6507\n",
      "[iter 100] loss=1.1180 val_loss=0.0000 scale=1.0000 norm=1.6687\n",
      "[iter 0] loss=1.2538 val_loss=0.0000 scale=1.0000 norm=1.6486\n",
      "[iter 100] loss=1.1204 val_loss=0.0000 scale=1.0000 norm=1.6657\n",
      "[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=1.6470\n",
      "[iter 100] loss=1.1222 val_loss=0.0000 scale=1.0000 norm=1.6633\n",
      "[iter 0] loss=1.2552 val_loss=0.0000 scale=1.0000 norm=1.6459\n",
      "[iter 100] loss=1.1228 val_loss=0.0000 scale=1.0000 norm=1.6624\n",
      "[iter 0] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.6462\n",
      "[iter 100] loss=1.1237 val_loss=0.0000 scale=1.0000 norm=1.6618\n",
      "[iter 0] loss=1.2559 val_loss=0.0000 scale=1.0000 norm=1.6446\n",
      "[iter 100] loss=1.1239 val_loss=0.0000 scale=1.0000 norm=1.6611\n",
      "[iter 0] loss=1.2570 val_loss=0.0000 scale=1.0000 norm=1.6427\n",
      "[iter 100] loss=1.1246 val_loss=0.0000 scale=1.0000 norm=1.6591\n",
      "[iter 0] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.6424\n",
      "[iter 100] loss=1.1258 val_loss=0.0000 scale=0.5000 norm=0.8296\n",
      "[iter 0] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.6422\n",
      "[iter 100] loss=1.1266 val_loss=0.0000 scale=1.0000 norm=1.6585\n",
      "[iter 0] loss=1.2581 val_loss=0.0000 scale=1.0000 norm=1.6406\n",
      "[iter 100] loss=1.1279 val_loss=0.0000 scale=1.0000 norm=1.6567\n",
      "[iter 0] loss=1.2589 val_loss=0.0000 scale=1.0000 norm=1.6393\n",
      "[iter 100] loss=1.1283 val_loss=0.0000 scale=0.5000 norm=0.8274\n",
      "[iter 0] loss=1.2589 val_loss=0.0000 scale=1.0000 norm=1.6391\n",
      "[iter 100] loss=1.1296 val_loss=0.0000 scale=1.0000 norm=1.6537\n",
      "[iter 0] loss=1.2588 val_loss=0.0000 scale=1.0000 norm=1.6393\n",
      "[iter 100] loss=1.1279 val_loss=0.0000 scale=0.5000 norm=0.8278\n",
      "[iter 0] loss=1.2586 val_loss=0.0000 scale=1.0000 norm=1.6397\n",
      "[iter 100] loss=1.1287 val_loss=0.0000 scale=1.0000 norm=1.6525\n",
      "[iter 0] loss=1.2594 val_loss=0.0000 scale=1.0000 norm=1.6381\n",
      "[iter 100] loss=1.1307 val_loss=0.0000 scale=0.5000 norm=0.8250\n",
      "[iter 0] loss=1.2607 val_loss=0.0000 scale=1.0000 norm=1.6362\n",
      "[iter 100] loss=1.1324 val_loss=0.0000 scale=1.0000 norm=1.6496\n",
      "[iter 0] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.6362\n",
      "[iter 100] loss=1.1330 val_loss=0.0000 scale=1.0000 norm=1.6501\n",
      "[iter 0] loss=1.2607 val_loss=0.0000 scale=1.0000 norm=1.6362\n",
      "[iter 100] loss=1.1313 val_loss=0.0000 scale=1.0000 norm=1.6519\n",
      "[iter 0] loss=1.2612 val_loss=0.0000 scale=1.0000 norm=1.6351\n",
      "[iter 100] loss=1.1342 val_loss=0.0000 scale=1.0000 norm=1.6492\n",
      "[iter 0] loss=1.2611 val_loss=0.0000 scale=1.0000 norm=1.6355\n",
      "[iter 100] loss=1.1345 val_loss=0.0000 scale=1.0000 norm=1.6481\n",
      "[iter 0] loss=1.2611 val_loss=0.0000 scale=1.0000 norm=1.6354\n",
      "[iter 100] loss=1.1352 val_loss=0.0000 scale=1.0000 norm=1.6497\n",
      "[iter 0] loss=1.2611 val_loss=0.0000 scale=1.0000 norm=1.6352\n",
      "[iter 100] loss=1.1346 val_loss=0.0000 scale=1.0000 norm=1.6503\n",
      "[iter 0] loss=1.2620 val_loss=0.0000 scale=1.0000 norm=1.6338\n",
      "[iter 100] loss=1.1372 val_loss=0.0000 scale=1.0000 norm=1.6479\n",
      "[iter 0] loss=1.2624 val_loss=0.0000 scale=1.0000 norm=1.6330\n",
      "[iter 100] loss=1.1376 val_loss=0.0000 scale=1.0000 norm=1.6468\n",
      "[iter 0] loss=1.2635 val_loss=0.0000 scale=1.0000 norm=1.6313\n",
      "[iter 100] loss=1.1402 val_loss=0.0000 scale=1.0000 norm=1.6450\n",
      "[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.6300\n",
      "[iter 100] loss=1.1421 val_loss=0.0000 scale=1.0000 norm=1.6427\n",
      "[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.6299\n",
      "[iter 100] loss=1.1418 val_loss=0.0000 scale=1.0000 norm=1.6437\n",
      "[iter 0] loss=1.2635 val_loss=0.0000 scale=1.0000 norm=1.6311\n",
      "[iter 100] loss=1.1374 val_loss=0.0000 scale=1.0000 norm=1.6451\n",
      "[iter 0] loss=1.2629 val_loss=0.0000 scale=1.0000 norm=1.6321\n",
      "[iter 100] loss=1.1356 val_loss=0.0000 scale=1.0000 norm=1.6466\n",
      "[iter 0] loss=1.2624 val_loss=0.0000 scale=1.0000 norm=1.6330\n",
      "[iter 100] loss=1.1352 val_loss=0.0000 scale=1.0000 norm=1.6463\n",
      "[iter 0] loss=1.2633 val_loss=0.0000 scale=1.0000 norm=1.6313\n",
      "[iter 100] loss=1.1361 val_loss=0.0000 scale=0.5000 norm=0.8224\n",
      "[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.6299\n",
      "[iter 100] loss=1.1373 val_loss=0.0000 scale=1.0000 norm=1.6430\n",
      "[iter 0] loss=1.2633 val_loss=0.0000 scale=1.0000 norm=1.6312\n",
      "[iter 100] loss=1.1383 val_loss=0.0000 scale=1.0000 norm=1.6451\n",
      "[iter 0] loss=1.2639 val_loss=0.0000 scale=1.0000 norm=1.6301\n",
      "[iter 100] loss=1.1398 val_loss=0.0000 scale=1.0000 norm=1.6442\n",
      "[iter 0] loss=1.2638 val_loss=0.0000 scale=1.0000 norm=1.6304\n",
      "[iter 100] loss=1.1367 val_loss=0.0000 scale=1.0000 norm=1.6456\n",
      "[iter 0] loss=1.2635 val_loss=0.0000 scale=1.0000 norm=1.6307\n",
      "[iter 100] loss=1.1361 val_loss=0.0000 scale=1.0000 norm=1.6445\n",
      "[iter 0] loss=1.2642 val_loss=0.0000 scale=1.0000 norm=1.6296\n",
      "[iter 100] loss=1.1359 val_loss=0.0000 scale=1.0000 norm=1.6449\n",
      "[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=1.6284\n",
      "[iter 100] loss=1.1371 val_loss=0.0000 scale=1.0000 norm=1.6431\n",
      "[iter 0] loss=1.2650 val_loss=0.0000 scale=1.0000 norm=1.6279\n",
      "[iter 100] loss=1.1385 val_loss=0.0000 scale=1.0000 norm=1.6417\n",
      "[iter 0] loss=1.2650 val_loss=0.0000 scale=1.0000 norm=1.6279\n",
      "[iter 100] loss=1.1385 val_loss=0.0000 scale=1.0000 norm=1.6421\n",
      "[iter 0] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.6271\n",
      "[iter 100] loss=1.1399 val_loss=0.0000 scale=1.0000 norm=1.6416\n",
      "[iter 0] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.6271\n",
      "[iter 100] loss=1.1400 val_loss=0.0000 scale=1.0000 norm=1.6418\n",
      "[iter 0] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.6271\n",
      "[iter 100] loss=1.1408 val_loss=0.0000 scale=1.0000 norm=1.6422\n",
      "[iter 0] loss=1.2663 val_loss=0.0000 scale=1.0000 norm=1.6254\n",
      "[iter 100] loss=1.1410 val_loss=0.0000 scale=1.0000 norm=1.6397\n",
      "[iter 0] loss=1.2663 val_loss=0.0000 scale=1.0000 norm=1.6254\n",
      "[iter 100] loss=1.1425 val_loss=0.0000 scale=1.0000 norm=1.6383\n",
      "[iter 0] loss=1.2676 val_loss=0.0000 scale=1.0000 norm=1.6235\n",
      "[iter 100] loss=1.1453 val_loss=0.0000 scale=1.0000 norm=1.6360\n",
      "[iter 0] loss=1.2669 val_loss=0.0000 scale=1.0000 norm=1.6247\n",
      "[iter 100] loss=1.1439 val_loss=0.0000 scale=1.0000 norm=1.6381\n",
      "[iter 0] loss=1.2656 val_loss=0.0000 scale=1.0000 norm=1.6261\n",
      "[iter 100] loss=1.1411 val_loss=0.0000 scale=1.0000 norm=1.6410\n",
      "[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=1.6264\n",
      "[iter 100] loss=1.1388 val_loss=0.0000 scale=1.0000 norm=1.6432\n",
      "[iter 0] loss=1.2646 val_loss=0.0000 scale=1.0000 norm=1.6268\n",
      "[iter 100] loss=1.1384 val_loss=0.0000 scale=1.0000 norm=1.6435\n",
      "[iter 0] loss=1.2644 val_loss=0.0000 scale=1.0000 norm=1.6257\n",
      "[iter 100] loss=1.1374 val_loss=0.0000 scale=1.0000 norm=1.6425\n",
      "[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.6264\n",
      "[iter 100] loss=1.1368 val_loss=0.0000 scale=1.0000 norm=1.6431\n",
      "[iter 0] loss=1.2640 val_loss=0.0000 scale=1.0000 norm=1.6251\n",
      "[iter 100] loss=1.1379 val_loss=0.0000 scale=1.0000 norm=1.6410\n",
      "[iter 0] loss=1.2634 val_loss=0.0000 scale=1.0000 norm=1.6256\n",
      "[iter 100] loss=1.1380 val_loss=0.0000 scale=1.0000 norm=1.6422\n",
      "[iter 0] loss=1.2626 val_loss=0.0000 scale=1.0000 norm=1.6256\n",
      "[iter 100] loss=1.1379 val_loss=0.0000 scale=1.0000 norm=1.6419\n",
      "[iter 0] loss=1.2624 val_loss=0.0000 scale=1.0000 norm=1.6247\n",
      "[iter 100] loss=1.1395 val_loss=0.0000 scale=1.0000 norm=1.6410\n",
      "[iter 0] loss=1.2611 val_loss=0.0000 scale=1.0000 norm=1.6270\n",
      "[iter 100] loss=1.1382 val_loss=0.0000 scale=1.0000 norm=1.6434\n",
      "[iter 0] loss=1.2710 val_loss=0.0000 scale=1.0000 norm=1.6112\n",
      "[iter 100] loss=1.1517 val_loss=0.0000 scale=1.0000 norm=1.6264\n",
      "[iter 0] loss=1.2725 val_loss=0.0000 scale=1.0000 norm=1.6076\n",
      "[iter 100] loss=1.1562 val_loss=0.0000 scale=1.0000 norm=1.6216\n",
      "[iter 0] loss=1.2734 val_loss=0.0000 scale=1.0000 norm=1.6048\n",
      "[iter 100] loss=1.1593 val_loss=0.0000 scale=1.0000 norm=1.6184\n",
      "[iter 0] loss=1.2938 val_loss=0.0000 scale=1.0000 norm=1.5772\n",
      "[iter 100] loss=1.1841 val_loss=0.0000 scale=1.0000 norm=1.5901\n",
      "[iter 0] loss=1.3089 val_loss=0.0000 scale=1.0000 norm=1.5565\n",
      "[iter 100] loss=1.2036 val_loss=0.0000 scale=1.0000 norm=1.5685\n",
      "[iter 0] loss=1.3530 val_loss=0.0000 scale=1.0000 norm=1.5064\n",
      "[iter 100] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=1.5160\n",
      "[iter 0] loss=1.3584 val_loss=0.0000 scale=1.0000 norm=1.4982\n",
      "[iter 100] loss=1.2613 val_loss=0.0000 scale=1.0000 norm=1.5077\n",
      "[iter 0] loss=1.3624 val_loss=0.0000 scale=1.0000 norm=1.4917\n",
      "[iter 100] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=1.5011\n",
      "[iter 0] loss=1.3630 val_loss=0.0000 scale=1.0000 norm=1.4907\n",
      "[iter 100] loss=1.2679 val_loss=0.0000 scale=1.0000 norm=1.5002\n",
      "[iter 0] loss=1.3656 val_loss=0.0000 scale=1.0000 norm=1.4862\n",
      "[iter 100] loss=1.2727 val_loss=0.0000 scale=1.0000 norm=1.4957\n",
      "[iter 0] loss=1.3649 val_loss=0.0000 scale=1.0000 norm=1.4867\n",
      "[iter 100] loss=1.2727 val_loss=0.0000 scale=1.0000 norm=1.4961\n",
      "[iter 0] loss=1.3860 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 100] loss=1.2970 val_loss=0.0000 scale=1.0000 norm=1.4712\n",
      "[iter 0] loss=1.3855 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 100] loss=1.2967 val_loss=0.0000 scale=1.0000 norm=1.4724\n",
      "[iter 0] loss=1.3926 val_loss=0.0000 scale=1.0000 norm=1.4543\n",
      "[iter 100] loss=1.3045 val_loss=0.0000 scale=1.0000 norm=1.4638\n",
      "[iter 0] loss=1.3935 val_loss=0.0000 scale=1.0000 norm=1.4525\n",
      "[iter 100] loss=1.3066 val_loss=0.0000 scale=1.0000 norm=1.4632\n",
      "[iter 0] loss=1.3935 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 100] loss=1.3080 val_loss=0.0000 scale=1.0000 norm=1.4628\n",
      "[iter 0] loss=1.3933 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.4629\n",
      "[iter 0] loss=1.3960 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3126 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 0] loss=1.3947 val_loss=0.0000 scale=1.0000 norm=1.4501\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4062 val_loss=0.0000 scale=1.0000 norm=1.4364\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4464\n",
      "[iter 0] loss=1.4062 val_loss=0.0000 scale=1.0000 norm=1.4364\n",
      "[iter 100] loss=1.3253 val_loss=0.0000 scale=1.0000 norm=1.4463\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4359\n",
      "[iter 100] loss=1.3266 val_loss=0.0000 scale=1.0000 norm=1.4457\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4366\n",
      "[iter 100] loss=1.3259 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3272 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3261 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4377\n",
      "[iter 100] loss=1.3246 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 0] loss=1.4050 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 100] loss=1.3234 val_loss=0.0000 scale=1.0000 norm=1.4498\n",
      "[iter 0] loss=1.4044 val_loss=0.0000 scale=1.0000 norm=1.4387\n",
      "[iter 100] loss=1.3225 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4395\n",
      "[iter 100] loss=1.3215 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 0] loss=1.4031 val_loss=0.0000 scale=1.0000 norm=1.4403\n",
      "[iter 100] loss=1.3205 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 0] loss=1.4040 val_loss=0.0000 scale=1.0000 norm=1.4387\n",
      "[iter 100] loss=1.3231 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 0] loss=1.4035 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4488\n",
      "[iter 0] loss=1.4037 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 0] loss=1.4037 val_loss=0.0000 scale=1.0000 norm=1.4395\n",
      "[iter 100] loss=1.3229 val_loss=0.0000 scale=1.0000 norm=1.4494\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4402\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 0] loss=1.4033 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3235 val_loss=0.0000 scale=1.0000 norm=1.4483\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4393\n",
      "[iter 100] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.4473\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4386\n",
      "[iter 100] loss=1.3247 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 0] loss=1.4042 val_loss=0.0000 scale=1.0000 norm=1.4379\n",
      "[iter 100] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.4482\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4389\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 0] loss=1.4035 val_loss=0.0000 scale=1.0000 norm=1.4395\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4501\n",
      "[iter 0] loss=1.4028 val_loss=0.0000 scale=1.0000 norm=1.4408\n",
      "[iter 100] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.4509\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4406\n",
      "[iter 100] loss=1.3213 val_loss=0.0000 scale=1.0000 norm=1.4496\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3201 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4432\n",
      "[iter 100] loss=1.3185 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 100] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4424\n",
      "[iter 100] loss=1.3180 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.4019 val_loss=0.0000 scale=1.0000 norm=1.4414\n",
      "[iter 100] loss=1.3198 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 0] loss=1.4013 val_loss=0.0000 scale=1.0000 norm=1.4424\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 100] loss=1.3177 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 100] loss=1.3171 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4426\n",
      "[iter 100] loss=1.3176 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4429\n",
      "[iter 100] loss=1.3168 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 100] loss=1.3176 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4433\n",
      "[iter 100] loss=1.3168 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4429\n",
      "[iter 100] loss=1.3171 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 100] loss=1.3167 val_loss=0.0000 scale=1.0000 norm=1.4529\n",
      "[iter 0] loss=1.4018 val_loss=0.0000 scale=1.0000 norm=1.4413\n",
      "[iter 100] loss=1.3186 val_loss=0.0000 scale=1.0000 norm=1.4514\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4426\n",
      "[iter 100] loss=1.3180 val_loss=0.0000 scale=1.0000 norm=1.4530\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4428\n",
      "[iter 100] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 100] loss=1.3172 val_loss=0.0000 scale=1.0000 norm=1.4532\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4439\n",
      "[iter 100] loss=1.3183 val_loss=0.0000 scale=1.0000 norm=1.4540\n",
      "[iter 0] loss=1.3998 val_loss=0.0000 scale=1.0000 norm=1.4449\n",
      "[iter 100] loss=1.3165 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4446\n",
      "[iter 100] loss=1.3168 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 0] loss=1.4105 val_loss=0.0000 scale=1.0000 norm=1.4318\n",
      "[iter 100] loss=1.3274 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 0] loss=1.4099 val_loss=0.0000 scale=1.0000 norm=1.4332\n",
      "[iter 100] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 0] loss=1.4101 val_loss=0.0000 scale=1.0000 norm=1.4326\n",
      "[iter 100] loss=1.3278 val_loss=0.0000 scale=1.0000 norm=1.4425\n",
      "[iter 0] loss=1.4095 val_loss=0.0000 scale=1.0000 norm=1.4337\n",
      "[iter 100] loss=1.3264 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 0] loss=1.4096 val_loss=0.0000 scale=1.0000 norm=1.4334\n",
      "[iter 100] loss=1.3274 val_loss=0.0000 scale=1.0000 norm=1.4438\n",
      "[iter 0] loss=1.4098 val_loss=0.0000 scale=1.0000 norm=1.4328\n",
      "[iter 100] loss=1.3276 val_loss=0.0000 scale=1.0000 norm=1.4434\n",
      "[iter 0] loss=1.4096 val_loss=0.0000 scale=1.0000 norm=1.4334\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4445\n",
      "[iter 0] loss=1.4098 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4444\n",
      "[iter 0] loss=1.4100 val_loss=0.0000 scale=1.0000 norm=1.4325\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4433\n",
      "[iter 0] loss=1.4101 val_loss=0.0000 scale=1.0000 norm=1.4319\n",
      "[iter 100] loss=1.3280 val_loss=0.0000 scale=1.0000 norm=1.4430\n",
      "[iter 0] loss=1.4096 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 100] loss=1.3276 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 0] loss=1.4091 val_loss=0.0000 scale=1.0000 norm=1.4333\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4452\n",
      "[iter 0] loss=1.4090 val_loss=0.0000 scale=1.0000 norm=1.4335\n",
      "[iter 100] loss=1.3294 val_loss=0.0000 scale=1.0000 norm=1.4446\n",
      "[iter 0] loss=1.4083 val_loss=0.0000 scale=1.0000 norm=1.4349\n",
      "[iter 100] loss=1.3280 val_loss=0.0000 scale=1.0000 norm=1.4462\n",
      "[iter 0] loss=1.4082 val_loss=0.0000 scale=1.0000 norm=1.4351\n",
      "[iter 100] loss=1.3278 val_loss=0.0000 scale=1.0000 norm=1.4451\n",
      "[iter 0] loss=1.4086 val_loss=0.0000 scale=1.0000 norm=1.4344\n",
      "[iter 100] loss=1.3279 val_loss=0.0000 scale=1.0000 norm=1.4456\n",
      "[iter 0] loss=1.4079 val_loss=0.0000 scale=1.0000 norm=1.4355\n",
      "[iter 100] loss=1.3274 val_loss=0.0000 scale=1.0000 norm=1.4484\n",
      "[iter 0] loss=1.4073 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3272 val_loss=0.0000 scale=1.0000 norm=1.4498\n",
      "[iter 0] loss=1.4072 val_loss=0.0000 scale=1.0000 norm=1.4369\n",
      "[iter 100] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.4510\n",
      "[iter 0] loss=1.4072 val_loss=0.0000 scale=1.0000 norm=1.4369\n",
      "[iter 100] loss=1.3264 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4371\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4383\n",
      "[iter 100] loss=1.3247 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4394\n",
      "[iter 100] loss=1.3246 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4398\n",
      "[iter 100] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.4555\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3237 val_loss=0.0000 scale=1.0000 norm=1.4558\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 100] loss=1.3243 val_loss=0.0000 scale=1.0000 norm=1.4552\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4399\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.4055 val_loss=0.0000 scale=1.0000 norm=1.4403\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3181 val_loss=0.0000 scale=1.0000 norm=1.4564\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4416\n",
      "[iter 100] loss=1.3184 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 0] loss=1.4050 val_loss=0.0000 scale=1.0000 norm=1.4413\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4421\n",
      "[iter 100] loss=1.3183 val_loss=0.0000 scale=2.0000 norm=2.9133\n",
      "[iter 0] loss=1.4041 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 100] loss=1.3194 val_loss=0.0000 scale=1.0000 norm=1.4582\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4420\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4425\n",
      "[iter 100] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4426\n",
      "[iter 100] loss=1.3224 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3225 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4416\n",
      "[iter 100] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4416\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4543\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3225 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4421\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4421\n",
      "[iter 100] loss=1.3248 val_loss=0.0000 scale=1.0000 norm=1.4582\n",
      "[iter 0] loss=1.4052 val_loss=0.0000 scale=1.0000 norm=1.4415\n",
      "[iter 100] loss=1.3242 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4405\n",
      "[iter 100] loss=1.3242 val_loss=0.0000 scale=1.0000 norm=1.4572\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4405\n",
      "[iter 100] loss=1.3261 val_loss=0.0000 scale=2.0000 norm=2.9112\n",
      "[iter 0] loss=1.4058 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3263 val_loss=0.0000 scale=1.0000 norm=1.4552\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4402\n",
      "[iter 100] loss=1.3255 val_loss=0.0000 scale=1.0000 norm=1.4578\n",
      "[iter 0] loss=1.4062 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 100] loss=1.3275 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4379\n",
      "[iter 100] loss=1.3285 val_loss=0.0000 scale=1.0000 norm=1.4539\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4383\n",
      "[iter 100] loss=1.3286 val_loss=0.0000 scale=1.0000 norm=1.4543\n",
      "[iter 0] loss=1.4069 val_loss=0.0000 scale=1.0000 norm=1.4379\n",
      "[iter 100] loss=1.3292 val_loss=0.0000 scale=1.0000 norm=1.4539\n",
      "[iter 0] loss=1.4071 val_loss=0.0000 scale=1.0000 norm=1.4376\n",
      "[iter 100] loss=1.3284 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 0] loss=1.4072 val_loss=0.0000 scale=1.0000 norm=1.4375\n",
      "[iter 100] loss=1.3299 val_loss=0.0000 scale=1.0000 norm=1.4530\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4380\n",
      "[iter 100] loss=1.3296 val_loss=0.0000 scale=1.0000 norm=1.4530\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4380\n",
      "[iter 100] loss=1.3289 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 0] loss=1.4068 val_loss=0.0000 scale=1.0000 norm=1.4384\n",
      "[iter 100] loss=1.3280 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 0] loss=1.4064 val_loss=0.0000 scale=1.0000 norm=1.4391\n",
      "[iter 100] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 0] loss=1.4078 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3270 val_loss=0.0000 scale=1.0000 norm=1.4516\n",
      "[iter 0] loss=1.4074 val_loss=0.0000 scale=1.0000 norm=1.4377\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 0] loss=1.4075 val_loss=0.0000 scale=1.0000 norm=1.4369\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4514\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4376\n",
      "[iter 100] loss=1.3261 val_loss=0.0000 scale=1.0000 norm=1.4524\n",
      "[iter 0] loss=1.4061 val_loss=0.0000 scale=1.0000 norm=1.4389\n",
      "[iter 100] loss=1.3250 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4380\n",
      "[iter 100] loss=1.3250 val_loss=0.0000 scale=1.0000 norm=1.4538\n",
      "[iter 0] loss=1.4066 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 100] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.4533\n",
      "[iter 0] loss=1.4064 val_loss=0.0000 scale=1.0000 norm=1.4384\n",
      "[iter 100] loss=1.3249 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4388\n",
      "[iter 100] loss=1.3247 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4387\n",
      "[iter 100] loss=1.3252 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4385\n",
      "[iter 100] loss=1.3263 val_loss=0.0000 scale=1.0000 norm=1.4533\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 100] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 0] loss=1.4055 val_loss=0.0000 scale=1.0000 norm=1.4407\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4399\n",
      "[iter 100] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3274 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4409\n",
      "[iter 100] loss=1.3264 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 0] loss=1.4043 val_loss=0.0000 scale=1.0000 norm=1.4423\n",
      "[iter 100] loss=1.3255 val_loss=0.0000 scale=1.0000 norm=1.4564\n",
      "[iter 0] loss=1.4041 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 100] loss=1.3252 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4422\n",
      "[iter 100] loss=1.3269 val_loss=0.0000 scale=1.0000 norm=1.4567\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4418\n",
      "[iter 100] loss=1.3252 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 0] loss=1.4042 val_loss=0.0000 scale=1.0000 norm=1.4428\n",
      "[iter 100] loss=1.3260 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 0] loss=1.4040 val_loss=0.0000 scale=1.0000 norm=1.4429\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4569\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4438\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 0] loss=1.4027 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 100] loss=1.3251 val_loss=0.0000 scale=1.0000 norm=1.4597\n",
      "[iter 0] loss=1.4025 val_loss=0.0000 scale=1.0000 norm=1.4455\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4024 val_loss=0.0000 scale=1.0000 norm=1.4456\n",
      "[iter 100] loss=1.3235 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 0] loss=1.4018 val_loss=0.0000 scale=1.0000 norm=1.4467\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4462\n",
      "[iter 100] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.4610\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4470\n",
      "[iter 100] loss=1.3217 val_loss=0.0000 scale=2.0000 norm=2.9231\n",
      "[iter 0] loss=1.4024 val_loss=0.0000 scale=1.0000 norm=1.4455\n",
      "[iter 100] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.4598\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4445\n",
      "[iter 100] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 0] loss=1.4028 val_loss=0.0000 scale=1.0000 norm=1.4446\n",
      "[iter 100] loss=1.3240 val_loss=0.0000 scale=2.0000 norm=2.9200\n",
      "[iter 0] loss=1.4027 val_loss=0.0000 scale=1.0000 norm=1.4447\n",
      "[iter 100] loss=1.3233 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 0] loss=1.4030 val_loss=0.0000 scale=1.0000 norm=1.4442\n",
      "[iter 100] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.4592\n",
      "[iter 0] loss=1.4037 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 100] loss=1.3261 val_loss=0.0000 scale=1.0000 norm=1.4598\n",
      "[iter 0] loss=1.4043 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.4037 val_loss=0.0000 scale=1.0000 norm=1.4428\n",
      "[iter 100] loss=1.3254 val_loss=0.0000 scale=1.0000 norm=1.4594\n",
      "[iter 0] loss=1.4042 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3267 val_loss=0.0000 scale=1.0000 norm=1.4582\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4387\n",
      "[iter 100] loss=1.3257 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4389\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4527\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4389\n",
      "[iter 100] loss=1.3270 val_loss=0.0000 scale=2.0000 norm=2.9032\n",
      "[iter 0] loss=1.4058 val_loss=0.0000 scale=1.0000 norm=1.4391\n",
      "[iter 100] loss=1.3275 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4405\n",
      "[iter 100] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 0] loss=1.4042 val_loss=0.0000 scale=1.0000 norm=1.4414\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4552\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4429\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 100] loss=1.3261 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 0] loss=1.4041 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 100] loss=1.3279 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4041 val_loss=0.0000 scale=1.0000 norm=1.4418\n",
      "[iter 100] loss=1.3273 val_loss=0.0000 scale=1.0000 norm=1.4531\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4411\n",
      "[iter 100] loss=1.3273 val_loss=0.0000 scale=1.0000 norm=1.4539\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4424\n",
      "[iter 100] loss=1.3270 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4438\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4560\n",
      "[iter 0] loss=1.4026 val_loss=0.0000 scale=1.0000 norm=1.4452\n",
      "[iter 100] loss=1.3250 val_loss=0.0000 scale=1.0000 norm=1.4568\n",
      "[iter 0] loss=1.4027 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4563\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 100] loss=1.3239 val_loss=0.0000 scale=2.0000 norm=2.9154\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 100] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.4579\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4469\n",
      "[iter 100] loss=1.3234 val_loss=0.0000 scale=1.0000 norm=1.4586\n",
      "[iter 0] loss=1.4015 val_loss=0.0000 scale=1.0000 norm=1.4472\n",
      "[iter 100] loss=1.3229 val_loss=0.0000 scale=1.0000 norm=1.4589\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4488\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3204 val_loss=0.0000 scale=1.0000 norm=1.4609\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3203 val_loss=0.0000 scale=1.0000 norm=1.4612\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 100] loss=1.3203 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3204 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4618\n",
      "[iter 0] loss=1.4004 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.4621\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4488\n",
      "[iter 100] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.4002 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4630\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3208 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4486\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=1.4496\n",
      "[iter 100] loss=1.3198 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3210 val_loss=0.0000 scale=1.0000 norm=1.4623\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3226 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4473\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4476\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.4013 val_loss=0.0000 scale=1.0000 norm=1.4476\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4013 val_loss=0.0000 scale=1.0000 norm=1.4475\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4484\n",
      "[iter 100] loss=1.3214 val_loss=0.0000 scale=1.0000 norm=1.4621\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.3216 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4469\n",
      "[iter 100] loss=1.3230 val_loss=0.0000 scale=2.0000 norm=2.9196\n",
      "[iter 0] loss=1.4015 val_loss=0.0000 scale=1.0000 norm=1.4472\n",
      "[iter 100] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3183 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=1.4601\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4483\n",
      "[iter 100] loss=1.3201 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3198 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=1.4498\n",
      "[iter 100] loss=1.3189 val_loss=0.0000 scale=1.0000 norm=1.4633\n",
      "[iter 0] loss=1.3998 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3181 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 100] loss=1.3167 val_loss=0.0000 scale=1.0000 norm=1.4644\n",
      "[iter 0] loss=1.4002 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.3180 val_loss=0.0000 scale=1.0000 norm=1.4629\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3186 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4485\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 0] loss=1.4004 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3176 val_loss=0.0000 scale=2.0000 norm=2.9249\n",
      "[iter 0] loss=1.3997 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3157 val_loss=0.0000 scale=1.0000 norm=1.4655\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4651\n",
      "[iter 0] loss=1.3997 val_loss=0.0000 scale=1.0000 norm=1.4505\n",
      "[iter 100] loss=1.3158 val_loss=0.0000 scale=1.0000 norm=1.4658\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 100] loss=1.3156 val_loss=0.0000 scale=1.0000 norm=1.4662\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 100] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4503\n",
      "[iter 100] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=1.4665\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 100] loss=1.3155 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 0] loss=1.3998 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.3166 val_loss=0.0000 scale=1.0000 norm=1.4628\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 100] loss=1.3161 val_loss=0.0000 scale=1.0000 norm=1.4635\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 100] loss=1.3153 val_loss=0.0000 scale=1.0000 norm=1.4651\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 100] loss=1.3152 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 0] loss=1.3987 val_loss=0.0000 scale=1.0000 norm=1.4515\n",
      "[iter 100] loss=1.3153 val_loss=0.0000 scale=1.0000 norm=1.4654\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4518\n",
      "[iter 100] loss=1.3145 val_loss=0.0000 scale=1.0000 norm=1.4667\n",
      "[iter 0] loss=1.3985 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.3145 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 0] loss=1.3985 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4677\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4505\n",
      "[iter 100] loss=1.3158 val_loss=0.0000 scale=2.0000 norm=2.9312\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4494\n",
      "[iter 100] loss=1.3162 val_loss=0.0000 scale=2.0000 norm=2.9299\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 100] loss=1.3167 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.4002 val_loss=0.0000 scale=1.0000 norm=1.4488\n",
      "[iter 100] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.4640\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3172 val_loss=0.0000 scale=1.0000 norm=1.4639\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4486\n",
      "[iter 100] loss=1.3173 val_loss=0.0000 scale=1.0000 norm=1.4640\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4484\n",
      "[iter 100] loss=1.3178 val_loss=0.0000 scale=1.0000 norm=1.4626\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.4628\n",
      "[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3168 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4637\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 100] loss=1.3161 val_loss=0.0000 scale=1.0000 norm=1.4643\n",
      "[iter 0] loss=1.3993 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.3162 val_loss=0.0000 scale=1.0000 norm=1.4645\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3153 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 0] loss=1.3987 val_loss=0.0000 scale=1.0000 norm=1.4509\n",
      "[iter 100] loss=1.3171 val_loss=0.0000 scale=1.0000 norm=1.4656\n",
      "[iter 0] loss=1.3980 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 0] loss=1.3977 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 0] loss=1.3978 val_loss=0.0000 scale=1.0000 norm=1.4521\n",
      "[iter 100] loss=1.3165 val_loss=0.0000 scale=1.0000 norm=1.4661\n",
      "[iter 0] loss=1.3970 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 100] loss=1.3156 val_loss=0.0000 scale=1.0000 norm=1.4670\n",
      "[iter 0] loss=1.3974 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4662\n",
      "[iter 0] loss=1.3969 val_loss=0.0000 scale=1.0000 norm=1.4538\n",
      "[iter 100] loss=1.3158 val_loss=0.0000 scale=1.0000 norm=1.4671\n",
      "[iter 0] loss=1.3970 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 100] loss=1.3157 val_loss=0.0000 scale=1.0000 norm=1.4670\n",
      "[iter 0] loss=1.3971 val_loss=0.0000 scale=1.0000 norm=1.4533\n",
      "[iter 100] loss=1.3158 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 0] loss=1.3973 val_loss=0.0000 scale=1.0000 norm=1.4528\n",
      "[iter 100] loss=1.3155 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 100] loss=1.3153 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3970 val_loss=0.0000 scale=1.0000 norm=1.4529\n",
      "[iter 100] loss=1.3152 val_loss=0.0000 scale=1.0000 norm=1.4673\n",
      "[iter 0] loss=1.3971 val_loss=0.0000 scale=1.0000 norm=1.4528\n",
      "[iter 100] loss=1.3146 val_loss=0.0000 scale=2.0000 norm=2.9350\n",
      "[iter 0] loss=1.3968 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4682\n",
      "[iter 0] loss=1.3964 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 100] loss=1.3124 val_loss=0.0000 scale=1.0000 norm=1.4698\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 100] loss=1.3126 val_loss=0.0000 scale=1.0000 norm=1.4696\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 100] loss=1.3132 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4555\n",
      "[iter 100] loss=1.3119 val_loss=0.0000 scale=1.0000 norm=1.4704\n",
      "[iter 0] loss=1.3957 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=1.4704\n",
      "[iter 0] loss=1.3960 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 100] loss=1.3110 val_loss=0.0000 scale=1.0000 norm=1.4695\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3107 val_loss=0.0000 scale=1.0000 norm=1.4700\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 100] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.4693\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 100] loss=1.3097 val_loss=0.0000 scale=1.0000 norm=1.4702\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.4697\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3104 val_loss=0.0000 scale=1.0000 norm=1.4699\n",
      "[iter 0] loss=1.3964 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 100] loss=1.3114 val_loss=0.0000 scale=1.0000 norm=1.4689\n",
      "[iter 0] loss=1.3964 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4686\n",
      "[iter 0] loss=1.3973 val_loss=0.0000 scale=1.0000 norm=1.4527\n",
      "[iter 100] loss=1.3142 val_loss=0.0000 scale=1.0000 norm=1.4667\n",
      "[iter 0] loss=1.3980 val_loss=0.0000 scale=1.0000 norm=1.4514\n",
      "[iter 100] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.4650\n",
      "[iter 0] loss=1.3979 val_loss=0.0000 scale=1.0000 norm=1.4516\n",
      "[iter 100] loss=1.3143 val_loss=0.0000 scale=1.0000 norm=1.4658\n",
      "[iter 0] loss=1.3976 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 100] loss=1.3135 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 100] loss=1.3138 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 0] loss=1.3978 val_loss=0.0000 scale=1.0000 norm=1.4518\n",
      "[iter 100] loss=1.3145 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3979 val_loss=0.0000 scale=1.0000 norm=1.4517\n",
      "[iter 100] loss=1.3155 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 0] loss=1.3981 val_loss=0.0000 scale=1.0000 norm=1.4513\n",
      "[iter 100] loss=1.3154 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4525\n",
      "[iter 100] loss=1.3150 val_loss=0.0000 scale=2.0000 norm=2.9326\n",
      "[iter 0] loss=1.3971 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 100] loss=1.3140 val_loss=0.0000 scale=1.0000 norm=1.4669\n",
      "[iter 0] loss=1.3976 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.3161 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 0] loss=1.3977 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 100] loss=1.3156 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4505\n",
      "[iter 100] loss=1.3165 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 0] loss=1.3984 val_loss=0.0000 scale=1.0000 norm=1.4509\n",
      "[iter 100] loss=1.3179 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.3985 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3165 val_loss=0.0000 scale=1.0000 norm=1.4643\n",
      "[iter 0] loss=1.3992 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.3172 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 100] loss=1.3180 val_loss=0.0000 scale=1.0000 norm=1.4616\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 0] loss=1.3990 val_loss=0.0000 scale=1.0000 norm=1.4501\n",
      "[iter 100] loss=1.3166 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3988 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.3155 val_loss=0.0000 scale=1.0000 norm=1.4626\n",
      "[iter 0] loss=1.3981 val_loss=0.0000 scale=1.0000 norm=1.4517\n",
      "[iter 100] loss=1.3146 val_loss=0.0000 scale=1.0000 norm=1.4647\n",
      "[iter 0] loss=1.3976 val_loss=0.0000 scale=1.0000 norm=1.4529\n",
      "[iter 100] loss=1.3133 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3970 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 0] loss=1.3969 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4667\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 100] loss=1.3119 val_loss=0.0000 scale=1.0000 norm=1.4678\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3121 val_loss=0.0000 scale=1.0000 norm=1.4683\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 100] loss=1.3105 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 0] loss=1.3954 val_loss=0.0000 scale=1.0000 norm=1.4563\n",
      "[iter 100] loss=1.3096 val_loss=0.0000 scale=1.0000 norm=1.4691\n",
      "[iter 0] loss=1.3949 val_loss=0.0000 scale=1.0000 norm=1.4574\n",
      "[iter 100] loss=1.3093 val_loss=0.0000 scale=1.0000 norm=1.4705\n",
      "[iter 0] loss=1.3954 val_loss=0.0000 scale=1.0000 norm=1.4564\n",
      "[iter 100] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.4699\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4569\n",
      "[iter 100] loss=1.3098 val_loss=0.0000 scale=1.0000 norm=1.4705\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4570\n",
      "[iter 100] loss=1.3094 val_loss=0.0000 scale=1.0000 norm=1.4706\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 100] loss=1.3100 val_loss=0.0000 scale=1.0000 norm=1.4705\n",
      "[iter 0] loss=1.3958 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3117 val_loss=0.0000 scale=1.0000 norm=1.4691\n",
      "[iter 0] loss=1.3962 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4683\n",
      "[iter 0] loss=1.3962 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4682\n",
      "[iter 0] loss=1.3962 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=1.4684\n",
      "[iter 0] loss=1.3968 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.3129 val_loss=0.0000 scale=1.0000 norm=1.4667\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3121 val_loss=0.0000 scale=1.0000 norm=1.4675\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4673\n",
      "[iter 0] loss=1.3964 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4670\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4671\n",
      "[iter 0] loss=1.3966 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 100] loss=1.3126 val_loss=0.0000 scale=1.0000 norm=1.4665\n",
      "[iter 0] loss=1.3966 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 100] loss=1.3122 val_loss=0.0000 scale=1.0000 norm=1.4661\n",
      "[iter 0] loss=1.3966 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 0] loss=1.3969 val_loss=0.0000 scale=1.0000 norm=1.4530\n",
      "[iter 100] loss=1.3125 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 0] loss=1.3967 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 100] loss=1.3116 val_loss=0.0000 scale=1.0000 norm=1.4655\n",
      "[iter 0] loss=1.3968 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 100] loss=1.3116 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 100] loss=1.3096 val_loss=0.0000 scale=1.0000 norm=1.4671\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 100] loss=1.3085 val_loss=0.0000 scale=1.0000 norm=1.4688\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 100] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4563\n",
      "[iter 100] loss=1.3085 val_loss=0.0000 scale=1.0000 norm=1.4688\n",
      "[iter 0] loss=1.3949 val_loss=0.0000 scale=1.0000 norm=1.4568\n",
      "[iter 100] loss=1.3080 val_loss=0.0000 scale=1.0000 norm=1.4691\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 100] loss=1.3085 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4565\n",
      "[iter 100] loss=1.3087 val_loss=0.0000 scale=1.0000 norm=1.4689\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4558\n",
      "[iter 100] loss=1.3091 val_loss=0.0000 scale=1.0000 norm=1.4685\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.4684\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3089 val_loss=0.0000 scale=1.0000 norm=1.4686\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4563\n",
      "[iter 100] loss=1.3087 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4564\n",
      "[iter 100] loss=1.3083 val_loss=0.0000 scale=1.0000 norm=1.4691\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4553\n",
      "[iter 100] loss=1.3090 val_loss=0.0000 scale=1.0000 norm=1.4687\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3096 val_loss=0.0000 scale=1.0000 norm=1.4686\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 100] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.4688\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4562\n",
      "[iter 100] loss=1.3089 val_loss=0.0000 scale=1.0000 norm=1.4689\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 100] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.4689\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3127 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4663\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4538\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4668\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4540\n",
      "[iter 100] loss=1.3130 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4556\n",
      "[iter 100] loss=1.3115 val_loss=0.0000 scale=1.0000 norm=1.4676\n",
      "[iter 0] loss=1.3958 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 100] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=1.4671\n",
      "[iter 0] loss=1.3958 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.3111 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 100] loss=1.3114 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3967 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.3133 val_loss=0.0000 scale=1.0000 norm=1.4663\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 100] loss=1.3140 val_loss=0.0000 scale=1.0000 norm=1.4642\n",
      "[iter 0] loss=1.3981 val_loss=0.0000 scale=1.0000 norm=1.4515\n",
      "[iter 100] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.4637\n",
      "[iter 0] loss=1.3977 val_loss=0.0000 scale=1.0000 norm=1.4525\n",
      "[iter 100] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 0] loss=1.3979 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.3146 val_loss=0.0000 scale=1.0000 norm=1.4651\n",
      "[iter 0] loss=1.3981 val_loss=0.0000 scale=1.0000 norm=1.4517\n",
      "[iter 100] loss=1.3145 val_loss=0.0000 scale=1.0000 norm=1.4641\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4514\n",
      "[iter 100] loss=1.3153 val_loss=0.0000 scale=1.0000 norm=1.4638\n",
      "[iter 0] loss=1.3987 val_loss=0.0000 scale=1.0000 norm=1.4510\n",
      "[iter 100] loss=1.3161 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 0] loss=1.3987 val_loss=0.0000 scale=1.0000 norm=1.4510\n",
      "[iter 100] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.3177 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.3181 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.4004 val_loss=0.0000 scale=1.0000 norm=1.4482\n",
      "[iter 100] loss=1.3194 val_loss=0.0000 scale=1.0000 norm=1.4605\n",
      "[iter 0] loss=1.4004 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4603\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4472\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4592\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.3213 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4612\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4473\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4595\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4595\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3215 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4592\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3220 val_loss=0.0000 scale=1.0000 norm=1.4597\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4591\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4471\n",
      "[iter 100] loss=1.3231 val_loss=0.0000 scale=1.0000 norm=1.4586\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4470\n",
      "[iter 100] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 100] loss=1.3242 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 0] loss=1.4022 val_loss=0.0000 scale=1.0000 norm=1.4455\n",
      "[iter 100] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.4572\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4443\n",
      "[iter 100] loss=1.3269 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 0] loss=1.4028 val_loss=0.0000 scale=1.0000 norm=1.4442\n",
      "[iter 100] loss=1.3273 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4430\n",
      "[iter 100] loss=1.3279 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 0] loss=1.4035 val_loss=0.0000 scale=1.0000 norm=1.4428\n",
      "[iter 100] loss=1.3280 val_loss=0.0000 scale=1.0000 norm=1.4533\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4433\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 100] loss=1.3263 val_loss=0.0000 scale=1.0000 norm=1.4534\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4441\n",
      "[iter 100] loss=1.3259 val_loss=0.0000 scale=1.0000 norm=1.4539\n",
      "[iter 0] loss=1.4026 val_loss=0.0000 scale=1.0000 norm=1.4447\n",
      "[iter 100] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 100] loss=1.3269 val_loss=0.0000 scale=1.0000 norm=1.4530\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4444\n",
      "[iter 100] loss=1.3260 val_loss=0.0000 scale=1.0000 norm=1.4539\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4459\n",
      "[iter 100] loss=1.3240 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4459\n",
      "[iter 100] loss=1.3240 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4467\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4553\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.4557\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.4557\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4476\n",
      "[iter 100] loss=1.3208 val_loss=0.0000 scale=1.0000 norm=1.4568\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.3205 val_loss=0.0000 scale=1.0000 norm=1.4565\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4490\n",
      "[iter 100] loss=1.3186 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 0] loss=1.3998 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3184 val_loss=0.0000 scale=1.0000 norm=1.4589\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4496\n",
      "[iter 100] loss=1.3189 val_loss=0.0000 scale=1.0000 norm=1.4604\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.3998 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4616\n",
      "[iter 0] loss=1.3995 val_loss=0.0000 scale=1.0000 norm=1.4496\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4488\n",
      "[iter 100] loss=1.3196 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 0] loss=1.3992 val_loss=0.0000 scale=1.0000 norm=1.4501\n",
      "[iter 100] loss=1.3171 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4505\n",
      "[iter 100] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.4625\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3168 val_loss=0.0000 scale=1.0000 norm=1.4628\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4513\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.3177 val_loss=0.0000 scale=1.0000 norm=1.4623\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.3173 val_loss=0.0000 scale=1.0000 norm=1.4621\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3990 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3172 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3990 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.3173 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3989 val_loss=0.0000 scale=1.0000 norm=1.4510\n",
      "[iter 100] loss=1.3173 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 0] loss=1.3993 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "[iter 100] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 100] loss=1.3201 val_loss=0.0000 scale=1.0000 norm=1.4612\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 100] loss=1.3206 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 100] loss=1.3226 val_loss=0.0000 scale=1.0000 norm=1.4596\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.4589\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3231 val_loss=0.0000 scale=1.0000 norm=1.4591\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3233 val_loss=0.0000 scale=1.0000 norm=1.4597\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4498\n",
      "[iter 100] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3216 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4484\n",
      "[iter 100] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4482\n",
      "[iter 100] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.4586\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3231 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4483\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4586\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4583\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.3216 val_loss=0.0000 scale=1.0000 norm=1.4583\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4473\n",
      "[iter 100] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.3205 val_loss=0.0000 scale=1.0000 norm=1.4596\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4490\n",
      "[iter 100] loss=1.3183 val_loss=0.0000 scale=1.0000 norm=1.4609\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4503\n",
      "[iter 100] loss=1.3163 val_loss=0.0000 scale=1.0000 norm=1.4626\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4513\n",
      "[iter 100] loss=1.3152 val_loss=0.0000 scale=1.0000 norm=1.4637\n",
      "[iter 0] loss=1.3978 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4656\n",
      "[iter 0] loss=1.3973 val_loss=0.0000 scale=1.0000 norm=1.4536\n",
      "[iter 100] loss=1.3126 val_loss=0.0000 scale=1.0000 norm=1.4669\n",
      "[iter 0] loss=1.3970 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 100] loss=1.3126 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4552\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4690\n",
      "[iter 0] loss=1.3966 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4683\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 100] loss=1.3116 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 0] loss=1.3960 val_loss=0.0000 scale=1.0000 norm=1.4557\n",
      "[iter 100] loss=1.3115 val_loss=0.0000 scale=1.0000 norm=1.4702\n",
      "[iter 0] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.4567\n",
      "[iter 100] loss=1.3104 val_loss=0.0000 scale=1.0000 norm=1.4715\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4573\n",
      "[iter 100] loss=1.3099 val_loss=0.0000 scale=1.0000 norm=1.4723\n",
      "[iter 0] loss=1.3941 val_loss=0.0000 scale=1.0000 norm=1.4595\n",
      "[iter 100] loss=1.3100 val_loss=0.0000 scale=1.0000 norm=1.4742\n",
      "[iter 0] loss=1.3935 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 100] loss=1.3083 val_loss=0.0000 scale=1.0000 norm=1.4752\n",
      "[iter 0] loss=1.3934 val_loss=0.0000 scale=1.0000 norm=1.4608\n",
      "[iter 100] loss=1.3087 val_loss=0.0000 scale=1.0000 norm=1.4753\n",
      "[iter 0] loss=1.3930 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 100] loss=1.3083 val_loss=0.0000 scale=1.0000 norm=1.4764\n",
      "[iter 0] loss=1.3927 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 100] loss=1.3073 val_loss=0.0000 scale=1.0000 norm=1.4758\n",
      "[iter 0] loss=1.3921 val_loss=0.0000 scale=1.0000 norm=1.4629\n",
      "[iter 100] loss=1.3071 val_loss=0.0000 scale=1.0000 norm=1.4769\n",
      "[iter 0] loss=1.3927 val_loss=0.0000 scale=1.0000 norm=1.4619\n",
      "[iter 100] loss=1.3081 val_loss=0.0000 scale=1.0000 norm=1.4757\n",
      "[iter 0] loss=1.3926 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 100] loss=1.3091 val_loss=0.0000 scale=1.0000 norm=1.4758\n",
      "[iter 0] loss=1.3928 val_loss=0.0000 scale=1.0000 norm=1.4616\n",
      "[iter 100] loss=1.3086 val_loss=0.0000 scale=1.0000 norm=1.4759\n",
      "[iter 0] loss=1.3929 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 100] loss=1.3087 val_loss=0.0000 scale=1.0000 norm=1.4763\n",
      "[iter 0] loss=1.3935 val_loss=0.0000 scale=1.0000 norm=1.4603\n",
      "[iter 100] loss=1.3095 val_loss=0.0000 scale=1.0000 norm=1.4752\n",
      "[iter 0] loss=1.3936 val_loss=0.0000 scale=1.0000 norm=1.4601\n",
      "[iter 100] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.4758\n",
      "[iter 0] loss=1.3941 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 100] loss=1.3100 val_loss=0.0000 scale=1.0000 norm=1.4750\n",
      "[iter 0] loss=1.3941 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 100] loss=1.3099 val_loss=0.0000 scale=1.0000 norm=1.4751\n",
      "[iter 0] loss=1.3941 val_loss=0.0000 scale=1.0000 norm=1.4590\n",
      "[iter 100] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.4751\n",
      "[iter 0] loss=1.3941 val_loss=0.0000 scale=1.0000 norm=1.4591\n",
      "[iter 100] loss=1.3098 val_loss=0.0000 scale=1.0000 norm=1.4753\n",
      "[iter 0] loss=1.3939 val_loss=0.0000 scale=1.0000 norm=1.4595\n",
      "[iter 100] loss=1.3099 val_loss=0.0000 scale=1.0000 norm=1.4757\n",
      "[iter 0] loss=1.3947 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 100] loss=1.3102 val_loss=0.0000 scale=1.0000 norm=1.4748\n",
      "[iter 0] loss=1.3943 val_loss=0.0000 scale=1.0000 norm=1.4590\n",
      "[iter 100] loss=1.3100 val_loss=0.0000 scale=1.0000 norm=1.4755\n",
      "[iter 0] loss=1.3948 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 100] loss=1.3102 val_loss=0.0000 scale=1.0000 norm=1.4748\n",
      "[iter 0] loss=1.3943 val_loss=0.0000 scale=1.0000 norm=1.4585\n",
      "[iter 100] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.4752\n",
      "[iter 0] loss=1.3946 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 100] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.4750\n",
      "[iter 0] loss=1.3938 val_loss=0.0000 scale=1.0000 norm=1.4590\n",
      "[iter 100] loss=1.3094 val_loss=0.0000 scale=1.0000 norm=1.4760\n",
      "[iter 0] loss=1.3932 val_loss=0.0000 scale=1.0000 norm=1.4601\n",
      "[iter 100] loss=1.3091 val_loss=0.0000 scale=1.0000 norm=1.4767\n",
      "[iter 0] loss=1.3934 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 100] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.4760\n",
      "[iter 0] loss=1.3929 val_loss=0.0000 scale=1.0000 norm=1.4601\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4753\n",
      "[iter 0] loss=1.3931 val_loss=0.0000 scale=1.0000 norm=1.4598\n",
      "[iter 100] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=1.4751\n",
      "[iter 0] loss=1.3925 val_loss=0.0000 scale=1.0000 norm=1.4609\n",
      "[iter 100] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.4764\n",
      "[iter 0] loss=1.3927 val_loss=0.0000 scale=1.0000 norm=1.4603\n",
      "[iter 100] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=1.4757\n",
      "[iter 0] loss=1.3929 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 100] loss=1.3109 val_loss=0.0000 scale=1.0000 norm=1.4752\n",
      "[iter 0] loss=1.3926 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 100] loss=1.3100 val_loss=0.0000 scale=1.0000 norm=1.4761\n",
      "[iter 0] loss=1.3918 val_loss=0.0000 scale=1.0000 norm=1.4611\n",
      "[iter 100] loss=1.3085 val_loss=0.0000 scale=1.0000 norm=1.4777\n",
      "[iter 0] loss=1.3916 val_loss=0.0000 scale=1.0000 norm=1.4616\n",
      "[iter 100] loss=1.3075 val_loss=0.0000 scale=1.0000 norm=1.4784\n",
      "[iter 0] loss=1.3917 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 100] loss=1.3084 val_loss=0.0000 scale=1.0000 norm=1.4782\n",
      "[iter 0] loss=1.3915 val_loss=0.0000 scale=1.0000 norm=1.4612\n",
      "[iter 100] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.4779\n",
      "[iter 0] loss=1.3913 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 100] loss=1.3074 val_loss=0.0000 scale=1.0000 norm=1.4788\n",
      "[iter 0] loss=1.3913 val_loss=0.0000 scale=1.0000 norm=1.4618\n",
      "[iter 100] loss=1.3070 val_loss=0.0000 scale=1.0000 norm=1.4794\n",
      "[iter 0] loss=1.3914 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 100] loss=1.3069 val_loss=0.0000 scale=1.0000 norm=1.4788\n",
      "[iter 0] loss=1.3906 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 100] loss=1.3060 val_loss=0.0000 scale=1.0000 norm=1.4804\n",
      "[iter 0] loss=1.3896 val_loss=0.0000 scale=1.0000 norm=1.4644\n",
      "[iter 100] loss=1.3047 val_loss=0.0000 scale=1.0000 norm=1.4815\n",
      "[iter 0] loss=1.3887 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 100] loss=1.3040 val_loss=0.0000 scale=1.0000 norm=1.4822\n",
      "[iter 0] loss=1.3888 val_loss=0.0000 scale=1.0000 norm=1.4656\n",
      "[iter 100] loss=1.3038 val_loss=0.0000 scale=1.0000 norm=1.4818\n",
      "[iter 0] loss=1.3889 val_loss=0.0000 scale=1.0000 norm=1.4656\n",
      "[iter 100] loss=1.3053 val_loss=0.0000 scale=1.0000 norm=1.4813\n",
      "[iter 0] loss=1.3891 val_loss=0.0000 scale=1.0000 norm=1.4651\n",
      "[iter 100] loss=1.3052 val_loss=0.0000 scale=1.0000 norm=1.4816\n",
      "[iter 0] loss=1.3886 val_loss=0.0000 scale=1.0000 norm=1.4655\n",
      "[iter 100] loss=1.3051 val_loss=0.0000 scale=1.0000 norm=1.4816\n",
      "[iter 0] loss=1.3881 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 100] loss=1.3045 val_loss=0.0000 scale=1.0000 norm=1.4822\n",
      "[iter 0] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 100] loss=1.3057 val_loss=0.0000 scale=1.0000 norm=1.4814\n",
      "[iter 0] loss=1.3883 val_loss=0.0000 scale=1.0000 norm=1.4661\n",
      "[iter 100] loss=1.3077 val_loss=0.0000 scale=1.0000 norm=1.4805\n",
      "[iter 0] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.4662\n",
      "[iter 100] loss=1.3078 val_loss=0.0000 scale=1.0000 norm=1.4803\n",
      "[iter 0] loss=1.3885 val_loss=0.0000 scale=1.0000 norm=1.4658\n",
      "[iter 100] loss=1.3091 val_loss=0.0000 scale=1.0000 norm=1.4800\n",
      "[iter 0] loss=1.3891 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 100] loss=1.3093 val_loss=0.0000 scale=1.0000 norm=1.4780\n",
      "[iter 0] loss=1.3891 val_loss=0.0000 scale=1.0000 norm=1.4648\n",
      "[iter 100] loss=1.3107 val_loss=0.0000 scale=1.0000 norm=1.4776\n",
      "[iter 0] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 100] loss=1.3105 val_loss=0.0000 scale=1.0000 norm=1.4785\n",
      "[iter 0] loss=1.3882 val_loss=0.0000 scale=1.0000 norm=1.4665\n",
      "[iter 100] loss=1.3107 val_loss=0.0000 scale=1.0000 norm=1.4800\n",
      "[iter 0] loss=1.3882 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 100] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.4806\n",
      "[iter 0] loss=1.3886 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 100] loss=1.3107 val_loss=0.0000 scale=1.0000 norm=1.4789\n",
      "[iter 0] loss=1.3891 val_loss=0.0000 scale=1.0000 norm=1.4648\n",
      "[iter 100] loss=1.3110 val_loss=0.0000 scale=1.0000 norm=1.4777\n",
      "[iter 0] loss=1.3895 val_loss=0.0000 scale=1.0000 norm=1.4641\n",
      "[iter 100] loss=1.3125 val_loss=0.0000 scale=1.0000 norm=1.4770\n",
      "[iter 0] loss=1.3895 val_loss=0.0000 scale=1.0000 norm=1.4640\n",
      "[iter 100] loss=1.3119 val_loss=0.0000 scale=1.0000 norm=1.4773\n",
      "[iter 0] loss=1.3894 val_loss=0.0000 scale=1.0000 norm=1.4642\n",
      "[iter 100] loss=1.3119 val_loss=0.0000 scale=1.0000 norm=1.4771\n",
      "[iter 0] loss=1.3895 val_loss=0.0000 scale=1.0000 norm=1.4640\n",
      "[iter 100] loss=1.3134 val_loss=0.0000 scale=1.0000 norm=1.4761\n",
      "[iter 0] loss=1.3888 val_loss=0.0000 scale=1.0000 norm=1.4648\n",
      "[iter 100] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=1.4771\n",
      "[iter 0] loss=1.3880 val_loss=0.0000 scale=1.0000 norm=1.4658\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4773\n",
      "[iter 0] loss=1.3882 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4774\n",
      "[iter 0] loss=1.3875 val_loss=0.0000 scale=1.0000 norm=1.4671\n",
      "[iter 100] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.4789\n",
      "[iter 0] loss=1.3883 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4771\n",
      "[iter 0] loss=1.3878 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 100] loss=1.3140 val_loss=0.0000 scale=1.0000 norm=1.4789\n",
      "[iter 0] loss=1.3879 val_loss=0.0000 scale=1.0000 norm=1.4669\n",
      "[iter 100] loss=1.3130 val_loss=0.0000 scale=1.0000 norm=1.4789\n",
      "[iter 0] loss=1.3872 val_loss=0.0000 scale=1.0000 norm=1.4679\n",
      "[iter 100] loss=1.3127 val_loss=0.0000 scale=1.0000 norm=1.4803\n",
      "[iter 0] loss=1.3872 val_loss=0.0000 scale=1.0000 norm=1.4681\n",
      "[iter 100] loss=1.3122 val_loss=0.0000 scale=1.0000 norm=1.4811\n",
      "[iter 0] loss=1.3871 val_loss=0.0000 scale=1.0000 norm=1.4682\n",
      "[iter 100] loss=1.3120 val_loss=0.0000 scale=1.0000 norm=1.4807\n",
      "[iter 0] loss=1.3796 val_loss=0.0000 scale=1.0000 norm=1.4783\n",
      "[iter 100] loss=1.3036 val_loss=0.0000 scale=1.0000 norm=1.4911\n",
      "[iter 0] loss=1.3666 val_loss=0.0000 scale=1.0000 norm=1.4944\n",
      "[iter 100] loss=1.2897 val_loss=0.0000 scale=1.0000 norm=1.5083\n",
      "[iter 0] loss=1.3656 val_loss=0.0000 scale=1.0000 norm=1.4960\n",
      "[iter 100] loss=1.2892 val_loss=0.0000 scale=1.0000 norm=1.5099\n",
      "[iter 0] loss=1.3667 val_loss=0.0000 scale=1.0000 norm=1.4944\n",
      "[iter 100] loss=1.2898 val_loss=0.0000 scale=1.0000 norm=1.5085\n",
      "[iter 0] loss=1.3341 val_loss=0.0000 scale=1.0000 norm=1.5311\n",
      "[iter 100] loss=1.2535 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 0] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.5344\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.5507\n",
      "[iter 0] loss=1.3329 val_loss=0.0000 scale=1.0000 norm=1.5340\n",
      "[iter 100] loss=1.2524 val_loss=0.0000 scale=1.0000 norm=1.5498\n",
      "[iter 0] loss=1.3332 val_loss=0.0000 scale=1.0000 norm=1.5335\n",
      "[iter 100] loss=1.2533 val_loss=0.0000 scale=1.0000 norm=1.5494\n",
      "[iter 0] loss=1.3332 val_loss=0.0000 scale=1.0000 norm=1.5337\n",
      "[iter 100] loss=1.2535 val_loss=0.0000 scale=1.0000 norm=1.5491\n",
      "[iter 0] loss=1.3331 val_loss=0.0000 scale=1.0000 norm=1.5341\n",
      "[iter 100] loss=1.2531 val_loss=0.0000 scale=1.0000 norm=1.5504\n",
      "[iter 0] loss=1.3329 val_loss=0.0000 scale=1.0000 norm=1.5345\n",
      "[iter 100] loss=1.2532 val_loss=0.0000 scale=1.0000 norm=1.5503\n",
      "[iter 0] loss=1.3333 val_loss=0.0000 scale=1.0000 norm=1.5346\n",
      "[iter 100] loss=1.2528 val_loss=0.0000 scale=1.0000 norm=1.5508\n",
      "[iter 0] loss=1.3333 val_loss=0.0000 scale=1.0000 norm=1.5351\n",
      "[iter 100] loss=1.2521 val_loss=0.0000 scale=1.0000 norm=1.5516\n",
      "[iter 0] loss=1.3344 val_loss=0.0000 scale=1.0000 norm=1.5336\n",
      "[iter 100] loss=1.2533 val_loss=0.0000 scale=1.0000 norm=1.5508\n",
      "[iter 0] loss=1.3344 val_loss=0.0000 scale=1.0000 norm=1.5336\n",
      "[iter 100] loss=1.2526 val_loss=0.0000 scale=1.0000 norm=1.5504\n",
      "[iter 0] loss=1.3348 val_loss=0.0000 scale=1.0000 norm=1.5328\n",
      "[iter 100] loss=1.2533 val_loss=0.0000 scale=1.0000 norm=1.5498\n",
      "[iter 0] loss=1.3336 val_loss=0.0000 scale=1.0000 norm=1.5348\n",
      "[iter 100] loss=1.2531 val_loss=0.0000 scale=1.0000 norm=1.5515\n",
      "[iter 0] loss=1.3331 val_loss=0.0000 scale=1.0000 norm=1.5348\n",
      "[iter 100] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=1.5511\n",
      "[iter 0] loss=1.3329 val_loss=0.0000 scale=1.0000 norm=1.5352\n",
      "[iter 100] loss=1.2537 val_loss=0.0000 scale=1.0000 norm=1.5520\n",
      "[iter 0] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.5349\n",
      "[iter 100] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=1.5518\n",
      "[iter 0] loss=1.3336 val_loss=0.0000 scale=1.0000 norm=1.5332\n",
      "[iter 100] loss=1.2554 val_loss=0.0000 scale=1.0000 norm=1.5501\n",
      "[iter 0] loss=1.3330 val_loss=0.0000 scale=1.0000 norm=1.5335\n",
      "[iter 100] loss=1.2552 val_loss=0.0000 scale=1.0000 norm=1.5499\n",
      "[iter 0] loss=1.3346 val_loss=0.0000 scale=1.0000 norm=1.5300\n",
      "[iter 100] loss=1.2581 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 0] loss=1.3345 val_loss=0.0000 scale=1.0000 norm=1.5302\n",
      "[iter 100] loss=1.2583 val_loss=0.0000 scale=1.0000 norm=1.5474\n",
      "[iter 0] loss=1.3346 val_loss=0.0000 scale=1.0000 norm=1.5300\n",
      "[iter 100] loss=1.2582 val_loss=0.0000 scale=1.0000 norm=1.5472\n",
      "[iter 0] loss=1.3342 val_loss=0.0000 scale=1.0000 norm=1.5302\n",
      "[iter 100] loss=1.2591 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 0] loss=1.3351 val_loss=0.0000 scale=1.0000 norm=1.5285\n",
      "[iter 100] loss=1.2599 val_loss=0.0000 scale=1.0000 norm=1.5450\n",
      "[iter 0] loss=1.3351 val_loss=0.0000 scale=1.0000 norm=1.5286\n",
      "[iter 100] loss=1.2593 val_loss=0.0000 scale=1.0000 norm=1.5450\n",
      "[iter 0] loss=1.3353 val_loss=0.0000 scale=1.0000 norm=1.5276\n",
      "[iter 100] loss=1.2612 val_loss=0.0000 scale=1.0000 norm=1.5437\n",
      "[iter 0] loss=1.3354 val_loss=0.0000 scale=1.0000 norm=1.5274\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.5437\n",
      "[iter 0] loss=1.3345 val_loss=0.0000 scale=1.0000 norm=1.5284\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.5449\n",
      "[iter 0] loss=1.3345 val_loss=0.0000 scale=1.0000 norm=1.5286\n",
      "[iter 100] loss=1.2600 val_loss=0.0000 scale=1.0000 norm=1.5447\n",
      "[iter 0] loss=1.3337 val_loss=0.0000 scale=1.0000 norm=1.5294\n",
      "[iter 100] loss=1.2605 val_loss=0.0000 scale=1.0000 norm=1.5451\n",
      "[iter 0] loss=1.3347 val_loss=0.0000 scale=1.0000 norm=1.5280\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.5438\n",
      "[iter 0] loss=1.3336 val_loss=0.0000 scale=1.0000 norm=1.5295\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.5453\n",
      "[iter 0] loss=1.3366 val_loss=0.0000 scale=1.0000 norm=1.5242\n",
      "[iter 100] loss=1.2649 val_loss=0.0000 scale=1.0000 norm=1.5394\n",
      "[iter 0] loss=1.3373 val_loss=0.0000 scale=1.0000 norm=1.5235\n",
      "[iter 100] loss=1.2653 val_loss=0.0000 scale=1.0000 norm=1.5388\n",
      "[iter 0] loss=1.3377 val_loss=0.0000 scale=1.0000 norm=1.5227\n",
      "[iter 100] loss=1.2648 val_loss=0.0000 scale=1.0000 norm=1.5380\n",
      "[iter 0] loss=1.3366 val_loss=0.0000 scale=1.0000 norm=1.5241\n",
      "[iter 100] loss=1.2646 val_loss=0.0000 scale=1.0000 norm=1.5391\n",
      "[iter 0] loss=1.3360 val_loss=0.0000 scale=1.0000 norm=1.5254\n",
      "[iter 100] loss=1.2638 val_loss=0.0000 scale=1.0000 norm=1.5403\n",
      "[iter 0] loss=1.3355 val_loss=0.0000 scale=1.0000 norm=1.5263\n",
      "[iter 100] loss=1.2638 val_loss=0.0000 scale=1.0000 norm=1.5414\n",
      "[iter 0] loss=1.3346 val_loss=0.0000 scale=1.0000 norm=1.5274\n",
      "[iter 100] loss=1.2626 val_loss=0.0000 scale=1.0000 norm=1.5413\n",
      "[iter 0] loss=1.3337 val_loss=0.0000 scale=1.0000 norm=1.5284\n",
      "[iter 100] loss=1.2622 val_loss=0.0000 scale=1.0000 norm=1.5413\n",
      "[iter 0] loss=1.3336 val_loss=0.0000 scale=1.0000 norm=1.5286\n",
      "[iter 100] loss=1.2619 val_loss=0.0000 scale=1.0000 norm=1.5414\n",
      "[iter 0] loss=1.3337 val_loss=0.0000 scale=1.0000 norm=1.5284\n",
      "[iter 100] loss=1.2611 val_loss=0.0000 scale=1.0000 norm=1.5411\n",
      "[iter 0] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.5302\n",
      "[iter 100] loss=1.2610 val_loss=0.0000 scale=1.0000 norm=1.5425\n",
      "[iter 0] loss=1.3316 val_loss=0.0000 scale=1.0000 norm=1.5315\n",
      "[iter 100] loss=1.2610 val_loss=0.0000 scale=1.0000 norm=1.5433\n",
      "[iter 0] loss=1.3316 val_loss=0.0000 scale=1.0000 norm=1.5314\n",
      "[iter 100] loss=1.2603 val_loss=0.0000 scale=1.0000 norm=1.5427\n",
      "[iter 0] loss=1.3311 val_loss=0.0000 scale=1.0000 norm=1.5324\n",
      "[iter 100] loss=1.2596 val_loss=0.0000 scale=1.0000 norm=1.5440\n",
      "[iter 0] loss=1.3312 val_loss=0.0000 scale=1.0000 norm=1.5323\n",
      "[iter 100] loss=1.2601 val_loss=0.0000 scale=1.0000 norm=1.5434\n",
      "[iter 0] loss=1.3308 val_loss=0.0000 scale=1.0000 norm=1.5321\n",
      "[iter 100] loss=1.2616 val_loss=0.0000 scale=1.0000 norm=1.5431\n",
      "[iter 0] loss=1.3313 val_loss=0.0000 scale=1.0000 norm=1.5304\n",
      "[iter 100] loss=1.2643 val_loss=0.0000 scale=1.0000 norm=1.5411\n",
      "[iter 0] loss=1.3337 val_loss=0.0000 scale=1.0000 norm=1.5261\n",
      "[iter 100] loss=1.2688 val_loss=0.0000 scale=1.0000 norm=1.5370\n",
      "[iter 0] loss=1.3335 val_loss=0.0000 scale=1.0000 norm=1.5265\n",
      "[iter 100] loss=1.2679 val_loss=0.0000 scale=1.0000 norm=1.5374\n",
      "[iter 0] loss=1.3331 val_loss=0.0000 scale=1.0000 norm=1.5270\n",
      "[iter 100] loss=1.2676 val_loss=0.0000 scale=1.0000 norm=1.5377\n",
      "[iter 0] loss=1.3346 val_loss=0.0000 scale=1.0000 norm=1.5237\n",
      "[iter 100] loss=1.2704 val_loss=0.0000 scale=1.0000 norm=1.5345\n",
      "[iter 0] loss=1.3346 val_loss=0.0000 scale=1.0000 norm=1.5237\n",
      "[iter 100] loss=1.2694 val_loss=0.0000 scale=1.0000 norm=1.5345\n",
      "[iter 0] loss=1.3341 val_loss=0.0000 scale=1.0000 norm=1.5237\n",
      "[iter 100] loss=1.2694 val_loss=0.0000 scale=1.0000 norm=1.5344\n",
      "[iter 0] loss=1.3343 val_loss=0.0000 scale=1.0000 norm=1.5235\n",
      "[iter 100] loss=1.2688 val_loss=0.0000 scale=1.0000 norm=1.5345\n",
      "[iter 0] loss=1.3338 val_loss=0.0000 scale=1.0000 norm=1.5242\n",
      "[iter 100] loss=1.2685 val_loss=0.0000 scale=1.0000 norm=1.5355\n",
      "[iter 0] loss=1.3338 val_loss=0.0000 scale=1.0000 norm=1.5235\n",
      "[iter 100] loss=1.2684 val_loss=0.0000 scale=1.0000 norm=1.5345\n",
      "[iter 0] loss=1.3338 val_loss=0.0000 scale=1.0000 norm=1.5236\n",
      "[iter 100] loss=1.2683 val_loss=0.0000 scale=1.0000 norm=1.5348\n",
      "[iter 0] loss=1.3332 val_loss=0.0000 scale=1.0000 norm=1.5239\n",
      "[iter 100] loss=1.2681 val_loss=0.0000 scale=1.0000 norm=1.5350\n",
      "[iter 0] loss=1.3338 val_loss=0.0000 scale=1.0000 norm=1.5228\n",
      "[iter 100] loss=1.2685 val_loss=0.0000 scale=1.0000 norm=1.5336\n",
      "[iter 0] loss=1.3331 val_loss=0.0000 scale=1.0000 norm=1.5241\n",
      "[iter 100] loss=1.2678 val_loss=0.0000 scale=1.0000 norm=1.5347\n",
      "[iter 0] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.5249\n",
      "[iter 100] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=1.5359\n",
      "[iter 0] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.5247\n",
      "[iter 100] loss=1.2669 val_loss=0.0000 scale=1.0000 norm=1.5358\n",
      "[iter 0] loss=1.3328 val_loss=0.0000 scale=1.0000 norm=1.5245\n",
      "[iter 100] loss=1.2668 val_loss=0.0000 scale=1.0000 norm=1.5356\n",
      "[iter 0] loss=1.3317 val_loss=0.0000 scale=1.0000 norm=1.5263\n",
      "[iter 100] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.5371\n",
      "[iter 0] loss=1.3319 val_loss=0.0000 scale=1.0000 norm=1.5258\n",
      "[iter 100] loss=1.2653 val_loss=0.0000 scale=1.0000 norm=1.5365\n",
      "[iter 0] loss=1.3310 val_loss=0.0000 scale=1.0000 norm=1.5274\n",
      "[iter 100] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=1.5384\n",
      "[iter 0] loss=1.3304 val_loss=0.0000 scale=1.0000 norm=1.5285\n",
      "[iter 100] loss=1.2644 val_loss=0.0000 scale=1.0000 norm=1.5400\n",
      "[iter 0] loss=1.3301 val_loss=0.0000 scale=1.0000 norm=1.5292\n",
      "[iter 100] loss=1.2634 val_loss=0.0000 scale=1.0000 norm=1.5406\n",
      "[iter 0] loss=1.3299 val_loss=0.0000 scale=1.0000 norm=1.5295\n",
      "[iter 100] loss=1.2622 val_loss=0.0000 scale=1.0000 norm=1.5410\n",
      "[iter 0] loss=1.3288 val_loss=0.0000 scale=1.0000 norm=1.5311\n",
      "[iter 100] loss=1.2616 val_loss=0.0000 scale=1.0000 norm=1.5425\n",
      "[iter 0] loss=1.3285 val_loss=0.0000 scale=1.0000 norm=1.5316\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.5435\n",
      "[iter 0] loss=1.3280 val_loss=0.0000 scale=1.0000 norm=1.5318\n",
      "[iter 100] loss=1.2607 val_loss=0.0000 scale=1.0000 norm=1.5433\n",
      "[iter 0] loss=1.3278 val_loss=0.0000 scale=1.0000 norm=1.5321\n",
      "[iter 100] loss=1.2590 val_loss=0.0000 scale=1.0000 norm=1.5436\n",
      "[iter 0] loss=1.3272 val_loss=0.0000 scale=1.0000 norm=1.5334\n",
      "[iter 100] loss=1.2584 val_loss=0.0000 scale=1.0000 norm=1.5445\n",
      "[iter 0] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.5344\n",
      "[iter 100] loss=1.2587 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 0] loss=1.3266 val_loss=0.0000 scale=1.0000 norm=1.5344\n",
      "[iter 100] loss=1.2581 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.5356\n",
      "[iter 100] loss=1.2574 val_loss=0.0000 scale=1.0000 norm=1.5472\n",
      "[iter 0] loss=1.3263 val_loss=0.0000 scale=1.0000 norm=1.5343\n",
      "[iter 100] loss=1.2591 val_loss=0.0000 scale=1.0000 norm=1.5457\n",
      "[iter 0] loss=1.3265 val_loss=0.0000 scale=1.0000 norm=1.5341\n",
      "[iter 100] loss=1.2590 val_loss=0.0000 scale=1.0000 norm=1.5447\n",
      "[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=1.5350\n",
      "[iter 100] loss=1.2587 val_loss=0.0000 scale=1.0000 norm=1.5456\n",
      "[iter 0] loss=1.3261 val_loss=0.0000 scale=1.0000 norm=1.5347\n",
      "[iter 100] loss=1.2584 val_loss=0.0000 scale=1.0000 norm=1.5450\n",
      "[iter 0] loss=1.3255 val_loss=0.0000 scale=1.0000 norm=1.5357\n",
      "[iter 100] loss=1.2578 val_loss=0.0000 scale=1.0000 norm=1.5463\n",
      "[iter 0] loss=1.3249 val_loss=0.0000 scale=1.0000 norm=1.5366\n",
      "[iter 100] loss=1.2575 val_loss=0.0000 scale=1.0000 norm=1.5474\n",
      "[iter 0] loss=1.3250 val_loss=0.0000 scale=1.0000 norm=1.5362\n",
      "[iter 100] loss=1.2575 val_loss=0.0000 scale=1.0000 norm=1.5473\n",
      "[iter 0] loss=1.3257 val_loss=0.0000 scale=1.0000 norm=1.5350\n",
      "[iter 100] loss=1.2559 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 0] loss=1.3257 val_loss=0.0000 scale=1.0000 norm=1.5349\n",
      "[iter 100] loss=1.2552 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 0] loss=1.3254 val_loss=0.0000 scale=1.0000 norm=1.5356\n",
      "[iter 100] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=1.5469\n",
      "[iter 0] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.5367\n",
      "[iter 100] loss=1.2547 val_loss=0.0000 scale=1.0000 norm=1.5474\n",
      "[iter 0] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.5366\n",
      "[iter 100] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.5475\n",
      "[iter 0] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.5374\n",
      "[iter 100] loss=1.2550 val_loss=0.0000 scale=1.0000 norm=1.5478\n",
      "[iter 0] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.5373\n",
      "[iter 100] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=1.5480\n",
      "[iter 0] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.5368\n",
      "[iter 100] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 0] loss=1.3246 val_loss=0.0000 scale=1.0000 norm=1.5362\n",
      "[iter 100] loss=1.2539 val_loss=0.0000 scale=1.0000 norm=1.5481\n",
      "[iter 0] loss=1.3248 val_loss=0.0000 scale=1.0000 norm=1.5356\n",
      "[iter 100] loss=1.2524 val_loss=0.0000 scale=1.0000 norm=1.5469\n",
      "[iter 0] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.5374\n",
      "[iter 100] loss=1.2511 val_loss=0.0000 scale=1.0000 norm=1.5486\n",
      "[iter 0] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.5385\n",
      "[iter 100] loss=1.2501 val_loss=0.0000 scale=1.0000 norm=1.5493\n",
      "[iter 0] loss=1.3227 val_loss=0.0000 scale=1.0000 norm=1.5385\n",
      "[iter 100] loss=1.2494 val_loss=0.0000 scale=1.0000 norm=1.5496\n",
      "[iter 0] loss=1.3232 val_loss=0.0000 scale=1.0000 norm=1.5376\n",
      "[iter 100] loss=1.2503 val_loss=0.0000 scale=1.0000 norm=1.5484\n",
      "[iter 0] loss=1.3227 val_loss=0.0000 scale=1.0000 norm=1.5381\n",
      "[iter 100] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=1.5488\n",
      "[iter 0] loss=1.3242 val_loss=0.0000 scale=1.0000 norm=1.5350\n",
      "[iter 100] loss=1.2529 val_loss=0.0000 scale=1.0000 norm=1.5464\n",
      "[iter 0] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.5352\n",
      "[iter 100] loss=1.2514 val_loss=0.0000 scale=1.0000 norm=1.5471\n",
      "[iter 0] loss=1.3233 val_loss=0.0000 scale=1.0000 norm=1.5364\n",
      "[iter 100] loss=1.2514 val_loss=0.0000 scale=1.0000 norm=1.5486\n",
      "[iter 0] loss=1.3226 val_loss=0.0000 scale=1.0000 norm=1.5377\n",
      "[iter 100] loss=1.2508 val_loss=0.0000 scale=1.0000 norm=1.5493\n",
      "[iter 0] loss=1.3226 val_loss=0.0000 scale=1.0000 norm=1.5378\n",
      "[iter 100] loss=1.2491 val_loss=0.0000 scale=1.0000 norm=1.5489\n",
      "[iter 0] loss=1.3216 val_loss=0.0000 scale=1.0000 norm=1.5395\n",
      "[iter 100] loss=1.2477 val_loss=0.0000 scale=1.0000 norm=1.5511\n",
      "[iter 0] loss=1.3219 val_loss=0.0000 scale=1.0000 norm=1.5390\n",
      "[iter 100] loss=1.2456 val_loss=0.0000 scale=1.0000 norm=1.5509\n",
      "[iter 0] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.5407\n",
      "[iter 100] loss=1.2455 val_loss=0.0000 scale=1.0000 norm=1.5517\n",
      "[iter 0] loss=1.3206 val_loss=0.0000 scale=1.0000 norm=1.5410\n",
      "[iter 100] loss=1.2438 val_loss=0.0000 scale=1.0000 norm=1.5528\n",
      "[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.5426\n",
      "[iter 100] loss=1.2432 val_loss=0.0000 scale=1.0000 norm=1.5547\n",
      "[iter 0] loss=1.3188 val_loss=0.0000 scale=1.0000 norm=1.5440\n",
      "[iter 100] loss=1.2418 val_loss=0.0000 scale=1.0000 norm=1.5559\n",
      "[iter 0] loss=1.3240 val_loss=0.0000 scale=1.0000 norm=1.5358\n",
      "[iter 100] loss=1.2478 val_loss=0.0000 scale=1.0000 norm=1.5479\n",
      "[iter 0] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.5356\n",
      "[iter 100] loss=1.2473 val_loss=0.0000 scale=1.0000 norm=1.5475\n",
      "[iter 0] loss=1.3232 val_loss=0.0000 scale=1.0000 norm=1.5371\n",
      "[iter 100] loss=1.2469 val_loss=0.0000 scale=1.0000 norm=1.5489\n",
      "[iter 0] loss=1.3230 val_loss=0.0000 scale=1.0000 norm=1.5375\n",
      "[iter 100] loss=1.2468 val_loss=0.0000 scale=1.0000 norm=1.5486\n",
      "[iter 0] loss=1.3228 val_loss=0.0000 scale=1.0000 norm=1.5378\n",
      "[iter 100] loss=1.2475 val_loss=0.0000 scale=1.0000 norm=1.5494\n",
      "[iter 0] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.5389\n",
      "[iter 100] loss=1.2483 val_loss=0.0000 scale=1.0000 norm=1.5509\n",
      "[iter 0] loss=1.3212 val_loss=0.0000 scale=1.0000 norm=1.5399\n",
      "[iter 100] loss=1.2471 val_loss=0.0000 scale=1.0000 norm=1.5515\n",
      "[iter 0] loss=1.3204 val_loss=0.0000 scale=1.0000 norm=1.5414\n",
      "[iter 100] loss=1.2447 val_loss=0.0000 scale=1.0000 norm=1.5512\n",
      "[iter 0] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.5429\n",
      "[iter 100] loss=1.2442 val_loss=0.0000 scale=1.0000 norm=1.5526\n",
      "[iter 0] loss=1.3183 val_loss=0.0000 scale=1.0000 norm=1.5445\n",
      "[iter 100] loss=1.2430 val_loss=0.0000 scale=1.0000 norm=1.5540\n",
      "[iter 0] loss=1.3182 val_loss=0.0000 scale=1.0000 norm=1.5447\n",
      "[iter 100] loss=1.2421 val_loss=0.0000 scale=1.0000 norm=1.5541\n",
      "[iter 0] loss=1.3170 val_loss=0.0000 scale=1.0000 norm=1.5463\n",
      "[iter 100] loss=1.2414 val_loss=0.0000 scale=1.0000 norm=1.5555\n",
      "[iter 0] loss=1.3161 val_loss=0.0000 scale=1.0000 norm=1.5471\n",
      "[iter 100] loss=1.2404 val_loss=0.0000 scale=1.0000 norm=1.5567\n",
      "[iter 0] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.5485\n",
      "[iter 100] loss=1.2398 val_loss=0.0000 scale=1.0000 norm=1.5579\n",
      "[iter 0] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.5483\n",
      "[iter 100] loss=1.2409 val_loss=0.0000 scale=1.0000 norm=1.5572\n",
      "[iter 0] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.5467\n",
      "[iter 100] loss=1.2425 val_loss=0.0000 scale=1.0000 norm=1.5564\n",
      "[iter 0] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.5477\n",
      "[iter 100] loss=1.2437 val_loss=0.0000 scale=0.5000 norm=0.7788\n",
      "[iter 0] loss=1.3152 val_loss=0.0000 scale=1.0000 norm=1.5476\n",
      "[iter 100] loss=1.2435 val_loss=0.0000 scale=1.0000 norm=1.5572\n",
      "[iter 0] loss=1.3154 val_loss=0.0000 scale=1.0000 norm=1.5464\n",
      "[iter 100] loss=1.2439 val_loss=0.0000 scale=1.0000 norm=1.5555\n",
      "[iter 0] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.5475\n",
      "[iter 100] loss=1.2426 val_loss=0.0000 scale=1.0000 norm=1.5565\n",
      "[iter 0] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.5476\n",
      "[iter 100] loss=1.2426 val_loss=0.0000 scale=1.0000 norm=1.5571\n",
      "[iter 0] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.5477\n",
      "[iter 100] loss=1.2424 val_loss=0.0000 scale=1.0000 norm=1.5574\n",
      "[iter 0] loss=1.3149 val_loss=0.0000 scale=1.0000 norm=1.5465\n",
      "[iter 100] loss=1.2434 val_loss=0.0000 scale=1.0000 norm=1.5558\n",
      "[iter 0] loss=1.3142 val_loss=0.0000 scale=1.0000 norm=1.5466\n",
      "[iter 100] loss=1.2435 val_loss=0.0000 scale=1.0000 norm=1.5564\n",
      "[iter 0] loss=1.3142 val_loss=0.0000 scale=1.0000 norm=1.5457\n",
      "[iter 100] loss=1.2440 val_loss=0.0000 scale=1.0000 norm=1.5552\n",
      "[iter 0] loss=1.3136 val_loss=0.0000 scale=1.0000 norm=1.5467\n",
      "[iter 100] loss=1.2429 val_loss=0.0000 scale=1.0000 norm=1.5565\n",
      "[iter 0] loss=1.3134 val_loss=0.0000 scale=1.0000 norm=1.5470\n",
      "[iter 100] loss=1.2417 val_loss=0.0000 scale=1.0000 norm=1.5569\n",
      "[iter 0] loss=1.3136 val_loss=0.0000 scale=1.0000 norm=1.5469\n",
      "[iter 100] loss=1.2413 val_loss=0.0000 scale=1.0000 norm=1.5568\n",
      "[iter 0] loss=1.3127 val_loss=0.0000 scale=1.0000 norm=1.5476\n",
      "[iter 100] loss=1.2412 val_loss=0.0000 scale=1.0000 norm=1.5571\n",
      "[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.5492\n",
      "[iter 100] loss=1.2396 val_loss=0.0000 scale=1.0000 norm=1.5580\n",
      "[iter 0] loss=1.3117 val_loss=0.0000 scale=1.0000 norm=1.5491\n",
      "[iter 100] loss=1.2399 val_loss=0.0000 scale=1.0000 norm=1.5574\n",
      "[iter 0] loss=1.3109 val_loss=0.0000 scale=1.0000 norm=1.5505\n",
      "[iter 100] loss=1.2400 val_loss=0.0000 scale=1.0000 norm=1.5589\n",
      "[iter 0] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.5517\n",
      "[iter 100] loss=1.2390 val_loss=0.0000 scale=1.0000 norm=1.5604\n",
      "[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=1.5502\n",
      "[iter 100] loss=1.2422 val_loss=0.0000 scale=1.0000 norm=1.5579\n",
      "[iter 0] loss=1.3101 val_loss=0.0000 scale=1.0000 norm=1.5515\n",
      "[iter 100] loss=1.2416 val_loss=0.0000 scale=1.0000 norm=1.5601\n",
      "[iter 0] loss=1.3109 val_loss=0.0000 scale=1.0000 norm=1.5502\n",
      "[iter 100] loss=1.2416 val_loss=0.0000 scale=0.5000 norm=0.7798\n",
      "[iter 0] loss=1.3105 val_loss=0.0000 scale=1.0000 norm=1.5500\n",
      "[iter 100] loss=1.2425 val_loss=0.0000 scale=1.0000 norm=1.5581\n",
      "[iter 0] loss=1.3105 val_loss=0.0000 scale=1.0000 norm=1.5502\n",
      "[iter 100] loss=1.2422 val_loss=0.0000 scale=1.0000 norm=1.5585\n",
      "[iter 0] loss=1.3094 val_loss=0.0000 scale=1.0000 norm=1.5519\n",
      "[iter 100] loss=1.2420 val_loss=0.0000 scale=1.0000 norm=1.5605\n",
      "[iter 0] loss=1.3087 val_loss=0.0000 scale=1.0000 norm=1.5533\n",
      "[iter 100] loss=1.2414 val_loss=0.0000 scale=1.0000 norm=1.5620\n",
      "[iter 0] loss=1.3098 val_loss=0.0000 scale=1.0000 norm=1.5520\n",
      "[iter 100] loss=1.2425 val_loss=0.0000 scale=1.0000 norm=1.5600\n",
      "[iter 0] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.5528\n",
      "[iter 100] loss=1.2417 val_loss=0.0000 scale=1.0000 norm=1.5604\n",
      "[iter 0] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.5358\n",
      "[iter 100] loss=1.2563 val_loss=0.0000 scale=1.0000 norm=1.5426\n",
      "[iter 0] loss=1.3213 val_loss=0.0000 scale=1.0000 norm=1.5355\n",
      "[iter 100] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.5417\n",
      "[iter 0] loss=1.3204 val_loss=0.0000 scale=1.0000 norm=1.5370\n",
      "[iter 100] loss=1.2563 val_loss=0.0000 scale=1.0000 norm=1.5432\n",
      "[iter 0] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.5364\n",
      "[iter 100] loss=1.2574 val_loss=0.0000 scale=0.5000 norm=0.7711\n",
      "[iter 0] loss=1.3210 val_loss=0.0000 scale=1.0000 norm=1.5367\n",
      "[iter 100] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.5422\n",
      "[iter 0] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.5355\n",
      "[iter 100] loss=1.2583 val_loss=0.0000 scale=1.0000 norm=1.5412\n",
      "[iter 0] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.5346\n",
      "[iter 100] loss=1.2584 val_loss=0.0000 scale=1.0000 norm=1.5404\n",
      "[iter 0] loss=1.3223 val_loss=0.0000 scale=1.0000 norm=1.5347\n",
      "[iter 100] loss=1.2584 val_loss=0.0000 scale=1.0000 norm=1.5404\n",
      "[iter 0] loss=1.3215 val_loss=0.0000 scale=1.0000 norm=1.5358\n",
      "[iter 100] loss=1.2570 val_loss=0.0000 scale=1.0000 norm=1.5411\n",
      "[iter 0] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.5353\n",
      "[iter 100] loss=1.2574 val_loss=0.0000 scale=1.0000 norm=1.5404\n",
      "[iter 0] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.5353\n",
      "[iter 100] loss=1.2577 val_loss=0.0000 scale=1.0000 norm=1.5409\n",
      "[iter 0] loss=1.3205 val_loss=0.0000 scale=1.0000 norm=1.5371\n",
      "[iter 100] loss=1.2569 val_loss=0.0000 scale=1.0000 norm=1.5418\n",
      "[iter 0] loss=1.3204 val_loss=0.0000 scale=1.0000 norm=1.5374\n",
      "[iter 100] loss=1.2561 val_loss=0.0000 scale=1.0000 norm=1.5421\n",
      "[iter 0] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.5354\n",
      "[iter 100] loss=1.2582 val_loss=0.0000 scale=1.0000 norm=1.5400\n",
      "[iter 0] loss=1.3214 val_loss=0.0000 scale=1.0000 norm=1.5349\n",
      "[iter 100] loss=1.2579 val_loss=0.0000 scale=1.0000 norm=1.5397\n",
      "[iter 0] loss=1.3214 val_loss=0.0000 scale=1.0000 norm=1.5349\n",
      "[iter 100] loss=1.2573 val_loss=0.0000 scale=1.0000 norm=1.5395\n",
      "[iter 0] loss=1.3205 val_loss=0.0000 scale=1.0000 norm=1.5355\n",
      "[iter 100] loss=1.2562 val_loss=0.0000 scale=1.0000 norm=1.5403\n",
      "[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=1.5375\n",
      "[iter 100] loss=1.2555 val_loss=0.0000 scale=1.0000 norm=1.5420\n",
      "[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=1.5359\n",
      "[iter 100] loss=1.2561 val_loss=0.0000 scale=1.0000 norm=1.5403\n",
      "[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=1.5364\n",
      "[iter 100] loss=1.2561 val_loss=0.0000 scale=1.0000 norm=1.5407\n",
      "[iter 0] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.5377\n",
      "[iter 100] loss=1.2559 val_loss=0.0000 scale=1.0000 norm=1.5419\n",
      "[iter 0] loss=1.3194 val_loss=0.0000 scale=1.0000 norm=1.5374\n",
      "[iter 100] loss=1.2561 val_loss=0.0000 scale=1.0000 norm=1.5415\n",
      "[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.5373\n",
      "[iter 100] loss=1.2564 val_loss=0.0000 scale=1.0000 norm=1.5415\n",
      "[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=1.5373\n",
      "[iter 100] loss=1.2564 val_loss=0.0000 scale=1.0000 norm=1.5416\n",
      "[iter 0] loss=1.3196 val_loss=0.0000 scale=1.0000 norm=1.5361\n",
      "[iter 100] loss=1.2573 val_loss=0.0000 scale=1.0000 norm=1.5401\n",
      "[iter 0] loss=1.3185 val_loss=0.0000 scale=1.0000 norm=1.5377\n",
      "[iter 100] loss=1.2571 val_loss=0.0000 scale=1.0000 norm=1.5420\n",
      "[iter 0] loss=1.3176 val_loss=0.0000 scale=1.0000 norm=1.5394\n",
      "[iter 100] loss=1.2570 val_loss=0.0000 scale=1.0000 norm=1.5436\n",
      "[iter 0] loss=1.3176 val_loss=0.0000 scale=1.0000 norm=1.5392\n",
      "[iter 100] loss=1.2562 val_loss=0.0000 scale=1.0000 norm=1.5434\n",
      "[iter 0] loss=1.3169 val_loss=0.0000 scale=1.0000 norm=1.5403\n",
      "[iter 100] loss=1.2541 val_loss=0.0000 scale=1.0000 norm=1.5444\n",
      "[iter 0] loss=1.3156 val_loss=0.0000 scale=1.0000 norm=1.5418\n",
      "[iter 100] loss=1.2533 val_loss=0.0000 scale=1.0000 norm=1.5455\n",
      "[iter 0] loss=1.3162 val_loss=0.0000 scale=1.0000 norm=1.5410\n",
      "[iter 100] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=1.5438\n",
      "[iter 0] loss=1.3158 val_loss=0.0000 scale=1.0000 norm=1.5418\n",
      "[iter 100] loss=1.2531 val_loss=0.0000 scale=1.0000 norm=1.5450\n",
      "[iter 0] loss=1.3149 val_loss=0.0000 scale=1.0000 norm=1.5430\n",
      "[iter 100] loss=1.2523 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=1.5441\n",
      "[iter 100] loss=1.2517 val_loss=0.0000 scale=1.0000 norm=1.5471\n",
      "[iter 0] loss=1.3142 val_loss=0.0000 scale=1.0000 norm=1.5437\n",
      "[iter 100] loss=1.2521 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 0] loss=1.3138 val_loss=0.0000 scale=1.0000 norm=1.5446\n",
      "[iter 100] loss=1.2511 val_loss=0.0000 scale=1.0000 norm=1.5475\n",
      "[iter 0] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.5449\n",
      "[iter 100] loss=1.2513 val_loss=0.0000 scale=1.0000 norm=1.5480\n",
      "[iter 0] loss=1.3124 val_loss=0.0000 scale=1.0000 norm=1.5468\n",
      "[iter 100] loss=1.2486 val_loss=0.0000 scale=1.0000 norm=1.5493\n",
      "[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.5470\n",
      "[iter 100] loss=1.2476 val_loss=0.0000 scale=1.0000 norm=1.5493\n",
      "[iter 0] loss=1.3123 val_loss=0.0000 scale=1.0000 norm=1.5459\n",
      "[iter 100] loss=1.2477 val_loss=0.0000 scale=1.0000 norm=1.5486\n",
      "[iter 0] loss=1.3119 val_loss=0.0000 scale=1.0000 norm=1.5469\n",
      "[iter 100] loss=1.2476 val_loss=0.0000 scale=1.0000 norm=1.5499\n",
      "[iter 0] loss=1.3109 val_loss=0.0000 scale=1.0000 norm=1.5484\n",
      "[iter 100] loss=1.2473 val_loss=0.0000 scale=1.0000 norm=1.5515\n",
      "[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.5477\n",
      "[iter 100] loss=1.2482 val_loss=0.0000 scale=1.0000 norm=1.5508\n",
      "[iter 0] loss=1.3127 val_loss=0.0000 scale=1.0000 norm=1.5463\n",
      "[iter 100] loss=1.2492 val_loss=0.0000 scale=1.0000 norm=1.5495\n",
      "[iter 0] loss=1.3127 val_loss=0.0000 scale=1.0000 norm=1.5463\n",
      "[iter 100] loss=1.2491 val_loss=0.0000 scale=1.0000 norm=1.5496\n",
      "[iter 0] loss=1.3114 val_loss=0.0000 scale=1.0000 norm=1.5480\n",
      "[iter 100] loss=1.2470 val_loss=0.0000 scale=1.0000 norm=1.5504\n",
      "[iter 0] loss=1.3107 val_loss=0.0000 scale=1.0000 norm=1.5492\n",
      "[iter 100] loss=1.2471 val_loss=0.0000 scale=1.0000 norm=1.5519\n",
      "[iter 0] loss=1.3097 val_loss=0.0000 scale=1.0000 norm=1.5502\n",
      "[iter 100] loss=1.2477 val_loss=0.0000 scale=1.0000 norm=1.5531\n",
      "[iter 0] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.5520\n",
      "[iter 100] loss=1.2469 val_loss=0.0000 scale=1.0000 norm=1.5550\n",
      "[iter 0] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.5519\n",
      "[iter 100] loss=1.2467 val_loss=0.0000 scale=1.0000 norm=1.5552\n",
      "[iter 0] loss=1.3093 val_loss=0.0000 scale=1.0000 norm=1.5510\n",
      "[iter 100] loss=1.2466 val_loss=0.0000 scale=1.0000 norm=1.5543\n",
      "[iter 0] loss=1.3086 val_loss=0.0000 scale=1.0000 norm=1.5521\n",
      "[iter 100] loss=1.2461 val_loss=0.0000 scale=1.0000 norm=1.5558\n",
      "[iter 0] loss=1.3096 val_loss=0.0000 scale=1.0000 norm=1.5504\n",
      "[iter 100] loss=1.2478 val_loss=0.0000 scale=1.0000 norm=1.5543\n",
      "[iter 0] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.5512\n",
      "[iter 100] loss=1.2467 val_loss=0.0000 scale=1.0000 norm=1.5547\n",
      "[iter 0] loss=1.3089 val_loss=0.0000 scale=1.0000 norm=1.5510\n",
      "[iter 100] loss=1.2463 val_loss=0.0000 scale=1.0000 norm=1.5539\n",
      "[iter 0] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.5513\n",
      "[iter 100] loss=1.2464 val_loss=0.0000 scale=1.0000 norm=1.5544\n",
      "[iter 0] loss=1.3082 val_loss=0.0000 scale=1.0000 norm=1.5528\n",
      "[iter 100] loss=1.2456 val_loss=0.0000 scale=1.0000 norm=1.5556\n",
      "[iter 0] loss=1.3073 val_loss=0.0000 scale=1.0000 norm=1.5540\n",
      "[iter 100] loss=1.2447 val_loss=0.0000 scale=1.0000 norm=1.5570\n",
      "[iter 0] loss=1.3067 val_loss=0.0000 scale=1.0000 norm=1.5553\n",
      "[iter 100] loss=1.2446 val_loss=0.0000 scale=1.0000 norm=1.5580\n",
      "[iter 0] loss=1.3070 val_loss=0.0000 scale=1.0000 norm=1.5552\n",
      "[iter 100] loss=1.2448 val_loss=0.0000 scale=1.0000 norm=1.5574\n",
      "[iter 0] loss=1.3057 val_loss=0.0000 scale=1.0000 norm=1.5572\n",
      "[iter 100] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=1.5594\n",
      "[iter 0] loss=1.3047 val_loss=0.0000 scale=1.0000 norm=1.5591\n",
      "[iter 100] loss=1.2431 val_loss=0.0000 scale=1.0000 norm=1.5619\n",
      "[iter 0] loss=1.3050 val_loss=0.0000 scale=1.0000 norm=1.5586\n",
      "[iter 100] loss=1.2429 val_loss=0.0000 scale=1.0000 norm=1.5615\n",
      "[iter 0] loss=1.3043 val_loss=0.0000 scale=1.0000 norm=1.5596\n",
      "[iter 100] loss=1.2424 val_loss=0.0000 scale=1.0000 norm=1.5631\n"
     ]
    }
   ],
   "source": [
    "start_test_idx = 252 * 3\n",
    "\n",
    "res_sent = run_expanding_backtest_ngboost(\n",
    "    X,\n",
    "    y,\n",
    "    start_test_idx=start_test_idx,\n",
    "    var_levels=[0.95, 0.99],\n",
    "    es_mc_samples=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "204d6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sent.to_csv(\"../data/processed/sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7f46e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation rate 95: 0.06785243741765482\n",
      "Violation rate 99: 0.013175230566534914\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(res_sent[\"y_real\"], label = \"Return\")\n",
    "plt.plot(res_sent[\"VaR_95\"], label = \"VaR_95\")\n",
    "\n",
    "res_sent[\"violation_95\"] = (res_sent[\"y_real\"] < res_sent[\"VaR_95\"]).astype(int)\n",
    "res_sent[\"violation_99\"] = (res_sent[\"y_real\"] < res_sent[\"VaR_99\"]).astype(int)\n",
    "\n",
    "violation_rate_95 = res_sent[\"violation_95\"].mean()\n",
    "violation_rate_99 = res_sent[\"violation_99\"].mean()\n",
    "\n",
    "print(\"Violation rate 95:\", violation_rate_95)\n",
    "print(\"Violation rate 99:\", violation_rate_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a84d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violation rate 95: 0.08010505581089954\n",
    "#Violation rate 99: 0.018384766907419567\n",
    "#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad62f8",
   "metadata": {},
   "source": [
    "Violation rate 95: 0.07156927117531188\n",
    "Violation rate 99: 0.013131976362442548 - 756"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
