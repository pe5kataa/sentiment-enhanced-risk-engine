{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c6ab868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      Price    Return\n",
      "0  2017-01-03  2257.8301       NaN\n",
      "1  2017-01-04  2270.7500  0.005706\n",
      "2  2017-01-05  2269.0000 -0.000771\n",
      "3  2017-01-06  2276.9800  0.003511\n",
      "4  2017-01-09  2268.8999 -0.003555\n",
      "        date      Price    Return  sent_mean  sent_median  sent_std  \\\n",
      "0 2017-01-03  2257.8301       NaN   0.042987     0.042987  0.000000   \n",
      "1 2017-01-04  2270.7500  0.005706   0.289485     0.289485  0.000000   \n",
      "2 2017-01-05  2269.0000 -0.000771   0.394566     0.457251  0.167108   \n",
      "3 2017-01-06  2276.9800  0.003511        NaN          NaN       NaN   \n",
      "4 2017-01-09  2268.8999 -0.003555        NaN          NaN       NaN   \n",
      "\n",
      "   news_count  frac_neg  frac_pos  frac_neu  \n",
      "0         1.0       0.0  1.000000  0.000000  \n",
      "1         1.0       0.0  0.000000  1.000000  \n",
      "2         6.0       0.0  0.333333  0.666667  \n",
      "3         NaN       NaN       NaN       NaN  \n",
      "4         NaN       NaN       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "sent_data = pd.read_csv(\"../data/processed/daily_sentiment_news_2017_2025.csv\")\n",
    "sp500_returns = pd.read_csv(\"../data/processed/SP500_returns.csv\")\n",
    "print(sp500_returns.head())\n",
    "\n",
    "sp500_returns[\"date\"] = pd.to_datetime(sp500_returns[\"date\"]).dt.normalize()\n",
    "\n",
    "sent_data[\"date\"] = pd.to_datetime(sent_data[\"date\"], utc=True) \\\n",
    "                   .dt.tz_convert(None) \\\n",
    "                   .dt.normalize()\n",
    "\n",
    "mixed_data = sp500_returns.merge(sent_data, on=\"date\", how=\"left\").sort_values(\"date\")\n",
    "\n",
    "print(mixed_data.head())\n",
    "mixed_data.to_csv(\"../data/processed/mixed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8a85463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Return</th>\n",
       "      <th>sent_mean</th>\n",
       "      <th>news_count</th>\n",
       "      <th>frac_neg</th>\n",
       "      <th>Return_lag1</th>\n",
       "      <th>Return_lag2</th>\n",
       "      <th>Return_lag5</th>\n",
       "      <th>sent_mean_lag1</th>\n",
       "      <th>sent_mean_lag2</th>\n",
       "      <th>sent_mean_lag5</th>\n",
       "      <th>news_count_lag1</th>\n",
       "      <th>news_count_lag2</th>\n",
       "      <th>news_count_lag5</th>\n",
       "      <th>frac_neg_lag1</th>\n",
       "      <th>frac_neg_lag2</th>\n",
       "      <th>frac_neg_lag5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    Return  sent_mean  news_count  frac_neg  Return_lag1  \\\n",
       "0 2017-01-03       NaN   0.042987         1.0       0.0          NaN   \n",
       "1 2017-01-04  0.005706   0.289485         1.0       0.0          NaN   \n",
       "2 2017-01-05 -0.000771   0.394566         6.0       0.0     0.005706   \n",
       "3 2017-01-06  0.003511   0.000000         0.0       0.0    -0.000771   \n",
       "4 2017-01-09 -0.003555   0.000000         0.0       0.0     0.003511   \n",
       "\n",
       "   Return_lag2  Return_lag5  sent_mean_lag1  sent_mean_lag2  sent_mean_lag5  \\\n",
       "0          NaN          NaN             NaN             NaN             NaN   \n",
       "1          NaN          NaN        0.042987             NaN             NaN   \n",
       "2          NaN          NaN        0.289485        0.042987             NaN   \n",
       "3     0.005706          NaN        0.394566        0.289485             NaN   \n",
       "4    -0.000771          NaN        0.000000        0.394566             NaN   \n",
       "\n",
       "   news_count_lag1  news_count_lag2  news_count_lag5  frac_neg_lag1  \\\n",
       "0              NaN              NaN              NaN            NaN   \n",
       "1              1.0              NaN              NaN            0.0   \n",
       "2              1.0              1.0              NaN            0.0   \n",
       "3              6.0              1.0              NaN            0.0   \n",
       "4              0.0              6.0              NaN            0.0   \n",
       "\n",
       "   frac_neg_lag2  frac_neg_lag5  \n",
       "0            NaN            NaN  \n",
       "1            NaN            NaN  \n",
       "2            0.0            NaN  \n",
       "3            0.0            NaN  \n",
       "4            0.0            NaN  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_features = [\"sent_mean\", \"sent_median\", \"sent_std\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\"]\n",
    "\n",
    "# Fill missing sentiment with neutral-ish defaults\n",
    "# (so trading days without news don't become NaN and get dropped)\n",
    "# mixed_data[\"has_news\"] = (mixed_data[\"news_count\"].notna()).astype(int)\n",
    "\n",
    "mixed_data[\"news_count\"] = mixed_data[\"news_count\"].fillna(0)\n",
    "\n",
    "for value in [\"sent_mean\", \"sent_median\", \"sent_std\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\"]:\n",
    "    mixed_data[value] = mixed_data[value].fillna(0.0)\n",
    "    \n",
    "#lags = [1,2,5]\n",
    "#for col in [\"Return\", \"sent_mean\", \"sent_median\", \"news_count\", \"frac_neg\", \"frac_pos\", \"frac_neu\"]:\n",
    "#    for lag in lags:\n",
    "#        mixed_data[f\"{col}_lag{lag}\"] = mixed_data[col].shift(lag)\n",
    "\n",
    "X = mixed_data[[\"date\",\"Return\", \"sent_mean\", \"news_count\", \"frac_neg\"]].copy()\n",
    "\n",
    "lags = [1,2,5]\n",
    "for col in [\"Return\", \"sent_mean\", \"news_count\", \"frac_neg\"]:\n",
    "    for lag in lags:\n",
    "        X[f\"{col}_lag{lag}\"] = X[col].shift(lag)\n",
    "   \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "950be1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return</th>\n",
       "      <th>sent_mean</th>\n",
       "      <th>news_count</th>\n",
       "      <th>frac_neg</th>\n",
       "      <th>Return_lag1</th>\n",
       "      <th>Return_lag2</th>\n",
       "      <th>Return_lag5</th>\n",
       "      <th>sent_mean_lag1</th>\n",
       "      <th>sent_mean_lag2</th>\n",
       "      <th>sent_mean_lag5</th>\n",
       "      <th>news_count_lag1</th>\n",
       "      <th>news_count_lag2</th>\n",
       "      <th>news_count_lag5</th>\n",
       "      <th>frac_neg_lag1</th>\n",
       "      <th>frac_neg_lag2</th>\n",
       "      <th>frac_neg_lag5</th>\n",
       "      <th>vol_30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Return  sent_mean  news_count  frac_neg  Return_lag1  \\\n",
       "date                                                                 \n",
       "2017-01-03       NaN   0.042987         1.0       0.0          NaN   \n",
       "2017-01-04  0.005706   0.289485         1.0       0.0          NaN   \n",
       "2017-01-05 -0.000771   0.394566         6.0       0.0     0.005706   \n",
       "2017-01-06  0.003511   0.000000         0.0       0.0    -0.000771   \n",
       "2017-01-09 -0.003555   0.000000         0.0       0.0     0.003511   \n",
       "\n",
       "            Return_lag2  Return_lag5  sent_mean_lag1  sent_mean_lag2  \\\n",
       "date                                                                   \n",
       "2017-01-03          NaN          NaN             NaN             NaN   \n",
       "2017-01-04          NaN          NaN        0.042987             NaN   \n",
       "2017-01-05          NaN          NaN        0.289485        0.042987   \n",
       "2017-01-06     0.005706          NaN        0.394566        0.289485   \n",
       "2017-01-09    -0.000771          NaN        0.000000        0.394566   \n",
       "\n",
       "            sent_mean_lag5  news_count_lag1  news_count_lag2  news_count_lag5  \\\n",
       "date                                                                            \n",
       "2017-01-03             NaN              NaN              NaN              NaN   \n",
       "2017-01-04             NaN              1.0              NaN              NaN   \n",
       "2017-01-05             NaN              1.0              1.0              NaN   \n",
       "2017-01-06             NaN              6.0              1.0              NaN   \n",
       "2017-01-09             NaN              0.0              6.0              NaN   \n",
       "\n",
       "            frac_neg_lag1  frac_neg_lag2  frac_neg_lag5  vol_30  \n",
       "date                                                             \n",
       "2017-01-03            NaN            NaN            NaN     NaN  \n",
       "2017-01-04            0.0            NaN            NaN     NaN  \n",
       "2017-01-05            0.0            0.0            NaN     NaN  \n",
       "2017-01-06            0.0            0.0            NaN     NaN  \n",
       "2017-01-09            0.0            0.0            NaN     NaN  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.set_index(\"date\", inplace=True)\n",
    "X[\"vol_30\"] = X[\"Return\"].rolling(5).std().shift(1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c1dc4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Return  sent_mean  news_count  frac_neg  Return_lag1  \\\n",
      "date                                                                 \n",
      "2017-01-12  0.002826   0.000000         0.0       0.0     0.000000   \n",
      "2017-01-13 -0.002147   0.000000         0.0       0.0     0.002826   \n",
      "2017-01-17  0.001848   0.000000         0.0       0.0    -0.002147   \n",
      "2017-01-18 -0.002972   0.000000         0.0       0.0     0.001848   \n",
      "2017-01-19  0.001762   0.052208         1.0       0.0    -0.002972   \n",
      "\n",
      "            Return_lag2  Return_lag5  sent_mean_lag1  sent_mean_lag2  \\\n",
      "date                                                                   \n",
      "2017-01-12    -0.003555     0.005706             0.0             0.0   \n",
      "2017-01-13     0.000000    -0.000771             0.0             0.0   \n",
      "2017-01-17     0.002826     0.003511             0.0             0.0   \n",
      "2017-01-18    -0.002147    -0.003555             0.0             0.0   \n",
      "2017-01-19     0.001848     0.000000             0.0             0.0   \n",
      "\n",
      "            sent_mean_lag5  news_count_lag1  news_count_lag2  news_count_lag5  \\\n",
      "date                                                                            \n",
      "2017-01-12        0.289485              0.0              0.0              1.0   \n",
      "2017-01-13        0.394566              0.0              0.0              6.0   \n",
      "2017-01-17        0.000000              0.0              0.0              0.0   \n",
      "2017-01-18        0.000000              0.0              0.0              0.0   \n",
      "2017-01-19        0.000000              0.0              0.0              0.0   \n",
      "\n",
      "            frac_neg_lag1  frac_neg_lag2  frac_neg_lag5    vol_30  \n",
      "date                                                               \n",
      "2017-01-12            0.0            0.0            0.0  0.003651  \n",
      "2017-01-13            0.0            0.0            0.0  0.002861  \n",
      "2017-01-17            0.0            0.0            0.0  0.003061  \n",
      "2017-01-18            0.0            0.0            0.0  0.002666  \n",
      "2017-01-19            0.0            0.0            0.0  0.002490  \n",
      "date\n",
      "2017-01-12   -0.002147\n",
      "2017-01-13    0.001848\n",
      "2017-01-17   -0.002972\n",
      "2017-01-18    0.001762\n",
      "2017-01-19   -0.003616\n",
      "Name: Return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = X['Return']\n",
    "X = X.shift(1)\n",
    "\n",
    "X = X.dropna()\n",
    "y = y.reindex(X.index)\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c5f3d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ngboost import NGBoost\n",
    "from ngboost.distns import T\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "\n",
    "def run_expanding_backtest_ngboost(X, y, start_test_idx, var_levels=(0.95, 0.99), es_mc_samples=3000):\n",
    "    model = NGBoost(\n",
    "        Dist=T,\n",
    "        Score=LogScore,\n",
    "        Base=DecisionTreeRegressor(max_depth=2, min_samples_leaf=50),\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=200,\n",
    "        natural_gradient=False,\n",
    "        verbose=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    rows = []\n",
    "\n",
    "    X_vals = X.values\n",
    "    y_vals = y.values\n",
    "    idx = X.index\n",
    "    n = len(X_vals)\n",
    "\n",
    "    for t in range(start_test_idx, n):\n",
    "        # Expanding window: train on everything up to t-1\n",
    "        window = 252   \n",
    "        start = max(0, t - window)\n",
    "        X_train = X.iloc[start:t].values\n",
    "        y_train = y.iloc[start:t].values \n",
    "\n",
    "        X_test = X_vals[t:t+1]\n",
    "        y_test = float(y_vals[t])\n",
    "\n",
    "        # Standardize X\n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_std = scaler_X.fit_transform(X_train)\n",
    "        X_test_std  = scaler_X.transform(X_test)\n",
    "\n",
    "        # Standardize y\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_std = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model.fit(X_train_std, y_train_std)\n",
    "        dist = model.pred_dist(X_test_std)\n",
    "\n",
    "        out = {\"Date\": idx[t], \"y_real\": y_test}\n",
    "\n",
    "        # Optional: store distribution mean/sigma in original units\n",
    "        mu_z = float(dist.mean()[0])\n",
    "        sig_z = float(np.sqrt(dist.var[0]))\n",
    "        out[\"mu_pred\"] = float(scaler_y.inverse_transform([[mu_z]])[0, 0])\n",
    "        out[\"sigma_pred\"] = float(scaler_y.scale_[0] * sig_z)\n",
    "\n",
    "        for a in var_levels:\n",
    "            q = 1 - a  # left-tail probability (e.g. 0.01 for 99% VaR)\n",
    "\n",
    "            var_z = float(np.asarray(dist.ppf(q)).reshape(-1)[0])\n",
    "\n",
    "            samples_z = dist.sample(es_mc_samples).reshape(-1)\n",
    "            es_z = float(samples_z[samples_z <= var_z].mean())\n",
    "\n",
    "            out[f\"VaR_{int(a*100)}\"] = float(scaler_y.inverse_transform([[var_z]])[0, 0])\n",
    "            out[f\"ES_{int(a*100)}\"]  = float(scaler_y.inverse_transform([[es_z]])[0, 0])\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b9141a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.3712 val_loss=0.0000 scale=1.0000 norm=1.4827\n",
      "[iter 100] loss=1.2585 val_loss=0.0000 scale=1.0000 norm=1.4880\n",
      "[iter 0] loss=1.3687 val_loss=0.0000 scale=1.0000 norm=1.4871\n",
      "[iter 100] loss=1.2538 val_loss=0.0000 scale=1.0000 norm=1.4915\n",
      "[iter 0] loss=1.3706 val_loss=0.0000 scale=1.0000 norm=1.4834\n",
      "[iter 100] loss=1.2578 val_loss=0.0000 scale=1.0000 norm=1.4891\n",
      "[iter 0] loss=1.3693 val_loss=0.0000 scale=1.0000 norm=1.4860\n",
      "[iter 100] loss=1.2549 val_loss=0.0000 scale=1.0000 norm=1.4915\n",
      "[iter 0] loss=1.3676 val_loss=0.0000 scale=1.0000 norm=1.4883\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.4927\n",
      "[iter 0] loss=1.3648 val_loss=0.0000 scale=1.0000 norm=1.4911\n",
      "[iter 100] loss=1.2479 val_loss=0.0000 scale=1.0000 norm=1.4940\n",
      "[iter 0] loss=1.3648 val_loss=0.0000 scale=1.0000 norm=1.4913\n",
      "[iter 100] loss=1.2475 val_loss=0.0000 scale=1.0000 norm=1.4936\n",
      "[iter 0] loss=1.3681 val_loss=0.0000 scale=1.0000 norm=1.4861\n",
      "[iter 100] loss=1.2518 val_loss=0.0000 scale=1.0000 norm=1.4878\n",
      "[iter 0] loss=1.3684 val_loss=0.0000 scale=1.0000 norm=1.4883\n",
      "[iter 100] loss=1.2550 val_loss=0.0000 scale=1.0000 norm=1.4898\n",
      "[iter 0] loss=1.3678 val_loss=0.0000 scale=1.0000 norm=1.4888\n",
      "[iter 100] loss=1.2532 val_loss=0.0000 scale=1.0000 norm=1.4895\n",
      "[iter 0] loss=1.3676 val_loss=0.0000 scale=1.0000 norm=1.4892\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.4912\n",
      "[iter 0] loss=1.3653 val_loss=0.0000 scale=1.0000 norm=1.4920\n",
      "[iter 100] loss=1.2525 val_loss=0.0000 scale=1.0000 norm=1.4947\n",
      "[iter 0] loss=1.3646 val_loss=0.0000 scale=1.0000 norm=1.4961\n",
      "[iter 100] loss=1.2505 val_loss=0.0000 scale=1.0000 norm=1.4998\n",
      "[iter 0] loss=1.3661 val_loss=0.0000 scale=1.0000 norm=1.4929\n",
      "[iter 100] loss=1.2503 val_loss=0.0000 scale=1.0000 norm=1.4967\n",
      "[iter 0] loss=1.3673 val_loss=0.0000 scale=1.0000 norm=1.4921\n",
      "[iter 100] loss=1.2487 val_loss=0.0000 scale=1.0000 norm=1.4957\n",
      "[iter 0] loss=1.3693 val_loss=0.0000 scale=1.0000 norm=1.4889\n",
      "[iter 100] loss=1.2484 val_loss=0.0000 scale=1.0000 norm=1.4933\n",
      "[iter 0] loss=1.3687 val_loss=0.0000 scale=1.0000 norm=1.4902\n",
      "[iter 100] loss=1.2512 val_loss=0.0000 scale=1.0000 norm=1.4943\n",
      "[iter 0] loss=1.3675 val_loss=0.0000 scale=1.0000 norm=1.4913\n",
      "[iter 100] loss=1.2495 val_loss=0.0000 scale=1.0000 norm=1.4963\n",
      "[iter 0] loss=1.3689 val_loss=0.0000 scale=1.0000 norm=1.4881\n",
      "[iter 100] loss=1.2494 val_loss=0.0000 scale=1.0000 norm=1.4933\n",
      "[iter 0] loss=1.3688 val_loss=0.0000 scale=1.0000 norm=1.4883\n",
      "[iter 100] loss=1.2476 val_loss=0.0000 scale=1.0000 norm=1.4940\n",
      "[iter 0] loss=1.3673 val_loss=0.0000 scale=1.0000 norm=1.4900\n",
      "[iter 100] loss=1.2456 val_loss=0.0000 scale=1.0000 norm=1.4986\n",
      "[iter 0] loss=1.3678 val_loss=0.0000 scale=1.0000 norm=1.4889\n",
      "[iter 100] loss=1.2463 val_loss=0.0000 scale=1.0000 norm=1.4976\n",
      "[iter 0] loss=1.3668 val_loss=0.0000 scale=1.0000 norm=1.4912\n",
      "[iter 100] loss=1.2438 val_loss=0.0000 scale=1.0000 norm=1.5003\n",
      "[iter 0] loss=1.3654 val_loss=0.0000 scale=1.0000 norm=1.4933\n",
      "[iter 100] loss=1.2438 val_loss=0.0000 scale=1.0000 norm=1.5013\n",
      "[iter 0] loss=1.3660 val_loss=0.0000 scale=1.0000 norm=1.4919\n",
      "[iter 100] loss=1.2460 val_loss=0.0000 scale=1.0000 norm=1.4993\n",
      "[iter 0] loss=1.3675 val_loss=0.0000 scale=1.0000 norm=1.4889\n",
      "[iter 100] loss=1.2476 val_loss=0.0000 scale=1.0000 norm=1.4968\n",
      "[iter 0] loss=1.3697 val_loss=0.0000 scale=1.0000 norm=1.4864\n",
      "[iter 100] loss=1.2507 val_loss=0.0000 scale=1.0000 norm=1.4927\n",
      "[iter 0] loss=1.3495 val_loss=0.0000 scale=1.0000 norm=1.5161\n",
      "[iter 100] loss=1.2248 val_loss=0.0000 scale=1.0000 norm=1.5234\n",
      "[iter 0] loss=1.3390 val_loss=0.0000 scale=1.0000 norm=1.5335\n",
      "[iter 100] loss=1.2106 val_loss=0.0000 scale=1.0000 norm=1.5407\n",
      "[iter 0] loss=1.3402 val_loss=0.0000 scale=1.0000 norm=1.5310\n",
      "[iter 100] loss=1.2135 val_loss=0.0000 scale=1.0000 norm=1.5368\n",
      "[iter 0] loss=1.3057 val_loss=0.0000 scale=1.0000 norm=1.5792\n",
      "[iter 100] loss=1.1722 val_loss=0.0000 scale=1.0000 norm=1.5868\n",
      "[iter 0] loss=1.3082 val_loss=0.0000 scale=1.0000 norm=1.5750\n",
      "[iter 100] loss=1.1772 val_loss=0.0000 scale=1.0000 norm=1.5807\n",
      "[iter 0] loss=1.2799 val_loss=0.0000 scale=1.0000 norm=1.6180\n",
      "[iter 100] loss=1.1403 val_loss=0.0000 scale=1.0000 norm=1.6268\n",
      "[iter 0] loss=1.2771 val_loss=0.0000 scale=1.0000 norm=1.6275\n",
      "[iter 100] loss=1.1309 val_loss=0.0000 scale=1.0000 norm=1.6358\n",
      "[iter 0] loss=1.2615 val_loss=0.0000 scale=1.0000 norm=1.6545\n",
      "[iter 100] loss=1.1102 val_loss=0.0000 scale=1.0000 norm=1.6659\n",
      "[iter 0] loss=1.2538 val_loss=0.0000 scale=1.0000 norm=1.6728\n",
      "[iter 100] loss=1.0974 val_loss=0.0000 scale=1.0000 norm=1.6852\n",
      "[iter 0] loss=1.2557 val_loss=0.0000 scale=1.0000 norm=1.6721\n",
      "[iter 100] loss=1.0976 val_loss=0.0000 scale=1.0000 norm=1.6848\n",
      "[iter 0] loss=1.1731 val_loss=0.0000 scale=1.0000 norm=1.7912\n",
      "[iter 100] loss=1.0053 val_loss=0.0000 scale=0.5000 norm=0.9016\n",
      "[iter 0] loss=1.1564 val_loss=0.0000 scale=1.0000 norm=1.8267\n",
      "[iter 100] loss=0.9819 val_loss=0.0000 scale=0.5000 norm=0.9169\n",
      "[iter 0] loss=1.1452 val_loss=0.0000 scale=1.0000 norm=1.8545\n",
      "[iter 100] loss=0.9669 val_loss=0.0000 scale=0.5000 norm=0.9270\n",
      "[iter 0] loss=1.0553 val_loss=0.0000 scale=0.5000 norm=1.0029\n",
      "[iter 100] loss=0.9019 val_loss=0.0000 scale=0.5000 norm=0.9865\n",
      "[iter 0] loss=1.0076 val_loss=0.0000 scale=0.5000 norm=1.0520\n",
      "[iter 100] loss=0.8568 val_loss=0.0000 scale=0.5000 norm=1.0289\n",
      "[iter 0] loss=0.9130 val_loss=0.0000 scale=0.5000 norm=1.1479\n",
      "[iter 100] loss=0.7513 val_loss=0.0000 scale=0.5000 norm=1.1221\n",
      "[iter 0] loss=0.9151 val_loss=0.0000 scale=0.5000 norm=1.1537\n",
      "[iter 100] loss=0.7471 val_loss=0.0000 scale=0.5000 norm=1.1243\n",
      "[iter 0] loss=0.9220 val_loss=0.0000 scale=0.5000 norm=1.1542\n",
      "[iter 100] loss=0.7490 val_loss=0.0000 scale=0.5000 norm=1.1234\n",
      "[iter 0] loss=0.9216 val_loss=0.0000 scale=0.5000 norm=1.1550\n",
      "[iter 100] loss=0.7487 val_loss=0.0000 scale=0.5000 norm=1.1215\n",
      "[iter 0] loss=0.9284 val_loss=0.0000 scale=0.5000 norm=1.1544\n",
      "[iter 100] loss=0.7482 val_loss=0.0000 scale=0.5000 norm=1.1217\n",
      "[iter 0] loss=0.9308 val_loss=0.0000 scale=0.5000 norm=1.1546\n",
      "[iter 100] loss=0.7424 val_loss=0.0000 scale=0.5000 norm=1.1242\n",
      "[iter 0] loss=0.9134 val_loss=0.0000 scale=0.5000 norm=1.1827\n",
      "[iter 100] loss=0.7147 val_loss=0.0000 scale=0.5000 norm=1.1519\n",
      "[iter 0] loss=0.9158 val_loss=0.0000 scale=0.5000 norm=1.1788\n",
      "[iter 100] loss=0.7169 val_loss=0.0000 scale=0.5000 norm=1.1486\n",
      "[iter 0] loss=0.9193 val_loss=0.0000 scale=0.5000 norm=1.1827\n",
      "[iter 100] loss=0.7107 val_loss=0.0000 scale=0.5000 norm=1.1516\n",
      "[iter 0] loss=0.9312 val_loss=0.0000 scale=0.5000 norm=1.1713\n",
      "[iter 100] loss=0.7175 val_loss=0.0000 scale=0.5000 norm=1.1419\n",
      "[iter 0] loss=0.9406 val_loss=0.0000 scale=0.5000 norm=1.1617\n",
      "[iter 100] loss=0.7230 val_loss=0.0000 scale=0.5000 norm=1.1354\n",
      "[iter 0] loss=0.9444 val_loss=0.0000 scale=0.5000 norm=1.1564\n",
      "[iter 100] loss=0.7238 val_loss=0.0000 scale=0.5000 norm=1.1313\n",
      "[iter 0] loss=0.9556 val_loss=0.0000 scale=0.5000 norm=1.1488\n",
      "[iter 100] loss=0.7296 val_loss=0.0000 scale=0.5000 norm=1.1231\n",
      "[iter 0] loss=0.9640 val_loss=0.0000 scale=0.5000 norm=1.1382\n",
      "[iter 100] loss=0.7346 val_loss=0.0000 scale=0.5000 norm=1.1170\n",
      "[iter 0] loss=0.9707 val_loss=0.0000 scale=0.5000 norm=1.1291\n",
      "[iter 100] loss=0.7391 val_loss=0.0000 scale=0.5000 norm=1.1064\n",
      "[iter 0] loss=0.9729 val_loss=0.0000 scale=0.5000 norm=1.1325\n",
      "[iter 100] loss=0.7348 val_loss=0.0000 scale=0.5000 norm=1.1109\n",
      "[iter 0] loss=0.9733 val_loss=0.0000 scale=0.5000 norm=1.1324\n",
      "[iter 100] loss=0.7378 val_loss=0.0000 scale=0.5000 norm=1.1114\n",
      "[iter 0] loss=0.9816 val_loss=0.0000 scale=0.5000 norm=1.1245\n",
      "[iter 100] loss=0.7398 val_loss=0.0000 scale=0.5000 norm=1.1009\n",
      "[iter 0] loss=0.9862 val_loss=0.0000 scale=0.5000 norm=1.1180\n",
      "[iter 100] loss=0.7458 val_loss=0.0000 scale=0.5000 norm=1.0970\n",
      "[iter 0] loss=0.9900 val_loss=0.0000 scale=0.5000 norm=1.1125\n",
      "[iter 100] loss=0.7477 val_loss=0.0000 scale=0.5000 norm=1.0924\n",
      "[iter 0] loss=0.9986 val_loss=0.0000 scale=1.0000 norm=2.2061\n",
      "[iter 100] loss=0.7454 val_loss=0.0000 scale=0.5000 norm=1.0840\n",
      "[iter 0] loss=1.0071 val_loss=0.0000 scale=1.0000 norm=2.1849\n",
      "[iter 100] loss=0.7493 val_loss=0.0000 scale=0.5000 norm=1.0758\n",
      "[iter 0] loss=1.0079 val_loss=0.0000 scale=1.0000 norm=2.1843\n",
      "[iter 100] loss=0.7498 val_loss=0.0000 scale=0.5000 norm=1.0779\n",
      "[iter 0] loss=1.0161 val_loss=0.0000 scale=0.5000 norm=1.0826\n",
      "[iter 100] loss=0.7541 val_loss=0.0000 scale=0.5000 norm=1.0716\n",
      "[iter 0] loss=1.0231 val_loss=0.0000 scale=1.0000 norm=2.1476\n",
      "[iter 100] loss=0.7580 val_loss=0.0000 scale=0.5000 norm=1.0651\n",
      "[iter 0] loss=1.0331 val_loss=0.0000 scale=1.0000 norm=2.1273\n",
      "[iter 100] loss=0.7624 val_loss=0.0000 scale=0.5000 norm=1.0589\n",
      "[iter 0] loss=1.0385 val_loss=0.0000 scale=0.5000 norm=1.0566\n",
      "[iter 100] loss=0.7594 val_loss=0.0000 scale=0.5000 norm=1.0588\n",
      "[iter 0] loss=1.0382 val_loss=0.0000 scale=1.0000 norm=2.1136\n",
      "[iter 100] loss=0.7531 val_loss=0.0000 scale=0.5000 norm=1.0637\n",
      "[iter 0] loss=1.0420 val_loss=0.0000 scale=0.5000 norm=1.0521\n",
      "[iter 100] loss=0.7551 val_loss=0.0000 scale=0.5000 norm=1.0606\n",
      "[iter 0] loss=1.0457 val_loss=0.0000 scale=0.5000 norm=1.0472\n",
      "[iter 100] loss=0.7580 val_loss=0.0000 scale=0.5000 norm=1.0557\n",
      "[iter 0] loss=1.0470 val_loss=0.0000 scale=0.5000 norm=1.0460\n",
      "[iter 100] loss=0.7609 val_loss=0.0000 scale=0.5000 norm=1.0549\n",
      "[iter 0] loss=1.0548 val_loss=0.0000 scale=0.5000 norm=1.0373\n",
      "[iter 100] loss=0.7659 val_loss=0.0000 scale=0.5000 norm=1.0462\n",
      "[iter 0] loss=1.0555 val_loss=0.0000 scale=1.0000 norm=2.0728\n",
      "[iter 100] loss=0.7665 val_loss=0.0000 scale=0.5000 norm=1.0410\n",
      "[iter 0] loss=1.0641 val_loss=0.0000 scale=1.0000 norm=2.0543\n",
      "[iter 100] loss=0.7794 val_loss=0.0000 scale=0.5000 norm=1.0303\n",
      "[iter 0] loss=1.0625 val_loss=0.0000 scale=1.0000 norm=2.0583\n",
      "[iter 100] loss=0.7703 val_loss=0.0000 scale=0.5000 norm=1.0376\n",
      "[iter 0] loss=1.0632 val_loss=0.0000 scale=0.5000 norm=1.0282\n",
      "[iter 100] loss=0.7763 val_loss=0.0000 scale=0.5000 norm=1.0302\n",
      "[iter 0] loss=1.0593 val_loss=0.0000 scale=1.0000 norm=2.0667\n",
      "[iter 100] loss=0.7762 val_loss=0.0000 scale=0.5000 norm=1.0379\n",
      "[iter 0] loss=1.0616 val_loss=0.0000 scale=1.0000 norm=2.0613\n",
      "[iter 100] loss=0.7823 val_loss=0.0000 scale=0.5000 norm=1.0354\n",
      "[iter 0] loss=1.0657 val_loss=0.0000 scale=1.0000 norm=2.0510\n",
      "[iter 100] loss=0.7780 val_loss=0.0000 scale=0.5000 norm=1.0334\n",
      "[iter 0] loss=1.0655 val_loss=0.0000 scale=1.0000 norm=2.0507\n",
      "[iter 100] loss=0.7761 val_loss=0.0000 scale=0.5000 norm=1.0310\n",
      "[iter 0] loss=1.0646 val_loss=0.0000 scale=1.0000 norm=2.0521\n",
      "[iter 100] loss=0.7742 val_loss=0.0000 scale=0.5000 norm=1.0365\n",
      "[iter 0] loss=1.0693 val_loss=0.0000 scale=1.0000 norm=2.0393\n",
      "[iter 100] loss=0.7762 val_loss=0.0000 scale=0.5000 norm=1.0299\n",
      "[iter 0] loss=1.0712 val_loss=0.0000 scale=1.0000 norm=2.0343\n",
      "[iter 100] loss=0.7788 val_loss=0.0000 scale=0.5000 norm=1.0283\n",
      "[iter 0] loss=1.0699 val_loss=0.0000 scale=1.0000 norm=2.0373\n",
      "[iter 100] loss=0.7817 val_loss=0.0000 scale=0.5000 norm=1.0282\n",
      "[iter 0] loss=1.0767 val_loss=0.0000 scale=1.0000 norm=2.0234\n",
      "[iter 100] loss=0.7831 val_loss=0.0000 scale=0.5000 norm=1.0253\n",
      "[iter 0] loss=1.0782 val_loss=0.0000 scale=1.0000 norm=2.0194\n",
      "[iter 100] loss=0.7832 val_loss=0.0000 scale=0.5000 norm=1.0228\n",
      "[iter 0] loss=1.0812 val_loss=0.0000 scale=1.0000 norm=2.0113\n",
      "[iter 100] loss=0.7876 val_loss=0.0000 scale=0.5000 norm=1.0155\n",
      "[iter 0] loss=1.0827 val_loss=0.0000 scale=1.0000 norm=2.0076\n",
      "[iter 100] loss=0.7896 val_loss=0.0000 scale=0.5000 norm=1.0138\n",
      "[iter 0] loss=1.0791 val_loss=0.0000 scale=1.0000 norm=2.0163\n",
      "[iter 100] loss=0.7895 val_loss=0.0000 scale=0.5000 norm=1.0173\n",
      "[iter 0] loss=1.0817 val_loss=0.0000 scale=1.0000 norm=2.0106\n",
      "[iter 100] loss=0.7909 val_loss=0.0000 scale=0.5000 norm=1.0169\n",
      "[iter 0] loss=1.0830 val_loss=0.0000 scale=1.0000 norm=2.0076\n",
      "[iter 100] loss=0.7993 val_loss=0.0000 scale=0.5000 norm=1.0066\n",
      "[iter 0] loss=1.0816 val_loss=0.0000 scale=0.5000 norm=1.0054\n",
      "[iter 100] loss=0.7885 val_loss=0.0000 scale=0.5000 norm=1.0102\n",
      "[iter 0] loss=1.0819 val_loss=0.0000 scale=1.0000 norm=2.0109\n",
      "[iter 100] loss=0.7960 val_loss=0.0000 scale=0.5000 norm=1.0081\n",
      "[iter 0] loss=1.0777 val_loss=0.0000 scale=0.5000 norm=1.0107\n",
      "[iter 100] loss=0.7929 val_loss=0.0000 scale=0.5000 norm=1.0122\n",
      "[iter 0] loss=1.0782 val_loss=0.0000 scale=0.5000 norm=1.0102\n",
      "[iter 100] loss=0.7867 val_loss=0.0000 scale=0.5000 norm=1.0125\n",
      "[iter 0] loss=1.0755 val_loss=0.0000 scale=0.5000 norm=1.0134\n",
      "[iter 100] loss=0.7993 val_loss=0.0000 scale=0.5000 norm=1.0067\n",
      "[iter 0] loss=1.0751 val_loss=0.0000 scale=0.5000 norm=1.0137\n",
      "[iter 100] loss=0.7987 val_loss=0.0000 scale=0.5000 norm=1.0076\n",
      "[iter 0] loss=1.0816 val_loss=0.0000 scale=0.5000 norm=1.0063\n",
      "[iter 100] loss=0.8059 val_loss=0.0000 scale=0.5000 norm=1.0038\n",
      "[iter 0] loss=1.0822 val_loss=0.0000 scale=0.5000 norm=1.0055\n",
      "[iter 100] loss=0.8060 val_loss=0.0000 scale=0.5000 norm=1.0020\n",
      "[iter 0] loss=1.0840 val_loss=0.0000 scale=0.5000 norm=1.0031\n",
      "[iter 100] loss=0.8199 val_loss=0.0000 scale=1.0000 norm=1.9862\n",
      "[iter 0] loss=1.0852 val_loss=0.0000 scale=0.5000 norm=1.0021\n",
      "[iter 100] loss=0.8116 val_loss=0.0000 scale=0.5000 norm=0.9981\n",
      "[iter 0] loss=1.0903 val_loss=0.0000 scale=0.5000 norm=1.0007\n",
      "[iter 100] loss=0.8094 val_loss=0.0000 scale=0.5000 norm=0.9985\n",
      "[iter 0] loss=1.0928 val_loss=0.0000 scale=0.5000 norm=0.9977\n",
      "[iter 100] loss=0.8139 val_loss=0.0000 scale=0.5000 norm=0.9947\n",
      "[iter 0] loss=1.0936 val_loss=0.0000 scale=0.5000 norm=0.9969\n",
      "[iter 100] loss=0.8170 val_loss=0.0000 scale=0.5000 norm=0.9927\n",
      "[iter 0] loss=1.0983 val_loss=0.0000 scale=0.5000 norm=0.9915\n",
      "[iter 100] loss=0.8330 val_loss=0.0000 scale=0.5000 norm=0.9826\n",
      "[iter 0] loss=1.0975 val_loss=0.0000 scale=0.5000 norm=0.9923\n",
      "[iter 100] loss=0.8277 val_loss=0.0000 scale=0.5000 norm=0.9852\n",
      "[iter 0] loss=1.0975 val_loss=0.0000 scale=0.5000 norm=0.9923\n",
      "[iter 100] loss=0.8381 val_loss=0.0000 scale=0.5000 norm=0.9803\n",
      "[iter 0] loss=1.0973 val_loss=0.0000 scale=0.5000 norm=0.9923\n",
      "[iter 100] loss=0.8341 val_loss=0.0000 scale=0.5000 norm=0.9825\n",
      "[iter 0] loss=1.0977 val_loss=0.0000 scale=0.5000 norm=0.9922\n",
      "[iter 100] loss=0.8365 val_loss=0.0000 scale=0.5000 norm=0.9826\n",
      "[iter 0] loss=1.0976 val_loss=0.0000 scale=0.5000 norm=0.9924\n",
      "[iter 100] loss=0.8404 val_loss=0.0000 scale=0.5000 norm=0.9807\n",
      "[iter 0] loss=1.1028 val_loss=0.0000 scale=0.5000 norm=0.9871\n",
      "[iter 100] loss=0.8455 val_loss=0.0000 scale=0.5000 norm=0.9754\n",
      "[iter 0] loss=1.1044 val_loss=0.0000 scale=0.5000 norm=0.9854\n",
      "[iter 100] loss=0.8460 val_loss=0.0000 scale=0.5000 norm=0.9699\n",
      "[iter 0] loss=1.1116 val_loss=0.0000 scale=0.5000 norm=0.9777\n",
      "[iter 100] loss=0.8449 val_loss=0.0000 scale=0.5000 norm=0.9652\n",
      "[iter 0] loss=1.1143 val_loss=0.0000 scale=0.5000 norm=0.9745\n",
      "[iter 100] loss=0.8476 val_loss=0.0000 scale=0.5000 norm=0.9584\n",
      "[iter 0] loss=1.1167 val_loss=0.0000 scale=1.0000 norm=1.9429\n",
      "[iter 100] loss=0.8527 val_loss=0.0000 scale=0.5000 norm=0.9539\n",
      "[iter 0] loss=1.1169 val_loss=0.0000 scale=0.5000 norm=0.9714\n",
      "[iter 100] loss=0.8517 val_loss=0.0000 scale=0.5000 norm=0.9541\n",
      "[iter 0] loss=1.1163 val_loss=0.0000 scale=0.5000 norm=0.9718\n",
      "[iter 100] loss=0.8529 val_loss=0.0000 scale=0.5000 norm=0.9507\n",
      "[iter 0] loss=1.1193 val_loss=0.0000 scale=0.5000 norm=0.9685\n",
      "[iter 100] loss=0.8599 val_loss=0.0000 scale=0.5000 norm=0.9536\n",
      "[iter 0] loss=1.1213 val_loss=0.0000 scale=1.0000 norm=1.9317\n",
      "[iter 100] loss=0.8610 val_loss=0.0000 scale=1.0000 norm=1.9017\n",
      "[iter 0] loss=1.1221 val_loss=0.0000 scale=1.0000 norm=1.9310\n",
      "[iter 100] loss=0.8576 val_loss=0.0000 scale=0.5000 norm=0.9527\n",
      "[iter 0] loss=1.1231 val_loss=0.0000 scale=1.0000 norm=1.9287\n",
      "[iter 100] loss=0.8640 val_loss=0.0000 scale=1.0000 norm=1.9016\n",
      "[iter 0] loss=1.1245 val_loss=0.0000 scale=1.0000 norm=1.9262\n",
      "[iter 100] loss=0.8541 val_loss=0.0000 scale=0.5000 norm=0.9540\n",
      "[iter 0] loss=1.1267 val_loss=0.0000 scale=1.0000 norm=1.9210\n",
      "[iter 100] loss=0.8622 val_loss=0.0000 scale=0.5000 norm=0.9506\n",
      "[iter 0] loss=1.1290 val_loss=0.0000 scale=1.0000 norm=1.9164\n",
      "[iter 100] loss=0.8641 val_loss=0.0000 scale=0.5000 norm=0.9479\n",
      "[iter 0] loss=1.1295 val_loss=0.0000 scale=1.0000 norm=1.9154\n",
      "[iter 100] loss=0.8713 val_loss=0.0000 scale=0.5000 norm=0.9477\n",
      "[iter 0] loss=1.1287 val_loss=0.0000 scale=1.0000 norm=1.9175\n",
      "[iter 100] loss=0.8717 val_loss=0.0000 scale=0.5000 norm=0.9458\n",
      "[iter 0] loss=1.1286 val_loss=0.0000 scale=1.0000 norm=1.9174\n",
      "[iter 100] loss=0.8732 val_loss=0.0000 scale=0.5000 norm=0.9470\n",
      "[iter 0] loss=1.1282 val_loss=0.0000 scale=1.0000 norm=1.9188\n",
      "[iter 100] loss=0.8716 val_loss=0.0000 scale=0.5000 norm=0.9497\n",
      "[iter 0] loss=1.1281 val_loss=0.0000 scale=1.0000 norm=1.9187\n",
      "[iter 100] loss=0.8783 val_loss=0.0000 scale=1.0000 norm=1.8969\n",
      "[iter 0] loss=1.1279 val_loss=0.0000 scale=1.0000 norm=1.9189\n",
      "[iter 100] loss=0.8751 val_loss=0.0000 scale=1.0000 norm=1.8914\n",
      "[iter 0] loss=1.1311 val_loss=0.0000 scale=1.0000 norm=1.9114\n",
      "[iter 100] loss=0.8746 val_loss=0.0000 scale=0.5000 norm=0.9458\n",
      "[iter 0] loss=1.1314 val_loss=0.0000 scale=1.0000 norm=1.9108\n",
      "[iter 100] loss=0.8719 val_loss=0.0000 scale=0.5000 norm=0.9492\n",
      "[iter 0] loss=1.1314 val_loss=0.0000 scale=1.0000 norm=1.9108\n",
      "[iter 100] loss=0.8738 val_loss=0.0000 scale=0.5000 norm=0.9483\n",
      "[iter 0] loss=1.1325 val_loss=0.0000 scale=1.0000 norm=1.9085\n",
      "[iter 100] loss=0.8752 val_loss=0.0000 scale=0.5000 norm=0.9476\n",
      "[iter 0] loss=1.1341 val_loss=0.0000 scale=1.0000 norm=1.9052\n",
      "[iter 100] loss=0.8777 val_loss=0.0000 scale=1.0000 norm=1.8844\n",
      "[iter 0] loss=1.1319 val_loss=0.0000 scale=1.0000 norm=1.9103\n",
      "[iter 100] loss=0.8868 val_loss=0.0000 scale=1.0000 norm=1.8717\n",
      "[iter 0] loss=1.1303 val_loss=0.0000 scale=0.5000 norm=0.9572\n",
      "[iter 100] loss=0.8909 val_loss=0.0000 scale=0.5000 norm=0.9347\n",
      "[iter 0] loss=1.1292 val_loss=0.0000 scale=0.5000 norm=0.9588\n",
      "[iter 100] loss=0.8814 val_loss=0.0000 scale=0.5000 norm=0.9365\n",
      "[iter 0] loss=1.1213 val_loss=0.0000 scale=0.5000 norm=0.9663\n",
      "[iter 100] loss=0.8855 val_loss=0.0000 scale=0.5000 norm=0.9396\n",
      "[iter 0] loss=1.1195 val_loss=0.0000 scale=0.5000 norm=0.9683\n",
      "[iter 100] loss=0.8899 val_loss=0.0000 scale=1.0000 norm=1.8782\n",
      "[iter 0] loss=1.1199 val_loss=0.0000 scale=0.5000 norm=0.9682\n",
      "[iter 100] loss=0.8909 val_loss=0.0000 scale=1.0000 norm=1.8783\n",
      "[iter 0] loss=1.1159 val_loss=0.0000 scale=0.5000 norm=0.9728\n",
      "[iter 100] loss=0.8851 val_loss=0.0000 scale=0.5000 norm=0.9424\n",
      "[iter 0] loss=1.1143 val_loss=0.0000 scale=0.5000 norm=0.9743\n",
      "[iter 100] loss=0.8818 val_loss=0.0000 scale=0.5000 norm=0.9446\n",
      "[iter 0] loss=1.1127 val_loss=0.0000 scale=0.5000 norm=0.9765\n",
      "[iter 100] loss=0.8752 val_loss=0.0000 scale=0.5000 norm=0.9461\n",
      "[iter 0] loss=1.1123 val_loss=0.0000 scale=0.5000 norm=0.9768\n",
      "[iter 100] loss=0.8765 val_loss=0.0000 scale=0.5000 norm=0.9494\n",
      "[iter 0] loss=1.1045 val_loss=0.0000 scale=0.5000 norm=0.9846\n",
      "[iter 100] loss=0.8696 val_loss=0.0000 scale=0.5000 norm=0.9567\n",
      "[iter 0] loss=1.1047 val_loss=0.0000 scale=0.5000 norm=0.9844\n",
      "[iter 100] loss=0.8668 val_loss=0.0000 scale=0.5000 norm=0.9574\n",
      "[iter 0] loss=1.1019 val_loss=0.0000 scale=0.5000 norm=0.9876\n",
      "[iter 100] loss=0.8629 val_loss=0.0000 scale=0.5000 norm=0.9621\n",
      "[iter 0] loss=1.0998 val_loss=0.0000 scale=0.5000 norm=0.9896\n",
      "[iter 100] loss=0.8573 val_loss=0.0000 scale=0.5000 norm=0.9628\n",
      "[iter 0] loss=1.0986 val_loss=0.0000 scale=0.5000 norm=0.9911\n",
      "[iter 100] loss=0.8533 val_loss=0.0000 scale=0.5000 norm=0.9681\n",
      "[iter 0] loss=1.0978 val_loss=0.0000 scale=0.5000 norm=0.9918\n",
      "[iter 100] loss=0.8503 val_loss=0.0000 scale=0.5000 norm=0.9681\n",
      "[iter 0] loss=1.0976 val_loss=0.0000 scale=0.5000 norm=0.9919\n",
      "[iter 100] loss=0.8427 val_loss=0.0000 scale=0.5000 norm=0.9698\n",
      "[iter 0] loss=1.0911 val_loss=0.0000 scale=0.5000 norm=0.9989\n",
      "[iter 100] loss=0.8379 val_loss=0.0000 scale=0.5000 norm=0.9741\n",
      "[iter 0] loss=1.0894 val_loss=0.0000 scale=0.5000 norm=1.0006\n",
      "[iter 100] loss=0.8353 val_loss=0.0000 scale=0.5000 norm=0.9757\n",
      "[iter 0] loss=1.0901 val_loss=0.0000 scale=0.5000 norm=0.9997\n",
      "[iter 100] loss=0.8386 val_loss=0.0000 scale=0.5000 norm=0.9744\n",
      "[iter 0] loss=1.0896 val_loss=0.0000 scale=0.5000 norm=0.9999\n",
      "[iter 100] loss=0.8366 val_loss=0.0000 scale=0.5000 norm=0.9750\n",
      "[iter 0] loss=1.0878 val_loss=0.0000 scale=0.5000 norm=1.0022\n",
      "[iter 100] loss=0.8276 val_loss=0.0000 scale=0.5000 norm=0.9801\n",
      "[iter 0] loss=1.0883 val_loss=0.0000 scale=0.5000 norm=1.0019\n",
      "[iter 100] loss=0.8288 val_loss=0.0000 scale=0.5000 norm=0.9749\n",
      "[iter 0] loss=1.0870 val_loss=0.0000 scale=0.5000 norm=1.0037\n",
      "[iter 100] loss=0.8309 val_loss=0.0000 scale=0.5000 norm=0.9748\n",
      "[iter 0] loss=1.0887 val_loss=0.0000 scale=0.5000 norm=1.0015\n",
      "[iter 100] loss=0.8364 val_loss=0.0000 scale=0.5000 norm=0.9760\n",
      "[iter 0] loss=1.0953 val_loss=0.0000 scale=0.5000 norm=0.9960\n",
      "[iter 100] loss=0.8473 val_loss=0.0000 scale=0.5000 norm=0.9686\n",
      "[iter 0] loss=1.0976 val_loss=0.0000 scale=0.5000 norm=0.9935\n",
      "[iter 100] loss=0.8452 val_loss=0.0000 scale=0.5000 norm=0.9709\n",
      "[iter 0] loss=1.1057 val_loss=0.0000 scale=0.5000 norm=0.9852\n",
      "[iter 100] loss=0.8452 val_loss=0.0000 scale=0.5000 norm=0.9652\n",
      "[iter 0] loss=1.1103 val_loss=0.0000 scale=0.5000 norm=0.9799\n",
      "[iter 100] loss=0.8518 val_loss=0.0000 scale=0.5000 norm=0.9610\n",
      "[iter 0] loss=1.1153 val_loss=0.0000 scale=0.5000 norm=0.9740\n",
      "[iter 100] loss=0.8530 val_loss=0.0000 scale=0.5000 norm=0.9572\n",
      "[iter 0] loss=1.1153 val_loss=0.0000 scale=0.5000 norm=0.9738\n",
      "[iter 100] loss=0.8594 val_loss=0.0000 scale=0.5000 norm=0.9570\n",
      "[iter 0] loss=1.1173 val_loss=0.0000 scale=0.5000 norm=0.9717\n",
      "[iter 100] loss=0.8572 val_loss=0.0000 scale=0.5000 norm=0.9581\n",
      "[iter 0] loss=1.1168 val_loss=0.0000 scale=0.5000 norm=0.9723\n",
      "[iter 100] loss=0.8589 val_loss=0.0000 scale=0.5000 norm=0.9604\n",
      "[iter 0] loss=1.1179 val_loss=0.0000 scale=0.5000 norm=0.9713\n",
      "[iter 100] loss=0.8664 val_loss=0.0000 scale=0.5000 norm=0.9570\n",
      "[iter 0] loss=1.1200 val_loss=0.0000 scale=0.5000 norm=0.9690\n",
      "[iter 100] loss=0.8728 val_loss=0.0000 scale=1.0000 norm=1.9077\n",
      "[iter 0] loss=1.1230 val_loss=0.0000 scale=0.5000 norm=0.9656\n",
      "[iter 100] loss=0.8727 val_loss=0.0000 scale=0.5000 norm=0.9518\n",
      "[iter 0] loss=1.1252 val_loss=0.0000 scale=0.5000 norm=0.9628\n",
      "[iter 100] loss=0.8667 val_loss=0.0000 scale=0.5000 norm=0.9526\n",
      "[iter 0] loss=1.1265 val_loss=0.0000 scale=0.5000 norm=0.9617\n",
      "[iter 100] loss=0.8780 val_loss=0.0000 scale=1.0000 norm=1.8960\n",
      "[iter 0] loss=1.1313 val_loss=0.0000 scale=0.5000 norm=0.9568\n",
      "[iter 100] loss=0.8808 val_loss=0.0000 scale=0.5000 norm=0.9437\n",
      "[iter 0] loss=1.1309 val_loss=0.0000 scale=0.5000 norm=0.9570\n",
      "[iter 100] loss=0.8916 val_loss=0.0000 scale=1.0000 norm=1.8800\n",
      "[iter 0] loss=1.1335 val_loss=0.0000 scale=0.5000 norm=0.9539\n",
      "[iter 100] loss=0.8864 val_loss=0.0000 scale=0.5000 norm=0.9423\n",
      "[iter 0] loss=1.1353 val_loss=0.0000 scale=0.5000 norm=0.9518\n",
      "[iter 100] loss=0.8825 val_loss=0.0000 scale=0.5000 norm=0.9409\n",
      "[iter 0] loss=1.1362 val_loss=0.0000 scale=0.5000 norm=0.9509\n",
      "[iter 100] loss=0.8876 val_loss=0.0000 scale=1.0000 norm=1.8743\n",
      "[iter 0] loss=1.1335 val_loss=0.0000 scale=0.5000 norm=0.9542\n",
      "[iter 100] loss=0.8736 val_loss=0.0000 scale=0.5000 norm=0.9410\n",
      "[iter 0] loss=1.1283 val_loss=0.0000 scale=1.0000 norm=1.9201\n",
      "[iter 100] loss=0.8731 val_loss=0.0000 scale=1.0000 norm=1.8832\n",
      "[iter 0] loss=1.1302 val_loss=0.0000 scale=0.5000 norm=0.9577\n",
      "[iter 100] loss=0.8704 val_loss=0.0000 scale=0.5000 norm=0.9455\n",
      "[iter 0] loss=1.1314 val_loss=0.0000 scale=0.5000 norm=0.9562\n",
      "[iter 100] loss=0.8803 val_loss=0.0000 scale=1.0000 norm=1.8687\n",
      "[iter 0] loss=1.1345 val_loss=0.0000 scale=1.0000 norm=1.9049\n",
      "[iter 100] loss=0.8786 val_loss=0.0000 scale=0.5000 norm=0.9348\n",
      "[iter 0] loss=1.1333 val_loss=0.0000 scale=0.5000 norm=0.9539\n",
      "[iter 100] loss=0.8677 val_loss=0.0000 scale=0.5000 norm=0.9382\n",
      "[iter 0] loss=1.1330 val_loss=0.0000 scale=0.5000 norm=0.9541\n",
      "[iter 100] loss=0.8708 val_loss=0.0000 scale=0.5000 norm=0.9381\n",
      "[iter 0] loss=1.1335 val_loss=0.0000 scale=0.5000 norm=0.9537\n",
      "[iter 100] loss=0.8708 val_loss=0.0000 scale=0.5000 norm=0.9379\n",
      "[iter 0] loss=1.1352 val_loss=0.0000 scale=0.5000 norm=0.9517\n",
      "[iter 100] loss=0.8757 val_loss=0.0000 scale=0.5000 norm=0.9326\n",
      "[iter 0] loss=1.1363 val_loss=0.0000 scale=0.5000 norm=0.9504\n",
      "[iter 100] loss=0.8756 val_loss=0.0000 scale=0.5000 norm=0.9317\n",
      "[iter 0] loss=1.1368 val_loss=0.0000 scale=0.5000 norm=0.9499\n",
      "[iter 100] loss=0.8710 val_loss=0.0000 scale=0.5000 norm=0.9349\n",
      "[iter 0] loss=1.1367 val_loss=0.0000 scale=0.5000 norm=0.9499\n",
      "[iter 100] loss=0.8688 val_loss=0.0000 scale=0.5000 norm=0.9383\n",
      "[iter 0] loss=1.1368 val_loss=0.0000 scale=0.5000 norm=0.9499\n",
      "[iter 100] loss=0.8713 val_loss=0.0000 scale=0.5000 norm=0.9420\n",
      "[iter 0] loss=1.1407 val_loss=0.0000 scale=1.0000 norm=1.8908\n",
      "[iter 100] loss=0.8748 val_loss=0.0000 scale=0.5000 norm=0.9425\n",
      "[iter 0] loss=1.1404 val_loss=0.0000 scale=1.0000 norm=1.8912\n",
      "[iter 100] loss=0.8751 val_loss=0.0000 scale=1.0000 norm=1.8912\n",
      "[iter 0] loss=1.1401 val_loss=0.0000 scale=1.0000 norm=1.8915\n",
      "[iter 100] loss=0.8755 val_loss=0.0000 scale=0.5000 norm=0.9467\n",
      "[iter 0] loss=1.1403 val_loss=0.0000 scale=1.0000 norm=1.8915\n",
      "[iter 100] loss=0.8773 val_loss=0.0000 scale=0.5000 norm=0.9459\n",
      "[iter 0] loss=1.1403 val_loss=0.0000 scale=0.5000 norm=0.9458\n",
      "[iter 100] loss=0.8806 val_loss=0.0000 scale=0.5000 norm=0.9437\n",
      "[iter 0] loss=1.1455 val_loss=0.0000 scale=0.5000 norm=0.9403\n",
      "[iter 100] loss=0.8937 val_loss=0.0000 scale=0.5000 norm=0.9354\n",
      "[iter 0] loss=1.1458 val_loss=0.0000 scale=0.5000 norm=0.9399\n",
      "[iter 100] loss=0.8878 val_loss=0.0000 scale=0.5000 norm=0.9407\n",
      "[iter 0] loss=1.1535 val_loss=0.0000 scale=0.5000 norm=0.9335\n",
      "[iter 100] loss=0.9006 val_loss=0.0000 scale=1.0000 norm=1.8677\n",
      "[iter 0] loss=1.1550 val_loss=0.0000 scale=0.5000 norm=0.9319\n",
      "[iter 100] loss=0.8996 val_loss=0.0000 scale=0.5000 norm=0.9295\n",
      "[iter 0] loss=1.1576 val_loss=0.0000 scale=0.5000 norm=0.9290\n",
      "[iter 100] loss=0.9014 val_loss=0.0000 scale=0.5000 norm=0.9273\n",
      "[iter 0] loss=1.1583 val_loss=0.0000 scale=0.5000 norm=0.9283\n",
      "[iter 100] loss=0.9033 val_loss=0.0000 scale=0.5000 norm=0.9262\n",
      "[iter 0] loss=1.1614 val_loss=0.0000 scale=1.0000 norm=1.8499\n",
      "[iter 100] loss=0.9063 val_loss=0.0000 scale=0.5000 norm=0.9230\n",
      "[iter 0] loss=1.1654 val_loss=0.0000 scale=0.5000 norm=0.9207\n",
      "[iter 100] loss=0.9105 val_loss=0.0000 scale=1.0000 norm=1.8395\n",
      "[iter 0] loss=1.1688 val_loss=0.0000 scale=1.0000 norm=1.8343\n",
      "[iter 100] loss=0.9201 val_loss=0.0000 scale=0.5000 norm=0.9155\n",
      "[iter 0] loss=1.1689 val_loss=0.0000 scale=1.0000 norm=1.8342\n",
      "[iter 100] loss=0.9306 val_loss=0.0000 scale=0.5000 norm=0.9124\n",
      "[iter 0] loss=1.1703 val_loss=0.0000 scale=1.0000 norm=1.8317\n",
      "[iter 100] loss=0.9325 val_loss=0.0000 scale=0.5000 norm=0.9119\n",
      "[iter 0] loss=1.1702 val_loss=0.0000 scale=1.0000 norm=1.8318\n",
      "[iter 100] loss=0.9365 val_loss=0.0000 scale=1.0000 norm=1.8168\n",
      "[iter 0] loss=1.1707 val_loss=0.0000 scale=1.0000 norm=1.8315\n",
      "[iter 100] loss=0.9381 val_loss=0.0000 scale=0.5000 norm=0.9074\n",
      "[iter 0] loss=1.1729 val_loss=0.0000 scale=1.0000 norm=1.8268\n",
      "[iter 100] loss=0.9403 val_loss=0.0000 scale=0.5000 norm=0.9087\n",
      "[iter 0] loss=1.1746 val_loss=0.0000 scale=1.0000 norm=1.8235\n",
      "[iter 100] loss=0.9426 val_loss=0.0000 scale=0.5000 norm=0.9065\n",
      "[iter 0] loss=1.1755 val_loss=0.0000 scale=1.0000 norm=1.8218\n",
      "[iter 100] loss=0.9499 val_loss=0.0000 scale=0.5000 norm=0.9035\n",
      "[iter 0] loss=1.1763 val_loss=0.0000 scale=1.0000 norm=1.8202\n",
      "[iter 100] loss=0.9511 val_loss=0.0000 scale=0.5000 norm=0.9040\n",
      "[iter 0] loss=1.1788 val_loss=0.0000 scale=1.0000 norm=1.8148\n",
      "[iter 100] loss=0.9548 val_loss=0.0000 scale=1.0000 norm=1.8008\n",
      "[iter 0] loss=1.1782 val_loss=0.0000 scale=1.0000 norm=1.8159\n",
      "[iter 100] loss=0.9512 val_loss=0.0000 scale=0.5000 norm=0.9046\n",
      "[iter 0] loss=1.1792 val_loss=0.0000 scale=1.0000 norm=1.8139\n",
      "[iter 100] loss=0.9561 val_loss=0.0000 scale=0.5000 norm=0.9013\n",
      "[iter 0] loss=1.1794 val_loss=0.0000 scale=1.0000 norm=1.8137\n",
      "[iter 100] loss=0.9657 val_loss=0.0000 scale=0.5000 norm=0.8986\n",
      "[iter 0] loss=1.1814 val_loss=0.0000 scale=1.0000 norm=1.8093\n",
      "[iter 100] loss=0.9662 val_loss=0.0000 scale=0.5000 norm=0.8967\n",
      "[iter 0] loss=1.1816 val_loss=0.0000 scale=1.0000 norm=1.8092\n",
      "[iter 100] loss=0.9649 val_loss=0.0000 scale=0.5000 norm=0.8968\n",
      "[iter 0] loss=1.1816 val_loss=0.0000 scale=1.0000 norm=1.8090\n",
      "[iter 100] loss=0.9646 val_loss=0.0000 scale=1.0000 norm=1.7951\n",
      "[iter 0] loss=1.1817 val_loss=0.0000 scale=1.0000 norm=1.8089\n",
      "[iter 100] loss=0.9662 val_loss=0.0000 scale=0.5000 norm=0.9018\n",
      "[iter 0] loss=1.1810 val_loss=0.0000 scale=1.0000 norm=1.8103\n",
      "[iter 100] loss=0.9660 val_loss=0.0000 scale=0.5000 norm=0.9000\n",
      "[iter 0] loss=1.1797 val_loss=0.0000 scale=1.0000 norm=1.8129\n",
      "[iter 100] loss=0.9620 val_loss=0.0000 scale=0.5000 norm=0.9002\n",
      "[iter 0] loss=1.1797 val_loss=0.0000 scale=1.0000 norm=1.8128\n",
      "[iter 100] loss=0.9612 val_loss=0.0000 scale=1.0000 norm=1.8003\n",
      "[iter 0] loss=1.1803 val_loss=0.0000 scale=1.0000 norm=1.8120\n",
      "[iter 100] loss=0.9613 val_loss=0.0000 scale=0.5000 norm=0.8999\n",
      "[iter 0] loss=1.1799 val_loss=0.0000 scale=1.0000 norm=1.8126\n",
      "[iter 100] loss=0.9597 val_loss=0.0000 scale=0.5000 norm=0.8980\n",
      "[iter 0] loss=1.1794 val_loss=0.0000 scale=1.0000 norm=1.8133\n",
      "[iter 100] loss=0.9588 val_loss=0.0000 scale=0.5000 norm=0.8994\n",
      "[iter 0] loss=1.1808 val_loss=0.0000 scale=0.5000 norm=0.9052\n",
      "[iter 100] loss=0.9674 val_loss=0.0000 scale=0.5000 norm=0.8980\n",
      "[iter 0] loss=1.1811 val_loss=0.0000 scale=1.0000 norm=1.8100\n",
      "[iter 100] loss=0.9651 val_loss=0.0000 scale=0.5000 norm=0.8990\n",
      "[iter 0] loss=1.1807 val_loss=0.0000 scale=1.0000 norm=1.8104\n",
      "[iter 100] loss=0.9683 val_loss=0.0000 scale=0.5000 norm=0.8963\n",
      "[iter 0] loss=1.1814 val_loss=0.0000 scale=1.0000 norm=1.8093\n",
      "[iter 100] loss=0.9730 val_loss=0.0000 scale=0.5000 norm=0.8929\n",
      "[iter 0] loss=1.1825 val_loss=0.0000 scale=1.0000 norm=1.8068\n",
      "[iter 100] loss=0.9771 val_loss=0.0000 scale=0.5000 norm=0.8931\n",
      "[iter 0] loss=1.1825 val_loss=0.0000 scale=1.0000 norm=1.8069\n",
      "[iter 100] loss=0.9768 val_loss=0.0000 scale=0.5000 norm=0.8945\n",
      "[iter 0] loss=1.1825 val_loss=0.0000 scale=1.0000 norm=1.8071\n",
      "[iter 100] loss=0.9777 val_loss=0.0000 scale=0.5000 norm=0.8942\n",
      "[iter 0] loss=1.1830 val_loss=0.0000 scale=0.5000 norm=0.9031\n",
      "[iter 100] loss=0.9764 val_loss=0.0000 scale=0.5000 norm=0.8933\n",
      "[iter 0] loss=1.1836 val_loss=0.0000 scale=1.0000 norm=1.8051\n",
      "[iter 100] loss=0.9791 val_loss=0.0000 scale=0.5000 norm=0.8913\n",
      "[iter 0] loss=1.1838 val_loss=0.0000 scale=1.0000 norm=1.8047\n",
      "[iter 100] loss=0.9766 val_loss=0.0000 scale=0.5000 norm=0.8926\n",
      "[iter 0] loss=1.1838 val_loss=0.0000 scale=1.0000 norm=1.8049\n",
      "[iter 100] loss=0.9768 val_loss=0.0000 scale=0.5000 norm=0.8921\n",
      "[iter 0] loss=1.1837 val_loss=0.0000 scale=1.0000 norm=1.8049\n",
      "[iter 100] loss=0.9775 val_loss=0.0000 scale=0.5000 norm=0.8915\n",
      "[iter 0] loss=1.1842 val_loss=0.0000 scale=1.0000 norm=1.8042\n",
      "[iter 100] loss=0.9793 val_loss=0.0000 scale=0.5000 norm=0.8923\n",
      "[iter 0] loss=1.1835 val_loss=0.0000 scale=1.0000 norm=1.8054\n",
      "[iter 100] loss=0.9769 val_loss=0.0000 scale=0.5000 norm=0.8923\n",
      "[iter 0] loss=1.1835 val_loss=0.0000 scale=1.0000 norm=1.8054\n",
      "[iter 100] loss=0.9779 val_loss=0.0000 scale=0.5000 norm=0.8917\n",
      "[iter 0] loss=1.1832 val_loss=0.0000 scale=1.0000 norm=1.8059\n",
      "[iter 100] loss=0.9745 val_loss=0.0000 scale=0.5000 norm=0.8914\n",
      "[iter 0] loss=1.1854 val_loss=0.0000 scale=0.5000 norm=0.9005\n",
      "[iter 100] loss=0.9845 val_loss=0.0000 scale=0.5000 norm=0.8881\n",
      "[iter 0] loss=1.1857 val_loss=0.0000 scale=0.5000 norm=0.9004\n",
      "[iter 100] loss=0.9869 val_loss=0.0000 scale=0.5000 norm=0.8872\n",
      "[iter 0] loss=1.1854 val_loss=0.0000 scale=0.5000 norm=0.9007\n",
      "[iter 100] loss=0.9895 val_loss=0.0000 scale=0.5000 norm=0.8880\n",
      "[iter 0] loss=1.1873 val_loss=0.0000 scale=0.5000 norm=0.8989\n",
      "[iter 100] loss=0.9919 val_loss=0.0000 scale=0.5000 norm=0.8896\n",
      "[iter 0] loss=1.1872 val_loss=0.0000 scale=0.5000 norm=0.8990\n",
      "[iter 100] loss=0.9897 val_loss=0.0000 scale=0.5000 norm=0.8895\n",
      "[iter 0] loss=1.1880 val_loss=0.0000 scale=0.5000 norm=0.8981\n",
      "[iter 100] loss=0.9916 val_loss=0.0000 scale=0.5000 norm=0.8885\n",
      "[iter 0] loss=1.1877 val_loss=0.0000 scale=0.5000 norm=0.8982\n",
      "[iter 100] loss=0.9915 val_loss=0.0000 scale=0.5000 norm=0.8887\n",
      "[iter 0] loss=1.1874 val_loss=0.0000 scale=0.5000 norm=0.8984\n",
      "[iter 100] loss=0.9927 val_loss=0.0000 scale=0.5000 norm=0.8893\n",
      "[iter 0] loss=1.1880 val_loss=0.0000 scale=0.5000 norm=0.8979\n",
      "[iter 100] loss=0.9889 val_loss=0.0000 scale=0.5000 norm=0.8926\n",
      "[iter 0] loss=1.1888 val_loss=0.0000 scale=0.5000 norm=0.8970\n",
      "[iter 100] loss=0.9915 val_loss=0.0000 scale=0.5000 norm=0.8885\n",
      "[iter 0] loss=1.1893 val_loss=0.0000 scale=0.5000 norm=0.8966\n",
      "[iter 100] loss=0.9902 val_loss=0.0000 scale=0.5000 norm=0.8894\n",
      "[iter 0] loss=1.1906 val_loss=0.0000 scale=0.5000 norm=0.8952\n",
      "[iter 100] loss=0.9911 val_loss=0.0000 scale=0.5000 norm=0.8908\n",
      "[iter 0] loss=1.1906 val_loss=0.0000 scale=0.5000 norm=0.8952\n",
      "[iter 100] loss=0.9916 val_loss=0.0000 scale=1.0000 norm=1.7829\n",
      "[iter 0] loss=1.1911 val_loss=0.0000 scale=0.5000 norm=0.8950\n",
      "[iter 100] loss=0.9926 val_loss=0.0000 scale=0.5000 norm=0.8917\n",
      "[iter 0] loss=1.1893 val_loss=0.0000 scale=0.5000 norm=0.8968\n",
      "[iter 100] loss=0.9879 val_loss=0.0000 scale=0.5000 norm=0.8939\n",
      "[iter 0] loss=1.1858 val_loss=0.0000 scale=1.0000 norm=1.8011\n",
      "[iter 100] loss=0.9848 val_loss=0.0000 scale=0.5000 norm=0.8959\n",
      "[iter 0] loss=1.1909 val_loss=0.0000 scale=1.0000 norm=1.7912\n",
      "[iter 100] loss=0.9911 val_loss=0.0000 scale=0.5000 norm=0.8913\n",
      "[iter 0] loss=1.1915 val_loss=0.0000 scale=1.0000 norm=1.7902\n",
      "[iter 100] loss=0.9923 val_loss=0.0000 scale=0.5000 norm=0.8912\n",
      "[iter 0] loss=1.1960 val_loss=0.0000 scale=1.0000 norm=1.7810\n",
      "[iter 100] loss=0.9918 val_loss=0.0000 scale=0.5000 norm=0.8876\n",
      "[iter 0] loss=1.1941 val_loss=0.0000 scale=1.0000 norm=1.7854\n",
      "[iter 100] loss=0.9927 val_loss=0.0000 scale=0.5000 norm=0.8892\n",
      "[iter 0] loss=1.1954 val_loss=0.0000 scale=1.0000 norm=1.7826\n",
      "[iter 100] loss=0.9975 val_loss=0.0000 scale=1.0000 norm=1.7722\n",
      "[iter 0] loss=1.1935 val_loss=0.0000 scale=1.0000 norm=1.7862\n",
      "[iter 100] loss=0.9967 val_loss=0.0000 scale=0.5000 norm=0.8874\n",
      "[iter 0] loss=1.1934 val_loss=0.0000 scale=1.0000 norm=1.7863\n",
      "[iter 100] loss=0.9925 val_loss=0.0000 scale=0.5000 norm=0.8867\n",
      "[iter 0] loss=1.1934 val_loss=0.0000 scale=1.0000 norm=1.7863\n",
      "[iter 100] loss=0.9893 val_loss=0.0000 scale=0.5000 norm=0.8874\n",
      "[iter 0] loss=1.1929 val_loss=0.0000 scale=1.0000 norm=1.7879\n",
      "[iter 100] loss=0.9939 val_loss=0.0000 scale=0.5000 norm=0.8857\n",
      "[iter 0] loss=1.1927 val_loss=0.0000 scale=1.0000 norm=1.7879\n",
      "[iter 100] loss=0.9935 val_loss=0.0000 scale=0.5000 norm=0.8844\n",
      "[iter 0] loss=1.1928 val_loss=0.0000 scale=1.0000 norm=1.7875\n",
      "[iter 100] loss=0.9918 val_loss=0.0000 scale=0.5000 norm=0.8842\n",
      "[iter 0] loss=1.1926 val_loss=0.0000 scale=1.0000 norm=1.7877\n",
      "[iter 100] loss=0.9869 val_loss=0.0000 scale=0.5000 norm=0.8854\n",
      "[iter 0] loss=1.1924 val_loss=0.0000 scale=1.0000 norm=1.7881\n",
      "[iter 100] loss=0.9882 val_loss=0.0000 scale=0.5000 norm=0.8843\n",
      "[iter 0] loss=1.1926 val_loss=0.0000 scale=1.0000 norm=1.7879\n",
      "[iter 100] loss=0.9872 val_loss=0.0000 scale=0.5000 norm=0.8846\n",
      "[iter 0] loss=1.1922 val_loss=0.0000 scale=1.0000 norm=1.7884\n",
      "[iter 100] loss=0.9853 val_loss=0.0000 scale=0.5000 norm=0.8851\n",
      "[iter 0] loss=1.1929 val_loss=0.0000 scale=1.0000 norm=1.7871\n",
      "[iter 100] loss=0.9899 val_loss=0.0000 scale=0.5000 norm=0.8852\n",
      "[iter 0] loss=1.1926 val_loss=0.0000 scale=1.0000 norm=1.7878\n",
      "[iter 100] loss=0.9903 val_loss=0.0000 scale=0.5000 norm=0.8850\n",
      "[iter 0] loss=1.1918 val_loss=0.0000 scale=1.0000 norm=1.7894\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=0.5000 norm=0.8882\n",
      "[iter 0] loss=1.1851 val_loss=0.0000 scale=1.0000 norm=1.8006\n",
      "[iter 100] loss=0.9806 val_loss=0.0000 scale=0.5000 norm=0.8946\n",
      "[iter 0] loss=1.1795 val_loss=0.0000 scale=1.0000 norm=1.8102\n",
      "[iter 100] loss=0.9725 val_loss=0.0000 scale=0.5000 norm=0.9006\n",
      "[iter 0] loss=1.1847 val_loss=0.0000 scale=1.0000 norm=1.8004\n",
      "[iter 100] loss=0.9812 val_loss=0.0000 scale=0.5000 norm=0.8949\n",
      "[iter 0] loss=1.1790 val_loss=0.0000 scale=1.0000 norm=1.8070\n",
      "[iter 100] loss=0.9826 val_loss=0.0000 scale=0.5000 norm=0.8968\n",
      "[iter 0] loss=1.1814 val_loss=0.0000 scale=1.0000 norm=1.8022\n",
      "[iter 100] loss=0.9743 val_loss=0.0000 scale=0.5000 norm=0.8950\n",
      "[iter 0] loss=1.1773 val_loss=0.0000 scale=1.0000 norm=1.8069\n",
      "[iter 100] loss=0.9731 val_loss=0.0000 scale=0.5000 norm=0.8972\n",
      "[iter 0] loss=1.1740 val_loss=0.0000 scale=1.0000 norm=1.8115\n",
      "[iter 100] loss=0.9668 val_loss=0.0000 scale=0.5000 norm=0.9034\n",
      "[iter 0] loss=1.1712 val_loss=0.0000 scale=1.0000 norm=1.8136\n",
      "[iter 100] loss=0.9749 val_loss=0.0000 scale=0.5000 norm=0.9013\n",
      "[iter 0] loss=1.1675 val_loss=0.0000 scale=1.0000 norm=1.8178\n",
      "[iter 100] loss=0.9739 val_loss=0.0000 scale=0.5000 norm=0.9027\n",
      "[iter 0] loss=1.1642 val_loss=0.0000 scale=1.0000 norm=1.8248\n",
      "[iter 100] loss=0.9725 val_loss=0.0000 scale=0.5000 norm=0.9053\n",
      "[iter 0] loss=1.1715 val_loss=0.0000 scale=1.0000 norm=1.8043\n",
      "[iter 100] loss=0.9830 val_loss=0.0000 scale=0.5000 norm=0.8941\n",
      "[iter 0] loss=1.1663 val_loss=0.0000 scale=1.0000 norm=1.8096\n",
      "[iter 100] loss=0.9877 val_loss=0.0000 scale=0.5000 norm=0.8939\n",
      "[iter 0] loss=1.1623 val_loss=0.0000 scale=1.0000 norm=1.8114\n",
      "[iter 100] loss=0.9873 val_loss=0.0000 scale=0.5000 norm=0.8943\n",
      "[iter 0] loss=1.1877 val_loss=0.0000 scale=1.0000 norm=1.7638\n",
      "[iter 100] loss=1.0050 val_loss=0.0000 scale=1.0000 norm=1.7543\n",
      "[iter 0] loss=1.2055 val_loss=0.0000 scale=1.0000 norm=1.7293\n",
      "[iter 100] loss=1.0262 val_loss=0.0000 scale=0.5000 norm=0.8621\n",
      "[iter 0] loss=1.2846 val_loss=0.0000 scale=1.0000 norm=1.6202\n",
      "[iter 100] loss=1.0985 val_loss=0.0000 scale=0.5000 norm=0.8159\n",
      "[iter 0] loss=1.2892 val_loss=0.0000 scale=1.0000 norm=1.6097\n",
      "[iter 100] loss=1.1094 val_loss=0.0000 scale=0.5000 norm=0.8100\n",
      "[iter 0] loss=1.2972 val_loss=0.0000 scale=1.0000 norm=1.5927\n",
      "[iter 100] loss=1.1238 val_loss=0.0000 scale=0.5000 norm=0.8007\n",
      "[iter 0] loss=1.2973 val_loss=0.0000 scale=1.0000 norm=1.5923\n",
      "[iter 100] loss=1.1206 val_loss=0.0000 scale=1.0000 norm=1.6021\n",
      "[iter 0] loss=1.2982 val_loss=0.0000 scale=1.0000 norm=1.5876\n",
      "[iter 100] loss=1.1273 val_loss=0.0000 scale=0.5000 norm=0.7980\n",
      "[iter 0] loss=1.2961 val_loss=0.0000 scale=1.0000 norm=1.5882\n",
      "[iter 100] loss=1.1245 val_loss=0.0000 scale=0.5000 norm=0.7968\n",
      "[iter 0] loss=1.3409 val_loss=0.0000 scale=1.0000 norm=1.5275\n",
      "[iter 100] loss=1.1782 val_loss=0.0000 scale=1.0000 norm=1.5367\n",
      "[iter 0] loss=1.3398 val_loss=0.0000 scale=1.0000 norm=1.5298\n",
      "[iter 100] loss=1.1760 val_loss=0.0000 scale=1.0000 norm=1.5399\n",
      "[iter 0] loss=1.3563 val_loss=0.0000 scale=1.0000 norm=1.5040\n",
      "[iter 100] loss=1.2009 val_loss=0.0000 scale=1.0000 norm=1.5130\n",
      "[iter 0] loss=1.3558 val_loss=0.0000 scale=1.0000 norm=1.5024\n",
      "[iter 100] loss=1.2032 val_loss=0.0000 scale=1.0000 norm=1.5076\n",
      "[iter 0] loss=1.3549 val_loss=0.0000 scale=1.0000 norm=1.5024\n",
      "[iter 100] loss=1.2078 val_loss=0.0000 scale=1.0000 norm=1.5069\n",
      "[iter 0] loss=1.3514 val_loss=0.0000 scale=1.0000 norm=1.5078\n",
      "[iter 100] loss=1.2038 val_loss=0.0000 scale=1.0000 norm=1.5109\n",
      "[iter 0] loss=1.3592 val_loss=0.0000 scale=1.0000 norm=1.4935\n",
      "[iter 100] loss=1.2140 val_loss=0.0000 scale=1.0000 norm=1.4958\n",
      "[iter 0] loss=1.3582 val_loss=0.0000 scale=1.0000 norm=1.4943\n",
      "[iter 100] loss=1.2206 val_loss=0.0000 scale=1.0000 norm=1.4962\n",
      "[iter 0] loss=1.3552 val_loss=0.0000 scale=1.0000 norm=1.4989\n",
      "[iter 100] loss=1.2188 val_loss=0.0000 scale=1.0000 norm=1.5012\n",
      "[iter 0] loss=1.3863 val_loss=0.0000 scale=1.0000 norm=1.4590\n",
      "[iter 100] loss=1.2558 val_loss=0.0000 scale=1.0000 norm=1.4596\n",
      "[iter 0] loss=1.3858 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 100] loss=1.2537 val_loss=0.0000 scale=1.0000 norm=1.4611\n",
      "[iter 0] loss=1.3866 val_loss=0.0000 scale=1.0000 norm=1.4576\n",
      "[iter 100] loss=1.2595 val_loss=0.0000 scale=1.0000 norm=1.4583\n",
      "[iter 0] loss=1.3851 val_loss=0.0000 scale=1.0000 norm=1.4605\n",
      "[iter 100] loss=1.2570 val_loss=0.0000 scale=1.0000 norm=1.4612\n",
      "[iter 0] loss=1.3826 val_loss=0.0000 scale=1.0000 norm=1.4652\n",
      "[iter 100] loss=1.2529 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3828 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 100] loss=1.2575 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 0] loss=1.3810 val_loss=0.0000 scale=1.0000 norm=1.4642\n",
      "[iter 100] loss=1.2568 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 0] loss=1.3809 val_loss=0.0000 scale=1.0000 norm=1.4645\n",
      "[iter 100] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.4656\n",
      "[iter 0] loss=1.3806 val_loss=0.0000 scale=1.0000 norm=1.4637\n",
      "[iter 100] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.4645\n",
      "[iter 0] loss=1.3791 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 100] loss=1.2572 val_loss=0.0000 scale=1.0000 norm=1.4646\n",
      "[iter 0] loss=1.3806 val_loss=0.0000 scale=1.0000 norm=1.4596\n",
      "[iter 100] loss=1.2575 val_loss=0.0000 scale=1.0000 norm=1.4604\n",
      "[iter 0] loss=1.3808 val_loss=0.0000 scale=1.0000 norm=1.4583\n",
      "[iter 100] loss=1.2591 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.3816 val_loss=0.0000 scale=1.0000 norm=1.4565\n",
      "[iter 100] loss=1.2601 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.3800 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 100] loss=1.2599 val_loss=0.0000 scale=1.0000 norm=1.4621\n",
      "[iter 0] loss=1.3783 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 100] loss=1.2579 val_loss=0.0000 scale=1.0000 norm=1.4630\n",
      "[iter 0] loss=1.3773 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 100] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.4665\n",
      "[iter 0] loss=1.3761 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 100] loss=1.2573 val_loss=0.0000 scale=1.0000 norm=1.4667\n",
      "[iter 0] loss=1.3755 val_loss=0.0000 scale=1.0000 norm=1.4668\n",
      "[iter 100] loss=1.2566 val_loss=0.0000 scale=1.0000 norm=1.4670\n",
      "[iter 0] loss=1.3753 val_loss=0.0000 scale=1.0000 norm=1.4650\n",
      "[iter 100] loss=1.2585 val_loss=0.0000 scale=1.0000 norm=1.4683\n",
      "[iter 0] loss=1.3771 val_loss=0.0000 scale=1.0000 norm=1.4614\n",
      "[iter 100] loss=1.2610 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 0] loss=1.3764 val_loss=0.0000 scale=1.0000 norm=1.4632\n",
      "[iter 100] loss=1.2588 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 0] loss=1.3751 val_loss=0.0000 scale=1.0000 norm=1.4655\n",
      "[iter 100] loss=1.2593 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 0] loss=1.3742 val_loss=0.0000 scale=1.0000 norm=1.4672\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.4701\n",
      "[iter 0] loss=1.3748 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 100] loss=1.2634 val_loss=0.0000 scale=1.0000 norm=1.4680\n",
      "[iter 0] loss=1.3770 val_loss=0.0000 scale=1.0000 norm=1.4622\n",
      "[iter 100] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 0] loss=1.3768 val_loss=0.0000 scale=1.0000 norm=1.4627\n",
      "[iter 100] loss=1.2582 val_loss=0.0000 scale=1.0000 norm=1.4638\n",
      "[iter 0] loss=1.3749 val_loss=0.0000 scale=1.0000 norm=1.4640\n",
      "[iter 100] loss=1.2562 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 0] loss=1.3756 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 100] loss=1.2580 val_loss=0.0000 scale=1.0000 norm=1.4682\n",
      "[iter 0] loss=1.3763 val_loss=0.0000 scale=1.0000 norm=1.4616\n",
      "[iter 100] loss=1.2585 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.3793 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 100] loss=1.2642 val_loss=0.0000 scale=1.0000 norm=1.4556\n",
      "[iter 0] loss=1.3774 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 100] loss=1.2659 val_loss=0.0000 scale=1.0000 norm=1.4604\n",
      "[iter 0] loss=1.3763 val_loss=0.0000 scale=1.0000 norm=1.4591\n",
      "[iter 100] loss=1.2653 val_loss=0.0000 scale=1.0000 norm=1.4627\n",
      "[iter 0] loss=1.3745 val_loss=0.0000 scale=1.0000 norm=1.4625\n",
      "[iter 100] loss=1.2633 val_loss=0.0000 scale=1.0000 norm=1.4654\n",
      "[iter 0] loss=1.3755 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 100] loss=1.2628 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 0] loss=1.3746 val_loss=0.0000 scale=1.0000 norm=1.4620\n",
      "[iter 100] loss=1.2614 val_loss=0.0000 scale=1.0000 norm=1.4642\n",
      "[iter 0] loss=1.3726 val_loss=0.0000 scale=1.0000 norm=1.4659\n",
      "[iter 100] loss=1.2581 val_loss=0.0000 scale=1.0000 norm=1.4662\n",
      "[iter 0] loss=1.3720 val_loss=0.0000 scale=1.0000 norm=1.4672\n",
      "[iter 100] loss=1.2560 val_loss=0.0000 scale=1.0000 norm=1.4666\n",
      "[iter 0] loss=1.3720 val_loss=0.0000 scale=1.0000 norm=1.4674\n",
      "[iter 100] loss=1.2557 val_loss=0.0000 scale=1.0000 norm=1.4664\n",
      "[iter 0] loss=1.3722 val_loss=0.0000 scale=1.0000 norm=1.4668\n",
      "[iter 100] loss=1.2588 val_loss=0.0000 scale=1.0000 norm=1.4675\n",
      "[iter 0] loss=1.3716 val_loss=0.0000 scale=1.0000 norm=1.4687\n",
      "[iter 100] loss=1.2570 val_loss=0.0000 scale=1.0000 norm=1.4672\n",
      "[iter 0] loss=1.3706 val_loss=0.0000 scale=1.0000 norm=1.4703\n",
      "[iter 100] loss=1.2530 val_loss=0.0000 scale=1.0000 norm=1.4696\n",
      "[iter 0] loss=1.3706 val_loss=0.0000 scale=1.0000 norm=1.4701\n",
      "[iter 100] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.4696\n",
      "[iter 0] loss=1.3697 val_loss=0.0000 scale=1.0000 norm=1.4705\n",
      "[iter 100] loss=1.2499 val_loss=0.0000 scale=1.0000 norm=1.4707\n",
      "[iter 0] loss=1.3682 val_loss=0.0000 scale=1.0000 norm=1.4736\n",
      "[iter 100] loss=1.2427 val_loss=0.0000 scale=1.0000 norm=1.4731\n",
      "[iter 0] loss=1.3664 val_loss=0.0000 scale=1.0000 norm=1.4767\n",
      "[iter 100] loss=1.2410 val_loss=0.0000 scale=1.0000 norm=1.4769\n",
      "[iter 0] loss=1.3650 val_loss=0.0000 scale=1.0000 norm=1.4794\n",
      "[iter 100] loss=1.2361 val_loss=0.0000 scale=1.0000 norm=1.4792\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4242\n",
      "[iter 100] loss=1.2911 val_loss=0.0000 scale=1.0000 norm=1.4229\n",
      "[iter 0] loss=1.4093 val_loss=0.0000 scale=1.0000 norm=1.4279\n",
      "[iter 100] loss=1.2898 val_loss=0.0000 scale=1.0000 norm=1.4256\n",
      "[iter 0] loss=1.4090 val_loss=0.0000 scale=1.0000 norm=1.4288\n",
      "[iter 100] loss=1.2861 val_loss=0.0000 scale=1.0000 norm=1.4260\n",
      "[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=1.4291\n",
      "[iter 100] loss=1.2839 val_loss=0.0000 scale=1.0000 norm=1.4269\n",
      "[iter 0] loss=1.4078 val_loss=0.0000 scale=1.0000 norm=1.4308\n",
      "[iter 100] loss=1.2809 val_loss=0.0000 scale=1.0000 norm=1.4287\n",
      "[iter 0] loss=1.4106 val_loss=0.0000 scale=1.0000 norm=1.4263\n",
      "[iter 100] loss=1.2839 val_loss=0.0000 scale=1.0000 norm=1.4233\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4257\n",
      "[iter 100] loss=1.2813 val_loss=0.0000 scale=1.0000 norm=1.4218\n",
      "[iter 0] loss=1.4108 val_loss=0.0000 scale=1.0000 norm=1.4263\n",
      "[iter 100] loss=1.2834 val_loss=0.0000 scale=1.0000 norm=1.4217\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4259\n",
      "[iter 100] loss=1.2819 val_loss=0.0000 scale=1.0000 norm=1.4205\n",
      "[iter 0] loss=1.4125 val_loss=0.0000 scale=1.0000 norm=1.4220\n",
      "[iter 100] loss=1.2793 val_loss=0.0000 scale=1.0000 norm=1.4150\n",
      "[iter 0] loss=1.4111 val_loss=0.0000 scale=1.0000 norm=1.4252\n",
      "[iter 100] loss=1.2724 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 0] loss=1.4116 val_loss=0.0000 scale=1.0000 norm=1.4234\n",
      "[iter 100] loss=1.2766 val_loss=0.0000 scale=1.0000 norm=1.4165\n",
      "[iter 0] loss=1.4096 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 100] loss=1.2747 val_loss=0.0000 scale=1.0000 norm=1.4200\n",
      "[iter 0] loss=1.4075 val_loss=0.0000 scale=1.0000 norm=1.4310\n",
      "[iter 100] loss=1.2704 val_loss=0.0000 scale=1.0000 norm=1.4244\n",
      "[iter 0] loss=1.4075 val_loss=0.0000 scale=1.0000 norm=1.4310\n",
      "[iter 100] loss=1.2739 val_loss=0.0000 scale=1.0000 norm=1.4245\n",
      "[iter 0] loss=1.4081 val_loss=0.0000 scale=1.0000 norm=1.4295\n",
      "[iter 100] loss=1.2709 val_loss=0.0000 scale=1.0000 norm=1.4239\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4321\n",
      "[iter 100] loss=1.2675 val_loss=0.0000 scale=1.0000 norm=1.4258\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4366\n",
      "[iter 100] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=1.4302\n",
      "[iter 0] loss=1.4053 val_loss=0.0000 scale=1.0000 norm=1.4345\n",
      "[iter 100] loss=1.2668 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 0] loss=1.4054 val_loss=0.0000 scale=1.0000 norm=1.4342\n",
      "[iter 100] loss=1.2695 val_loss=0.0000 scale=1.0000 norm=1.4280\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4373\n",
      "[iter 100] loss=1.2697 val_loss=0.0000 scale=1.0000 norm=1.4303\n",
      "[iter 0] loss=1.4023 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 100] loss=1.2655 val_loss=0.0000 scale=1.0000 norm=1.4341\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4438\n",
      "[iter 100] loss=1.2637 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 100] loss=1.2626 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 100] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=1.4382\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4390\n",
      "[iter 100] loss=1.2680 val_loss=0.0000 scale=1.0000 norm=1.4319\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4376\n",
      "[iter 100] loss=1.2700 val_loss=0.0000 scale=1.0000 norm=1.4298\n",
      "[iter 0] loss=1.4061 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 100] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=1.4269\n",
      "[iter 0] loss=1.4057 val_loss=0.0000 scale=1.0000 norm=1.4362\n",
      "[iter 100] loss=1.2731 val_loss=0.0000 scale=1.0000 norm=1.4288\n",
      "[iter 0] loss=1.4042 val_loss=0.0000 scale=1.0000 norm=1.4375\n",
      "[iter 100] loss=1.2718 val_loss=0.0000 scale=1.0000 norm=1.4300\n",
      "[iter 0] loss=1.4025 val_loss=0.0000 scale=1.0000 norm=1.4409\n",
      "[iter 100] loss=1.2708 val_loss=0.0000 scale=1.0000 norm=1.4349\n",
      "[iter 0] loss=1.4030 val_loss=0.0000 scale=1.0000 norm=1.4403\n",
      "[iter 100] loss=1.2695 val_loss=0.0000 scale=1.0000 norm=1.4348\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4434\n",
      "[iter 100] loss=1.2653 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4471\n",
      "[iter 100] loss=1.2625 val_loss=0.0000 scale=1.0000 norm=1.4417\n",
      "[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 100] loss=1.2650 val_loss=0.0000 scale=1.0000 norm=1.4420\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.2657 val_loss=0.0000 scale=1.0000 norm=1.4434\n",
      "[iter 0] loss=1.3999 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 100] loss=1.2640 val_loss=0.0000 scale=1.0000 norm=1.4415\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4445\n",
      "[iter 100] loss=1.2656 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 0] loss=1.4009 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 100] loss=1.2619 val_loss=0.0000 scale=1.0000 norm=1.4399\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4465\n",
      "[iter 100] loss=1.2589 val_loss=0.0000 scale=1.0000 norm=1.4415\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4460\n",
      "[iter 100] loss=1.2598 val_loss=0.0000 scale=1.0000 norm=1.4404\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4462\n",
      "[iter 100] loss=1.2577 val_loss=0.0000 scale=1.0000 norm=1.4398\n",
      "[iter 0] loss=1.3983 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 100] loss=1.2525 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 0] loss=1.3961 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 100] loss=1.2495 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4555\n",
      "[iter 100] loss=1.2501 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 0] loss=1.3954 val_loss=0.0000 scale=1.0000 norm=1.4558\n",
      "[iter 100] loss=1.2467 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 0] loss=1.3974 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.4463\n",
      "[iter 0] loss=1.4002 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.2552 val_loss=0.0000 scale=1.0000 norm=1.4401\n",
      "[iter 0] loss=1.3990 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 100] loss=1.2564 val_loss=0.0000 scale=1.0000 norm=1.4423\n",
      "[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.2549 val_loss=0.0000 scale=1.0000 norm=1.4390\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4449\n",
      "[iter 100] loss=1.2578 val_loss=0.0000 scale=1.0000 norm=1.4370\n",
      "[iter 0] loss=1.3996 val_loss=0.0000 scale=1.0000 norm=1.4483\n",
      "[iter 100] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=1.4395\n",
      "[iter 0] loss=1.3995 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.2484 val_loss=0.0000 scale=1.0000 norm=1.4386\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 100] loss=1.2497 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 0] loss=1.4007 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 100] loss=1.2540 val_loss=0.0000 scale=1.0000 norm=1.4408\n",
      "[iter 0] loss=1.4002 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.2557 val_loss=0.0000 scale=1.0000 norm=1.4420\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.2551 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 100] loss=1.2517 val_loss=0.0000 scale=1.0000 norm=1.4444\n",
      "[iter 0] loss=1.3967 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 100] loss=1.2455 val_loss=0.0000 scale=1.0000 norm=1.4484\n",
      "[iter 0] loss=1.4089 val_loss=0.0000 scale=1.0000 norm=1.4380\n",
      "[iter 100] loss=1.2557 val_loss=0.0000 scale=1.0000 norm=1.4306\n",
      "[iter 0] loss=1.4076 val_loss=0.0000 scale=1.0000 norm=1.4402\n",
      "[iter 100] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=1.4338\n",
      "[iter 0] loss=1.4126 val_loss=0.0000 scale=1.0000 norm=1.4324\n",
      "[iter 100] loss=1.2602 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 0] loss=1.4122 val_loss=0.0000 scale=1.0000 norm=1.4321\n",
      "[iter 100] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.4287\n",
      "[iter 0] loss=1.4124 val_loss=0.0000 scale=1.0000 norm=1.4303\n",
      "[iter 100] loss=1.2685 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 0] loss=1.4124 val_loss=0.0000 scale=1.0000 norm=1.4303\n",
      "[iter 100] loss=1.2653 val_loss=0.0000 scale=1.0000 norm=1.4273\n",
      "[iter 0] loss=1.4117 val_loss=0.0000 scale=1.0000 norm=1.4316\n",
      "[iter 100] loss=1.2656 val_loss=0.0000 scale=1.0000 norm=1.4272\n",
      "[iter 0] loss=1.4125 val_loss=0.0000 scale=1.0000 norm=1.4298\n",
      "[iter 100] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=1.4254\n",
      "[iter 0] loss=1.4117 val_loss=0.0000 scale=1.0000 norm=1.4315\n",
      "[iter 100] loss=1.2655 val_loss=0.0000 scale=1.0000 norm=1.4262\n",
      "[iter 0] loss=1.4119 val_loss=0.0000 scale=1.0000 norm=1.4314\n",
      "[iter 100] loss=1.2660 val_loss=0.0000 scale=1.0000 norm=1.4278\n",
      "[iter 0] loss=1.4112 val_loss=0.0000 scale=1.0000 norm=1.4332\n",
      "[iter 100] loss=1.2657 val_loss=0.0000 scale=1.0000 norm=1.4284\n",
      "[iter 0] loss=1.4087 val_loss=0.0000 scale=1.0000 norm=1.4371\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=1.4375\n",
      "[iter 100] loss=1.2615 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 0] loss=1.4127 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 100] loss=1.2673 val_loss=0.0000 scale=1.0000 norm=1.4255\n",
      "[iter 0] loss=1.4126 val_loss=0.0000 scale=1.0000 norm=1.4297\n",
      "[iter 100] loss=1.2652 val_loss=0.0000 scale=1.0000 norm=1.4244\n",
      "[iter 0] loss=1.4109 val_loss=0.0000 scale=1.0000 norm=1.4324\n",
      "[iter 100] loss=1.2652 val_loss=0.0000 scale=1.0000 norm=1.4262\n",
      "[iter 0] loss=1.4088 val_loss=0.0000 scale=1.0000 norm=1.4368\n",
      "[iter 100] loss=1.2690 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 0] loss=1.4076 val_loss=0.0000 scale=1.0000 norm=1.4394\n",
      "[iter 100] loss=1.2676 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 0] loss=1.4089 val_loss=0.0000 scale=1.0000 norm=1.4384\n",
      "[iter 100] loss=1.2701 val_loss=0.0000 scale=1.0000 norm=1.4325\n",
      "[iter 0] loss=1.4105 val_loss=0.0000 scale=1.0000 norm=1.4356\n",
      "[iter 100] loss=1.2701 val_loss=0.0000 scale=1.0000 norm=1.4310\n",
      "[iter 0] loss=1.4107 val_loss=0.0000 scale=1.0000 norm=1.4358\n",
      "[iter 100] loss=1.2734 val_loss=0.0000 scale=1.0000 norm=1.4324\n",
      "[iter 0] loss=1.4106 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 100] loss=1.2694 val_loss=0.0000 scale=1.0000 norm=1.4321\n",
      "[iter 0] loss=1.4083 val_loss=0.0000 scale=1.0000 norm=1.4380\n",
      "[iter 100] loss=1.2672 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 0] loss=1.4075 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 100] loss=1.2703 val_loss=0.0000 scale=1.0000 norm=1.4355\n",
      "[iter 0] loss=1.4066 val_loss=0.0000 scale=1.0000 norm=1.4403\n",
      "[iter 100] loss=1.2710 val_loss=0.0000 scale=1.0000 norm=1.4376\n",
      "[iter 0] loss=1.4069 val_loss=0.0000 scale=1.0000 norm=1.4402\n",
      "[iter 100] loss=1.2736 val_loss=0.0000 scale=1.0000 norm=1.4381\n",
      "[iter 0] loss=1.4052 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 100] loss=1.2706 val_loss=0.0000 scale=1.0000 norm=1.4391\n",
      "[iter 0] loss=1.4035 val_loss=0.0000 scale=1.0000 norm=1.4457\n",
      "[iter 100] loss=1.2694 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4458\n",
      "[iter 100] loss=1.2702 val_loss=0.0000 scale=1.0000 norm=1.4441\n",
      "[iter 0] loss=1.4048 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 100] loss=1.2711 val_loss=0.0000 scale=1.0000 norm=1.4414\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4433\n",
      "[iter 100] loss=1.2680 val_loss=0.0000 scale=1.0000 norm=1.4416\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 100] loss=1.2665 val_loss=0.0000 scale=1.0000 norm=1.4426\n",
      "[iter 0] loss=1.4036 val_loss=0.0000 scale=1.0000 norm=1.4437\n",
      "[iter 100] loss=1.2658 val_loss=0.0000 scale=1.0000 norm=1.4402\n",
      "[iter 0] loss=1.4031 val_loss=0.0000 scale=1.0000 norm=1.4447\n",
      "[iter 100] loss=1.2672 val_loss=0.0000 scale=1.0000 norm=1.4425\n",
      "[iter 0] loss=1.4029 val_loss=0.0000 scale=1.0000 norm=1.4455\n",
      "[iter 100] loss=1.2660 val_loss=0.0000 scale=1.0000 norm=1.4418\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4448\n",
      "[iter 100] loss=1.2680 val_loss=0.0000 scale=1.0000 norm=1.4414\n",
      "[iter 0] loss=1.4020 val_loss=0.0000 scale=1.0000 norm=1.4456\n",
      "[iter 100] loss=1.2627 val_loss=0.0000 scale=1.0000 norm=1.4441\n",
      "[iter 0] loss=1.4026 val_loss=0.0000 scale=1.0000 norm=1.4445\n",
      "[iter 100] loss=1.2635 val_loss=0.0000 scale=1.0000 norm=1.4415\n",
      "[iter 0] loss=1.4241 val_loss=0.0000 scale=1.0000 norm=1.4167\n",
      "[iter 100] loss=1.2901 val_loss=0.0000 scale=1.0000 norm=1.4083\n",
      "[iter 0] loss=1.4219 val_loss=0.0000 scale=1.0000 norm=1.4208\n",
      "[iter 100] loss=1.2897 val_loss=0.0000 scale=1.0000 norm=1.4140\n",
      "[iter 0] loss=1.4195 val_loss=0.0000 scale=1.0000 norm=1.4246\n",
      "[iter 100] loss=1.2857 val_loss=0.0000 scale=1.0000 norm=1.4166\n",
      "[iter 0] loss=1.4174 val_loss=0.0000 scale=1.0000 norm=1.4283\n",
      "[iter 100] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=1.4215\n",
      "[iter 0] loss=1.4165 val_loss=0.0000 scale=1.0000 norm=1.4285\n",
      "[iter 100] loss=1.2868 val_loss=0.0000 scale=1.0000 norm=1.4214\n",
      "[iter 0] loss=1.4174 val_loss=0.0000 scale=1.0000 norm=1.4266\n",
      "[iter 100] loss=1.2919 val_loss=0.0000 scale=1.0000 norm=1.4206\n",
      "[iter 0] loss=1.4168 val_loss=0.0000 scale=1.0000 norm=1.4270\n",
      "[iter 100] loss=1.2889 val_loss=0.0000 scale=1.0000 norm=1.4241\n",
      "[iter 0] loss=1.4167 val_loss=0.0000 scale=1.0000 norm=1.4275\n",
      "[iter 100] loss=1.2874 val_loss=0.0000 scale=1.0000 norm=1.4244\n",
      "[iter 0] loss=1.4155 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 100] loss=1.2887 val_loss=0.0000 scale=1.0000 norm=1.4281\n",
      "[iter 0] loss=1.4175 val_loss=0.0000 scale=1.0000 norm=1.4260\n",
      "[iter 100] loss=1.2911 val_loss=0.0000 scale=1.0000 norm=1.4245\n",
      "[iter 0] loss=1.4162 val_loss=0.0000 scale=1.0000 norm=1.4291\n",
      "[iter 100] loss=1.2916 val_loss=0.0000 scale=1.0000 norm=1.4273\n",
      "[iter 0] loss=1.4148 val_loss=0.0000 scale=1.0000 norm=1.4305\n",
      "[iter 100] loss=1.2856 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 0] loss=1.4126 val_loss=0.0000 scale=1.0000 norm=1.4343\n",
      "[iter 100] loss=1.2897 val_loss=0.0000 scale=1.0000 norm=1.4346\n",
      "[iter 0] loss=1.4105 val_loss=0.0000 scale=1.0000 norm=1.4377\n",
      "[iter 100] loss=1.2899 val_loss=0.0000 scale=1.0000 norm=1.4394\n",
      "[iter 0] loss=1.4098 val_loss=0.0000 scale=1.0000 norm=1.4391\n",
      "[iter 100] loss=1.2829 val_loss=0.0000 scale=1.0000 norm=1.4398\n",
      "[iter 0] loss=1.4072 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 100] loss=1.2861 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 0] loss=1.4073 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 100] loss=1.2830 val_loss=0.0000 scale=1.0000 norm=1.4421\n",
      "[iter 0] loss=1.4061 val_loss=0.0000 scale=1.0000 norm=1.4447\n",
      "[iter 100] loss=1.2815 val_loss=0.0000 scale=1.0000 norm=1.4439\n",
      "[iter 0] loss=1.4053 val_loss=0.0000 scale=1.0000 norm=1.4467\n",
      "[iter 100] loss=1.2812 val_loss=0.0000 scale=1.0000 norm=1.4443\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 100] loss=1.2778 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 100] loss=1.2712 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4525\n",
      "[iter 100] loss=1.2736 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 100] loss=1.2711 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4560\n",
      "[iter 100] loss=1.2768 val_loss=0.0000 scale=1.0000 norm=1.4540\n",
      "[iter 0] loss=1.4034 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 100] loss=1.2755 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.2780 val_loss=0.0000 scale=1.0000 norm=1.4451\n",
      "[iter 0] loss=1.4065 val_loss=0.0000 scale=1.0000 norm=1.4472\n",
      "[iter 100] loss=1.2720 val_loss=0.0000 scale=1.0000 norm=1.4456\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.2684 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.2701 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 0] loss=1.4061 val_loss=0.0000 scale=1.0000 norm=1.4481\n",
      "[iter 100] loss=1.2652 val_loss=0.0000 scale=1.0000 norm=1.4431\n",
      "[iter 0] loss=1.4076 val_loss=0.0000 scale=1.0000 norm=1.4450\n",
      "[iter 100] loss=1.2640 val_loss=0.0000 scale=1.0000 norm=1.4411\n",
      "[iter 0] loss=1.4100 val_loss=0.0000 scale=1.0000 norm=1.4412\n",
      "[iter 100] loss=1.2612 val_loss=0.0000 scale=1.0000 norm=1.4338\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 100] loss=1.2621 val_loss=0.0000 scale=1.0000 norm=1.4319\n",
      "[iter 0] loss=1.4107 val_loss=0.0000 scale=1.0000 norm=1.4407\n",
      "[iter 100] loss=1.2533 val_loss=0.0000 scale=1.0000 norm=1.4321\n",
      "[iter 0] loss=1.4132 val_loss=0.0000 scale=1.0000 norm=1.4361\n",
      "[iter 100] loss=1.2576 val_loss=0.0000 scale=1.0000 norm=1.4282\n",
      "[iter 0] loss=1.4152 val_loss=0.0000 scale=1.0000 norm=1.4335\n",
      "[iter 100] loss=1.2583 val_loss=0.0000 scale=1.0000 norm=1.4298\n",
      "[iter 0] loss=1.4170 val_loss=0.0000 scale=1.0000 norm=1.4311\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.4243\n",
      "[iter 0] loss=1.4176 val_loss=0.0000 scale=1.0000 norm=1.4311\n",
      "[iter 100] loss=1.2606 val_loss=0.0000 scale=1.0000 norm=1.4203\n",
      "[iter 0] loss=1.4190 val_loss=0.0000 scale=1.0000 norm=1.4285\n",
      "[iter 100] loss=1.2610 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 0] loss=1.4197 val_loss=0.0000 scale=1.0000 norm=1.4265\n",
      "[iter 100] loss=1.2588 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 0] loss=1.4217 val_loss=0.0000 scale=1.0000 norm=1.4230\n",
      "[iter 100] loss=1.2617 val_loss=0.0000 scale=1.0000 norm=1.4083\n",
      "[iter 0] loss=1.4206 val_loss=0.0000 scale=1.0000 norm=1.4256\n",
      "[iter 100] loss=1.2614 val_loss=0.0000 scale=1.0000 norm=1.4097\n",
      "[iter 0] loss=1.4200 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 100] loss=1.2654 val_loss=0.0000 scale=1.0000 norm=1.4122\n",
      "[iter 0] loss=1.4208 val_loss=0.0000 scale=1.0000 norm=1.4252\n",
      "[iter 100] loss=1.2680 val_loss=0.0000 scale=1.0000 norm=1.4134\n",
      "[iter 0] loss=1.4206 val_loss=0.0000 scale=1.0000 norm=1.4258\n",
      "[iter 100] loss=1.2664 val_loss=0.0000 scale=1.0000 norm=1.4168\n",
      "[iter 0] loss=1.4192 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 100] loss=1.2615 val_loss=0.0000 scale=1.0000 norm=1.4136\n",
      "[iter 0] loss=1.4183 val_loss=0.0000 scale=1.0000 norm=1.4290\n",
      "[iter 100] loss=1.2639 val_loss=0.0000 scale=1.0000 norm=1.4184\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=1.0000 norm=1.4308\n",
      "[iter 100] loss=1.2656 val_loss=0.0000 scale=1.0000 norm=1.4211\n",
      "[iter 0] loss=1.4159 val_loss=0.0000 scale=1.0000 norm=1.4341\n",
      "[iter 100] loss=1.2633 val_loss=0.0000 scale=1.0000 norm=1.4245\n",
      "[iter 0] loss=1.4163 val_loss=0.0000 scale=1.0000 norm=1.4335\n",
      "[iter 100] loss=1.2602 val_loss=0.0000 scale=1.0000 norm=1.4236\n",
      "[iter 0] loss=1.4148 val_loss=0.0000 scale=1.0000 norm=1.4365\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.4274\n",
      "[iter 0] loss=1.4164 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 100] loss=1.2593 val_loss=0.0000 scale=1.0000 norm=1.4238\n",
      "[iter 0] loss=1.4165 val_loss=0.0000 scale=1.0000 norm=1.4327\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.4221\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=1.0000 norm=1.4312\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.4213\n",
      "[iter 0] loss=1.4160 val_loss=0.0000 scale=1.0000 norm=1.4352\n",
      "[iter 100] loss=1.2555 val_loss=0.0000 scale=1.0000 norm=1.4262\n",
      "[iter 0] loss=1.4159 val_loss=0.0000 scale=1.0000 norm=1.4372\n",
      "[iter 100] loss=1.2568 val_loss=0.0000 scale=1.0000 norm=1.4248\n",
      "[iter 0] loss=1.4162 val_loss=0.0000 scale=1.0000 norm=1.4366\n",
      "[iter 100] loss=1.2530 val_loss=0.0000 scale=1.0000 norm=1.4223\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4320\n",
      "[iter 100] loss=1.2578 val_loss=0.0000 scale=1.0000 norm=1.4195\n",
      "[iter 0] loss=1.4194 val_loss=0.0000 scale=1.0000 norm=1.4324\n",
      "[iter 100] loss=1.2555 val_loss=0.0000 scale=1.0000 norm=1.4169\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4327\n",
      "[iter 100] loss=1.2589 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 0] loss=1.4218 val_loss=0.0000 scale=1.0000 norm=1.4288\n",
      "[iter 100] loss=1.2621 val_loss=0.0000 scale=1.0000 norm=1.4126\n",
      "[iter 0] loss=1.4261 val_loss=0.0000 scale=1.0000 norm=1.4225\n",
      "[iter 100] loss=1.2696 val_loss=0.0000 scale=1.0000 norm=1.4114\n",
      "[iter 0] loss=1.4257 val_loss=0.0000 scale=1.0000 norm=1.4231\n",
      "[iter 100] loss=1.2678 val_loss=0.0000 scale=1.0000 norm=1.4135\n",
      "[iter 0] loss=1.4232 val_loss=0.0000 scale=1.0000 norm=1.4263\n",
      "[iter 100] loss=1.2668 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 0] loss=1.4224 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 100] loss=1.2687 val_loss=0.0000 scale=1.0000 norm=1.4172\n",
      "[iter 0] loss=1.4212 val_loss=0.0000 scale=1.0000 norm=1.4291\n",
      "[iter 100] loss=1.2678 val_loss=0.0000 scale=1.0000 norm=1.4195\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=1.0000 norm=1.4255\n",
      "[iter 100] loss=1.2706 val_loss=0.0000 scale=1.0000 norm=1.4156\n",
      "[iter 0] loss=1.4181 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 100] loss=1.2642 val_loss=0.0000 scale=1.0000 norm=1.4225\n",
      "[iter 0] loss=1.4184 val_loss=0.0000 scale=1.0000 norm=1.4328\n",
      "[iter 100] loss=1.2665 val_loss=0.0000 scale=1.0000 norm=1.4218\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=1.0000 norm=1.4337\n",
      "[iter 100] loss=1.2695 val_loss=0.0000 scale=1.0000 norm=1.4224\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4309\n",
      "[iter 100] loss=1.2704 val_loss=0.0000 scale=1.0000 norm=1.4202\n",
      "[iter 0] loss=1.4215 val_loss=0.0000 scale=1.0000 norm=1.4273\n",
      "[iter 100] loss=1.2735 val_loss=0.0000 scale=1.0000 norm=1.4171\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=1.0000 norm=1.4253\n",
      "[iter 100] loss=1.2757 val_loss=0.0000 scale=1.0000 norm=1.4164\n",
      "[iter 0] loss=1.4239 val_loss=0.0000 scale=1.0000 norm=1.4248\n",
      "[iter 100] loss=1.2697 val_loss=0.0000 scale=1.0000 norm=1.4151\n",
      "[iter 0] loss=1.4246 val_loss=0.0000 scale=1.0000 norm=1.4230\n",
      "[iter 100] loss=1.2727 val_loss=0.0000 scale=1.0000 norm=1.4149\n",
      "[iter 0] loss=1.4266 val_loss=0.0000 scale=1.0000 norm=1.4197\n",
      "[iter 100] loss=1.2722 val_loss=0.0000 scale=1.0000 norm=1.4130\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=1.0000 norm=1.4220\n",
      "[iter 100] loss=1.2740 val_loss=0.0000 scale=1.0000 norm=1.4152\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=1.0000 norm=1.4227\n",
      "[iter 100] loss=1.2751 val_loss=0.0000 scale=1.0000 norm=1.4171\n",
      "[iter 0] loss=1.4255 val_loss=0.0000 scale=1.0000 norm=1.4230\n",
      "[iter 100] loss=1.2728 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 0] loss=1.4279 val_loss=0.0000 scale=1.0000 norm=1.4184\n",
      "[iter 100] loss=1.2737 val_loss=0.0000 scale=1.0000 norm=1.4119\n",
      "[iter 0] loss=1.4278 val_loss=0.0000 scale=1.0000 norm=1.4197\n",
      "[iter 100] loss=1.2657 val_loss=0.0000 scale=1.0000 norm=1.4128\n",
      "[iter 0] loss=1.4316 val_loss=0.0000 scale=1.0000 norm=1.4135\n",
      "[iter 100] loss=1.2680 val_loss=0.0000 scale=1.0000 norm=1.4055\n",
      "[iter 0] loss=1.4306 val_loss=0.0000 scale=1.0000 norm=1.4158\n",
      "[iter 100] loss=1.2658 val_loss=0.0000 scale=1.0000 norm=1.4088\n",
      "[iter 0] loss=1.4317 val_loss=0.0000 scale=1.0000 norm=1.4138\n",
      "[iter 100] loss=1.2704 val_loss=0.0000 scale=1.0000 norm=1.4043\n",
      "[iter 0] loss=1.4322 val_loss=0.0000 scale=1.0000 norm=1.4141\n",
      "[iter 100] loss=1.2681 val_loss=0.0000 scale=1.0000 norm=1.4035\n",
      "[iter 0] loss=1.4311 val_loss=0.0000 scale=1.0000 norm=1.4159\n",
      "[iter 100] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=1.4069\n",
      "[iter 0] loss=1.4297 val_loss=0.0000 scale=1.0000 norm=1.4175\n",
      "[iter 100] loss=1.2656 val_loss=0.0000 scale=1.0000 norm=1.4103\n",
      "[iter 0] loss=1.4304 val_loss=0.0000 scale=1.0000 norm=1.4155\n",
      "[iter 100] loss=1.2684 val_loss=0.0000 scale=1.0000 norm=1.4093\n",
      "[iter 0] loss=1.4228 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 100] loss=1.2549 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 0] loss=1.4224 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 100] loss=1.2538 val_loss=0.0000 scale=1.0000 norm=1.4222\n",
      "[iter 0] loss=1.4201 val_loss=0.0000 scale=1.0000 norm=1.4309\n",
      "[iter 100] loss=1.2471 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 0] loss=1.4192 val_loss=0.0000 scale=1.0000 norm=1.4325\n",
      "[iter 100] loss=1.2483 val_loss=0.0000 scale=1.0000 norm=1.4297\n",
      "[iter 0] loss=1.4219 val_loss=0.0000 scale=1.0000 norm=1.4279\n",
      "[iter 100] loss=1.2512 val_loss=0.0000 scale=1.0000 norm=1.4241\n",
      "[iter 0] loss=1.4227 val_loss=0.0000 scale=1.0000 norm=1.4263\n",
      "[iter 100] loss=1.2513 val_loss=0.0000 scale=1.0000 norm=1.4237\n",
      "[iter 0] loss=1.4234 val_loss=0.0000 scale=1.0000 norm=1.4256\n",
      "[iter 100] loss=1.2493 val_loss=0.0000 scale=1.0000 norm=1.4222\n",
      "[iter 0] loss=1.4239 val_loss=0.0000 scale=1.0000 norm=1.4251\n",
      "[iter 100] loss=1.2467 val_loss=0.0000 scale=1.0000 norm=1.4213\n",
      "[iter 0] loss=1.4234 val_loss=0.0000 scale=1.0000 norm=1.4253\n",
      "[iter 100] loss=1.2410 val_loss=0.0000 scale=1.0000 norm=1.4228\n",
      "[iter 0] loss=1.4253 val_loss=0.0000 scale=1.0000 norm=1.4215\n",
      "[iter 100] loss=1.2450 val_loss=0.0000 scale=1.0000 norm=1.4208\n",
      "[iter 0] loss=1.4244 val_loss=0.0000 scale=1.0000 norm=1.4237\n",
      "[iter 100] loss=1.2443 val_loss=0.0000 scale=1.0000 norm=1.4209\n",
      "[iter 0] loss=1.4246 val_loss=0.0000 scale=1.0000 norm=1.4235\n",
      "[iter 100] loss=1.2435 val_loss=0.0000 scale=1.0000 norm=1.4211\n",
      "[iter 0] loss=1.4260 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 100] loss=1.2489 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 0] loss=1.4277 val_loss=0.0000 scale=1.0000 norm=1.4190\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.4153\n",
      "[iter 0] loss=1.4261 val_loss=0.0000 scale=1.0000 norm=1.4214\n",
      "[iter 100] loss=1.2508 val_loss=0.0000 scale=1.0000 norm=1.4169\n",
      "[iter 0] loss=1.4269 val_loss=0.0000 scale=1.0000 norm=1.4193\n",
      "[iter 100] loss=1.2524 val_loss=0.0000 scale=1.0000 norm=1.4149\n",
      "[iter 0] loss=1.4283 val_loss=0.0000 scale=1.0000 norm=1.4168\n",
      "[iter 100] loss=1.2556 val_loss=0.0000 scale=1.0000 norm=1.4126\n",
      "[iter 0] loss=1.4295 val_loss=0.0000 scale=1.0000 norm=1.4143\n",
      "[iter 100] loss=1.2567 val_loss=0.0000 scale=1.0000 norm=1.4097\n",
      "[iter 0] loss=1.4300 val_loss=0.0000 scale=1.0000 norm=1.4143\n",
      "[iter 100] loss=1.2537 val_loss=0.0000 scale=1.0000 norm=1.4113\n",
      "[iter 0] loss=1.4280 val_loss=0.0000 scale=1.0000 norm=1.4177\n",
      "[iter 100] loss=1.2509 val_loss=0.0000 scale=1.0000 norm=1.4182\n",
      "[iter 0] loss=1.4291 val_loss=0.0000 scale=1.0000 norm=1.4153\n",
      "[iter 100] loss=1.2518 val_loss=0.0000 scale=1.0000 norm=1.4170\n",
      "[iter 0] loss=1.4316 val_loss=0.0000 scale=1.0000 norm=1.4108\n",
      "[iter 100] loss=1.2580 val_loss=0.0000 scale=1.0000 norm=1.4120\n",
      "[iter 0] loss=1.4333 val_loss=0.0000 scale=1.0000 norm=1.4075\n",
      "[iter 100] loss=1.2601 val_loss=0.0000 scale=1.0000 norm=1.4060\n",
      "[iter 0] loss=1.4326 val_loss=0.0000 scale=1.0000 norm=1.4091\n",
      "[iter 100] loss=1.2601 val_loss=0.0000 scale=1.0000 norm=1.4081\n",
      "[iter 0] loss=1.4329 val_loss=0.0000 scale=1.0000 norm=1.4083\n",
      "[iter 100] loss=1.2634 val_loss=0.0000 scale=1.0000 norm=1.4073\n",
      "[iter 0] loss=1.4349 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 100] loss=1.2679 val_loss=0.0000 scale=1.0000 norm=1.4025\n",
      "[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.4056\n",
      "[iter 100] loss=1.2699 val_loss=0.0000 scale=1.0000 norm=1.4028\n",
      "[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.4056\n",
      "[iter 100] loss=1.2705 val_loss=0.0000 scale=1.0000 norm=1.4008\n",
      "[iter 0] loss=1.4369 val_loss=0.0000 scale=1.0000 norm=1.4019\n",
      "[iter 100] loss=1.2769 val_loss=0.0000 scale=1.0000 norm=1.3982\n",
      "[iter 0] loss=1.4360 val_loss=0.0000 scale=1.0000 norm=1.4042\n",
      "[iter 100] loss=1.2760 val_loss=0.0000 scale=1.0000 norm=1.4018\n",
      "[iter 0] loss=1.4367 val_loss=0.0000 scale=1.0000 norm=1.4038\n",
      "[iter 100] loss=1.2762 val_loss=0.0000 scale=1.0000 norm=1.4000\n",
      "[iter 0] loss=1.4354 val_loss=0.0000 scale=1.0000 norm=1.4068\n",
      "[iter 100] loss=1.2755 val_loss=0.0000 scale=1.0000 norm=1.4038\n",
      "[iter 0] loss=1.4359 val_loss=0.0000 scale=1.0000 norm=1.4066\n",
      "[iter 100] loss=1.2767 val_loss=0.0000 scale=1.0000 norm=1.4035\n",
      "[iter 0] loss=1.4317 val_loss=0.0000 scale=1.0000 norm=1.4136\n",
      "[iter 100] loss=1.2705 val_loss=0.0000 scale=1.0000 norm=1.4128\n",
      "[iter 0] loss=1.4322 val_loss=0.0000 scale=1.0000 norm=1.4121\n",
      "[iter 100] loss=1.2748 val_loss=0.0000 scale=1.0000 norm=1.4102\n",
      "[iter 0] loss=1.4299 val_loss=0.0000 scale=1.0000 norm=1.4154\n",
      "[iter 100] loss=1.2683 val_loss=0.0000 scale=1.0000 norm=1.4140\n",
      "[iter 0] loss=1.4298 val_loss=0.0000 scale=1.0000 norm=1.4155\n",
      "[iter 100] loss=1.2695 val_loss=0.0000 scale=1.0000 norm=1.4151\n",
      "[iter 0] loss=1.4290 val_loss=0.0000 scale=1.0000 norm=1.4175\n",
      "[iter 100] loss=1.2704 val_loss=0.0000 scale=1.0000 norm=1.4195\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4318\n",
      "[iter 100] loss=1.2545 val_loss=0.0000 scale=1.0000 norm=1.4368\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4306\n",
      "[iter 100] loss=1.2550 val_loss=0.0000 scale=1.0000 norm=1.4337\n",
      "[iter 0] loss=1.4184 val_loss=0.0000 scale=1.0000 norm=1.4326\n",
      "[iter 100] loss=1.2534 val_loss=0.0000 scale=0.5000 norm=0.7172\n",
      "[iter 0] loss=1.4164 val_loss=0.0000 scale=1.0000 norm=1.4354\n",
      "[iter 100] loss=1.2499 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 0] loss=1.4090 val_loss=0.0000 scale=1.0000 norm=1.4466\n",
      "[iter 100] loss=1.2354 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 0] loss=1.4091 val_loss=0.0000 scale=1.0000 norm=1.4464\n",
      "[iter 100] loss=1.2353 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 0] loss=1.4043 val_loss=0.0000 scale=1.0000 norm=1.4548\n",
      "[iter 100] loss=1.2247 val_loss=0.0000 scale=0.5000 norm=0.7306\n",
      "[iter 0] loss=1.4025 val_loss=0.0000 scale=1.0000 norm=1.4586\n",
      "[iter 100] loss=1.2197 val_loss=0.0000 scale=1.0000 norm=1.4662\n",
      "[iter 0] loss=1.4030 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 100] loss=1.2209 val_loss=0.0000 scale=1.0000 norm=1.4610\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 100] loss=1.2141 val_loss=0.0000 scale=1.0000 norm=1.4689\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4628\n",
      "[iter 100] loss=1.2152 val_loss=0.0000 scale=1.0000 norm=1.4678\n",
      "[iter 0] loss=1.4006 val_loss=0.0000 scale=1.0000 norm=1.4621\n",
      "[iter 100] loss=1.2138 val_loss=0.0000 scale=1.0000 norm=1.4668\n",
      "[iter 0] loss=1.4011 val_loss=0.0000 scale=1.0000 norm=1.4623\n",
      "[iter 100] loss=1.2150 val_loss=0.0000 scale=1.0000 norm=1.4678\n",
      "[iter 0] loss=1.3919 val_loss=0.0000 scale=1.0000 norm=1.4751\n",
      "[iter 100] loss=1.2044 val_loss=0.0000 scale=1.0000 norm=1.4814\n",
      "[iter 0] loss=1.3913 val_loss=0.0000 scale=1.0000 norm=1.4761\n",
      "[iter 100] loss=1.2029 val_loss=0.0000 scale=1.0000 norm=1.4840\n",
      "[iter 0] loss=1.3912 val_loss=0.0000 scale=1.0000 norm=1.4762\n",
      "[iter 100] loss=1.2067 val_loss=0.0000 scale=1.0000 norm=1.4838\n",
      "[iter 0] loss=1.3923 val_loss=0.0000 scale=1.0000 norm=1.4752\n",
      "[iter 100] loss=1.2053 val_loss=0.0000 scale=1.0000 norm=1.4845\n",
      "[iter 0] loss=1.3937 val_loss=0.0000 scale=1.0000 norm=1.4723\n",
      "[iter 100] loss=1.2077 val_loss=0.0000 scale=1.0000 norm=1.4788\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 100] loss=1.2148 val_loss=0.0000 scale=1.0000 norm=1.4748\n",
      "[iter 0] loss=1.3976 val_loss=0.0000 scale=1.0000 norm=1.4653\n",
      "[iter 100] loss=1.2136 val_loss=0.0000 scale=1.0000 norm=1.4775\n",
      "[iter 0] loss=1.3992 val_loss=0.0000 scale=1.0000 norm=1.4629\n",
      "[iter 100] loss=1.2177 val_loss=0.0000 scale=0.5000 norm=0.7369\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4605\n",
      "[iter 100] loss=1.2226 val_loss=0.0000 scale=1.0000 norm=1.4717\n",
      "[iter 0] loss=1.4017 val_loss=0.0000 scale=1.0000 norm=1.4573\n",
      "[iter 100] loss=1.2250 val_loss=0.0000 scale=0.5000 norm=0.7356\n",
      "[iter 0] loss=1.4037 val_loss=0.0000 scale=1.0000 norm=1.4543\n",
      "[iter 100] loss=1.2281 val_loss=0.0000 scale=1.0000 norm=1.4644\n",
      "[iter 0] loss=1.4055 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.2258 val_loss=0.0000 scale=1.0000 norm=1.4693\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4518\n",
      "[iter 100] loss=1.2349 val_loss=0.0000 scale=1.0000 norm=1.4641\n",
      "[iter 0] loss=1.4068 val_loss=0.0000 scale=1.0000 norm=1.4490\n",
      "[iter 100] loss=1.2390 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 0] loss=1.4088 val_loss=0.0000 scale=1.0000 norm=1.4452\n",
      "[iter 100] loss=1.2420 val_loss=0.0000 scale=1.0000 norm=1.4576\n",
      "[iter 0] loss=1.4108 val_loss=0.0000 scale=1.0000 norm=1.4428\n",
      "[iter 100] loss=1.2437 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=1.0000 norm=1.4425\n",
      "[iter 100] loss=1.2438 val_loss=0.0000 scale=1.0000 norm=1.4497\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4495\n",
      "[iter 100] loss=1.2365 val_loss=0.0000 scale=1.0000 norm=1.4578\n",
      "[iter 0] loss=1.4069 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.2427 val_loss=0.0000 scale=1.0000 norm=1.4596\n",
      "[iter 0] loss=1.4082 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 100] loss=1.2540 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 0] loss=1.4080 val_loss=0.0000 scale=1.0000 norm=1.4477\n",
      "[iter 100] loss=1.2510 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 0] loss=1.4055 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 100] loss=1.2455 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 0] loss=1.4054 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.2397 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4541\n",
      "[iter 100] loss=1.2425 val_loss=0.0000 scale=1.0000 norm=1.4564\n",
      "[iter 0] loss=1.4062 val_loss=0.0000 scale=1.0000 norm=1.4518\n",
      "[iter 100] loss=1.2466 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 0] loss=1.4061 val_loss=0.0000 scale=1.0000 norm=1.4526\n",
      "[iter 100] loss=1.2436 val_loss=0.0000 scale=1.0000 norm=1.4558\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.2432 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.4093 val_loss=0.0000 scale=1.0000 norm=1.4475\n",
      "[iter 100] loss=1.2456 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 0] loss=1.4093 val_loss=0.0000 scale=1.0000 norm=1.4474\n",
      "[iter 100] loss=1.2509 val_loss=0.0000 scale=1.0000 norm=1.4500\n",
      "[iter 0] loss=1.4107 val_loss=0.0000 scale=1.0000 norm=1.4442\n",
      "[iter 100] loss=1.2561 val_loss=0.0000 scale=1.0000 norm=1.4467\n",
      "[iter 0] loss=1.4117 val_loss=0.0000 scale=1.0000 norm=1.4422\n",
      "[iter 100] loss=1.2602 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4441\n",
      "[iter 100] loss=1.2633 val_loss=0.0000 scale=0.5000 norm=0.7235\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 100] loss=1.2644 val_loss=0.0000 scale=1.0000 norm=1.4471\n",
      "[iter 0] loss=1.4128 val_loss=0.0000 scale=1.0000 norm=1.4404\n",
      "[iter 100] loss=1.2682 val_loss=0.0000 scale=1.0000 norm=1.4450\n",
      "[iter 0] loss=1.4115 val_loss=0.0000 scale=1.0000 norm=1.4434\n",
      "[iter 100] loss=1.2639 val_loss=0.0000 scale=1.0000 norm=1.4475\n",
      "[iter 0] loss=1.4121 val_loss=0.0000 scale=1.0000 norm=1.4424\n",
      "[iter 100] loss=1.2609 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 0] loss=1.4134 val_loss=0.0000 scale=1.0000 norm=1.4396\n",
      "[iter 100] loss=1.2660 val_loss=0.0000 scale=1.0000 norm=1.4456\n",
      "[iter 0] loss=1.4136 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 100] loss=1.2693 val_loss=0.0000 scale=1.0000 norm=1.4453\n",
      "[iter 0] loss=1.4138 val_loss=0.0000 scale=1.0000 norm=1.4386\n",
      "[iter 100] loss=1.2724 val_loss=0.0000 scale=1.0000 norm=1.4453\n",
      "[iter 0] loss=1.4158 val_loss=0.0000 scale=1.0000 norm=1.4353\n",
      "[iter 100] loss=1.2738 val_loss=0.0000 scale=1.0000 norm=1.4461\n",
      "[iter 0] loss=1.4160 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 100] loss=1.2782 val_loss=0.0000 scale=1.0000 norm=1.4427\n",
      "[iter 0] loss=1.4146 val_loss=0.0000 scale=1.0000 norm=1.4378\n",
      "[iter 100] loss=1.2741 val_loss=0.0000 scale=1.0000 norm=1.4471\n",
      "[iter 0] loss=1.4131 val_loss=0.0000 scale=1.0000 norm=1.4406\n",
      "[iter 100] loss=1.2699 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.4134 val_loss=0.0000 scale=1.0000 norm=1.4399\n",
      "[iter 100] loss=1.2713 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 0] loss=1.4148 val_loss=0.0000 scale=1.0000 norm=1.4368\n",
      "[iter 100] loss=1.2738 val_loss=0.0000 scale=1.0000 norm=1.4435\n",
      "[iter 0] loss=1.4136 val_loss=0.0000 scale=2.0000 norm=2.8796\n",
      "[iter 100] loss=1.2686 val_loss=0.0000 scale=1.0000 norm=1.4482\n",
      "[iter 0] loss=1.4154 val_loss=0.0000 scale=2.0000 norm=2.8720\n",
      "[iter 100] loss=1.2662 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 0] loss=1.4166 val_loss=0.0000 scale=1.0000 norm=1.4346\n",
      "[iter 100] loss=1.2690 val_loss=0.0000 scale=1.0000 norm=1.4413\n",
      "[iter 0] loss=1.4181 val_loss=0.0000 scale=2.0000 norm=2.8624\n",
      "[iter 100] loss=1.2703 val_loss=0.0000 scale=1.0000 norm=1.4393\n",
      "[iter 0] loss=1.4198 val_loss=0.0000 scale=1.0000 norm=1.4280\n",
      "[iter 100] loss=1.2762 val_loss=0.0000 scale=1.0000 norm=1.4324\n",
      "[iter 0] loss=1.4194 val_loss=0.0000 scale=2.0000 norm=2.8582\n",
      "[iter 100] loss=1.2737 val_loss=0.0000 scale=1.0000 norm=1.4323\n",
      "[iter 0] loss=1.4201 val_loss=0.0000 scale=2.0000 norm=2.8545\n",
      "[iter 100] loss=1.2757 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 0] loss=1.4213 val_loss=0.0000 scale=2.0000 norm=2.8508\n",
      "[iter 100] loss=1.2773 val_loss=0.0000 scale=0.5000 norm=0.7158\n",
      "[iter 0] loss=1.4208 val_loss=0.0000 scale=2.0000 norm=2.8532\n",
      "[iter 100] loss=1.2837 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 0] loss=1.4205 val_loss=0.0000 scale=2.0000 norm=2.8554\n",
      "[iter 100] loss=1.2790 val_loss=0.0000 scale=1.0000 norm=1.4344\n",
      "[iter 0] loss=1.4205 val_loss=0.0000 scale=2.0000 norm=2.8553\n",
      "[iter 100] loss=1.2794 val_loss=0.0000 scale=1.0000 norm=1.4333\n",
      "[iter 0] loss=1.4208 val_loss=0.0000 scale=2.0000 norm=2.8534\n",
      "[iter 100] loss=1.2824 val_loss=0.0000 scale=0.5000 norm=0.7156\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=2.0000 norm=2.8457\n",
      "[iter 100] loss=1.2869 val_loss=0.0000 scale=1.0000 norm=1.4310\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=2.0000 norm=2.8460\n",
      "[iter 100] loss=1.2889 val_loss=0.0000 scale=1.0000 norm=1.4318\n",
      "[iter 0] loss=1.4250 val_loss=0.0000 scale=2.0000 norm=2.8385\n",
      "[iter 100] loss=1.2906 val_loss=0.0000 scale=1.0000 norm=1.4291\n",
      "[iter 0] loss=1.4252 val_loss=0.0000 scale=1.0000 norm=1.4187\n",
      "[iter 100] loss=1.2931 val_loss=0.0000 scale=1.0000 norm=1.4276\n",
      "[iter 0] loss=1.4251 val_loss=0.0000 scale=1.0000 norm=1.4189\n",
      "[iter 100] loss=1.2939 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 0] loss=1.4252 val_loss=0.0000 scale=1.0000 norm=1.4188\n",
      "[iter 100] loss=1.2970 val_loss=0.0000 scale=1.0000 norm=1.4259\n",
      "[iter 0] loss=1.4236 val_loss=0.0000 scale=1.0000 norm=1.4220\n",
      "[iter 100] loss=1.2933 val_loss=0.0000 scale=1.0000 norm=1.4284\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 100] loss=1.2993 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 0] loss=1.4273 val_loss=0.0000 scale=1.0000 norm=1.4161\n",
      "[iter 100] loss=1.3014 val_loss=0.0000 scale=1.0000 norm=1.4257\n",
      "[iter 0] loss=1.4266 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 100] loss=1.2971 val_loss=0.0000 scale=1.0000 norm=1.4285\n",
      "[iter 0] loss=1.4267 val_loss=0.0000 scale=1.0000 norm=1.4179\n",
      "[iter 100] loss=1.2998 val_loss=0.0000 scale=1.0000 norm=1.4290\n",
      "[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.4144\n",
      "[iter 100] loss=1.3043 val_loss=0.0000 scale=1.0000 norm=1.4248\n",
      "[iter 0] loss=1.4275 val_loss=0.0000 scale=1.0000 norm=1.4165\n",
      "[iter 100] loss=1.3002 val_loss=0.0000 scale=1.0000 norm=1.4259\n",
      "[iter 0] loss=1.4274 val_loss=0.0000 scale=1.0000 norm=1.4171\n",
      "[iter 100] loss=1.2973 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 0] loss=1.4286 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 100] loss=1.3012 val_loss=0.0000 scale=1.0000 norm=1.4232\n",
      "[iter 0] loss=1.4295 val_loss=0.0000 scale=1.0000 norm=1.4125\n",
      "[iter 100] loss=1.3055 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 0] loss=1.4296 val_loss=0.0000 scale=1.0000 norm=1.4121\n",
      "[iter 100] loss=1.3077 val_loss=0.0000 scale=1.0000 norm=1.4206\n",
      "[iter 0] loss=1.4309 val_loss=0.0000 scale=1.0000 norm=1.4093\n",
      "[iter 100] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=1.4149\n",
      "[iter 0] loss=1.4312 val_loss=0.0000 scale=1.0000 norm=1.4086\n",
      "[iter 100] loss=1.3145 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 0] loss=1.4330 val_loss=0.0000 scale=1.0000 norm=1.4051\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4081\n",
      "[iter 0] loss=1.4335 val_loss=0.0000 scale=1.0000 norm=1.4036\n",
      "[iter 100] loss=1.3274 val_loss=0.0000 scale=1.0000 norm=1.4068\n",
      "[iter 0] loss=1.4349 val_loss=0.0000 scale=1.0000 norm=1.4008\n",
      "[iter 100] loss=1.3298 val_loss=0.0000 scale=1.0000 norm=1.4024\n",
      "[iter 0] loss=1.4352 val_loss=0.0000 scale=1.0000 norm=1.4000\n",
      "[iter 100] loss=1.3302 val_loss=0.0000 scale=1.0000 norm=1.4011\n",
      "[iter 0] loss=1.4301 val_loss=0.0000 scale=1.0000 norm=1.4072\n",
      "[iter 100] loss=1.3289 val_loss=0.0000 scale=1.0000 norm=1.4084\n",
      "[iter 0] loss=1.4297 val_loss=0.0000 scale=1.0000 norm=1.4082\n",
      "[iter 100] loss=1.3306 val_loss=0.0000 scale=1.0000 norm=1.4082\n",
      "[iter 0] loss=1.4305 val_loss=0.0000 scale=1.0000 norm=1.4069\n",
      "[iter 100] loss=1.3323 val_loss=0.0000 scale=1.0000 norm=1.4054\n",
      "[iter 0] loss=1.4312 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 100] loss=1.3342 val_loss=0.0000 scale=1.0000 norm=1.4031\n",
      "[iter 0] loss=1.4306 val_loss=0.0000 scale=1.0000 norm=1.4063\n",
      "[iter 100] loss=1.3344 val_loss=0.0000 scale=1.0000 norm=1.4032\n",
      "[iter 0] loss=1.4297 val_loss=0.0000 scale=1.0000 norm=1.4074\n",
      "[iter 100] loss=1.3259 val_loss=0.0000 scale=1.0000 norm=1.4023\n",
      "[iter 0] loss=1.4321 val_loss=0.0000 scale=1.0000 norm=1.4030\n",
      "[iter 100] loss=1.3301 val_loss=0.0000 scale=1.0000 norm=1.3982\n",
      "[iter 0] loss=1.4321 val_loss=0.0000 scale=1.0000 norm=1.4031\n",
      "[iter 100] loss=1.3318 val_loss=0.0000 scale=1.0000 norm=1.3986\n",
      "[iter 0] loss=1.4331 val_loss=0.0000 scale=1.0000 norm=1.4020\n",
      "[iter 100] loss=1.3336 val_loss=0.0000 scale=1.0000 norm=1.3969\n",
      "[iter 0] loss=1.4343 val_loss=0.0000 scale=1.0000 norm=1.3993\n",
      "[iter 100] loss=1.3356 val_loss=0.0000 scale=1.0000 norm=1.3951\n",
      "[iter 0] loss=1.4343 val_loss=0.0000 scale=1.0000 norm=1.3993\n",
      "[iter 100] loss=1.3343 val_loss=0.0000 scale=1.0000 norm=1.3952\n",
      "[iter 0] loss=1.4338 val_loss=0.0000 scale=1.0000 norm=1.3998\n",
      "[iter 100] loss=1.3320 val_loss=0.0000 scale=1.0000 norm=1.3948\n",
      "[iter 0] loss=1.4362 val_loss=0.0000 scale=1.0000 norm=1.3958\n",
      "[iter 100] loss=1.3394 val_loss=0.0000 scale=1.0000 norm=1.3898\n",
      "[iter 0] loss=1.4367 val_loss=0.0000 scale=1.0000 norm=1.3951\n",
      "[iter 100] loss=1.3384 val_loss=0.0000 scale=1.0000 norm=1.3886\n",
      "[iter 0] loss=1.4370 val_loss=0.0000 scale=1.0000 norm=1.3954\n",
      "[iter 100] loss=1.3388 val_loss=0.0000 scale=1.0000 norm=1.3891\n",
      "[iter 0] loss=1.4362 val_loss=0.0000 scale=1.0000 norm=1.3976\n",
      "[iter 100] loss=1.3355 val_loss=0.0000 scale=1.0000 norm=1.3929\n",
      "[iter 0] loss=1.4352 val_loss=0.0000 scale=1.0000 norm=1.4002\n",
      "[iter 100] loss=1.3322 val_loss=0.0000 scale=1.0000 norm=1.3958\n",
      "[iter 0] loss=1.4362 val_loss=0.0000 scale=1.0000 norm=1.3981\n",
      "[iter 100] loss=1.3321 val_loss=0.0000 scale=1.0000 norm=1.3934\n",
      "[iter 0] loss=1.4372 val_loss=0.0000 scale=1.0000 norm=1.3977\n",
      "[iter 100] loss=1.3311 val_loss=0.0000 scale=1.0000 norm=1.3942\n",
      "[iter 0] loss=1.4378 val_loss=0.0000 scale=1.0000 norm=1.3961\n",
      "[iter 100] loss=1.3325 val_loss=0.0000 scale=1.0000 norm=1.3926\n",
      "[iter 0] loss=1.4378 val_loss=0.0000 scale=1.0000 norm=1.3963\n",
      "[iter 100] loss=1.3337 val_loss=0.0000 scale=1.0000 norm=1.3925\n",
      "[iter 0] loss=1.4378 val_loss=0.0000 scale=1.0000 norm=1.3961\n",
      "[iter 100] loss=1.3332 val_loss=0.0000 scale=1.0000 norm=1.3923\n",
      "[iter 0] loss=1.4394 val_loss=0.0000 scale=1.0000 norm=1.3936\n",
      "[iter 100] loss=1.3339 val_loss=0.0000 scale=1.0000 norm=1.3903\n",
      "[iter 0] loss=1.4398 val_loss=0.0000 scale=1.0000 norm=1.3939\n",
      "[iter 100] loss=1.3369 val_loss=0.0000 scale=1.0000 norm=1.3891\n",
      "[iter 0] loss=1.4408 val_loss=0.0000 scale=1.0000 norm=1.3929\n",
      "[iter 100] loss=1.3391 val_loss=0.0000 scale=1.0000 norm=1.3900\n",
      "[iter 0] loss=1.4418 val_loss=0.0000 scale=1.0000 norm=1.3903\n",
      "[iter 100] loss=1.3411 val_loss=0.0000 scale=1.0000 norm=1.3882\n",
      "[iter 0] loss=1.4417 val_loss=0.0000 scale=1.0000 norm=1.3909\n",
      "[iter 100] loss=1.3407 val_loss=0.0000 scale=1.0000 norm=1.3890\n",
      "[iter 0] loss=1.4421 val_loss=0.0000 scale=1.0000 norm=1.3901\n",
      "[iter 100] loss=1.3427 val_loss=0.0000 scale=1.0000 norm=1.3875\n",
      "[iter 0] loss=1.4439 val_loss=0.0000 scale=1.0000 norm=1.3870\n",
      "[iter 100] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=1.3855\n",
      "[iter 0] loss=1.4451 val_loss=0.0000 scale=1.0000 norm=1.3839\n",
      "[iter 100] loss=1.3485 val_loss=0.0000 scale=1.0000 norm=1.3826\n",
      "[iter 0] loss=1.4466 val_loss=0.0000 scale=1.0000 norm=1.3808\n",
      "[iter 100] loss=1.3513 val_loss=0.0000 scale=1.0000 norm=1.3797\n",
      "[iter 0] loss=1.4470 val_loss=0.0000 scale=1.0000 norm=1.3798\n",
      "[iter 100] loss=1.3483 val_loss=0.0000 scale=1.0000 norm=1.3822\n",
      "[iter 0] loss=1.4472 val_loss=0.0000 scale=1.0000 norm=1.3795\n",
      "[iter 100] loss=1.3499 val_loss=0.0000 scale=1.0000 norm=1.3821\n",
      "[iter 0] loss=1.4480 val_loss=0.0000 scale=1.0000 norm=1.3788\n",
      "[iter 100] loss=1.3502 val_loss=0.0000 scale=1.0000 norm=1.3805\n",
      "[iter 0] loss=1.4485 val_loss=0.0000 scale=1.0000 norm=1.3777\n",
      "[iter 100] loss=1.3518 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4485 val_loss=0.0000 scale=1.0000 norm=1.3777\n",
      "[iter 100] loss=1.3553 val_loss=0.0000 scale=1.0000 norm=1.3773\n",
      "[iter 0] loss=1.4502 val_loss=0.0000 scale=1.0000 norm=1.3754\n",
      "[iter 100] loss=1.3610 val_loss=0.0000 scale=1.0000 norm=1.3747\n",
      "[iter 0] loss=1.4506 val_loss=0.0000 scale=1.0000 norm=1.3751\n",
      "[iter 100] loss=1.3604 val_loss=0.0000 scale=1.0000 norm=1.3745\n",
      "[iter 0] loss=1.4518 val_loss=0.0000 scale=1.0000 norm=1.3723\n",
      "[iter 100] loss=1.3601 val_loss=0.0000 scale=1.0000 norm=1.3706\n",
      "[iter 0] loss=1.4525 val_loss=0.0000 scale=1.0000 norm=1.3705\n",
      "[iter 100] loss=1.3599 val_loss=0.0000 scale=1.0000 norm=1.3688\n",
      "[iter 0] loss=1.4529 val_loss=0.0000 scale=1.0000 norm=1.3692\n",
      "[iter 100] loss=1.3615 val_loss=0.0000 scale=1.0000 norm=1.3664\n",
      "[iter 0] loss=1.4547 val_loss=0.0000 scale=1.0000 norm=1.3660\n",
      "[iter 100] loss=1.3652 val_loss=0.0000 scale=1.0000 norm=1.3613\n",
      "[iter 0] loss=1.4443 val_loss=0.0000 scale=1.0000 norm=1.3794\n",
      "[iter 100] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=1.3755\n",
      "[iter 0] loss=1.4452 val_loss=0.0000 scale=1.0000 norm=1.3769\n",
      "[iter 100] loss=1.3577 val_loss=0.0000 scale=1.0000 norm=1.3739\n",
      "[iter 0] loss=1.4452 val_loss=0.0000 scale=1.0000 norm=1.3772\n",
      "[iter 100] loss=1.3564 val_loss=0.0000 scale=1.0000 norm=1.3752\n",
      "[iter 0] loss=1.4460 val_loss=0.0000 scale=1.0000 norm=1.3749\n",
      "[iter 100] loss=1.3592 val_loss=0.0000 scale=1.0000 norm=1.3737\n",
      "[iter 0] loss=1.4463 val_loss=0.0000 scale=1.0000 norm=1.3744\n",
      "[iter 100] loss=1.3607 val_loss=0.0000 scale=1.0000 norm=1.3731\n",
      "[iter 0] loss=1.4463 val_loss=0.0000 scale=1.0000 norm=1.3743\n",
      "[iter 100] loss=1.3616 val_loss=0.0000 scale=1.0000 norm=1.3734\n",
      "[iter 0] loss=1.4464 val_loss=0.0000 scale=1.0000 norm=1.3740\n",
      "[iter 100] loss=1.3609 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 0] loss=1.4465 val_loss=0.0000 scale=1.0000 norm=1.3736\n",
      "[iter 100] loss=1.3609 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 0] loss=1.4479 val_loss=0.0000 scale=1.0000 norm=1.3702\n",
      "[iter 100] loss=1.3637 val_loss=0.0000 scale=1.0000 norm=1.3697\n",
      "[iter 0] loss=1.4483 val_loss=0.0000 scale=1.0000 norm=1.3689\n",
      "[iter 100] loss=1.3638 val_loss=0.0000 scale=1.0000 norm=1.3696\n",
      "[iter 0] loss=1.4482 val_loss=0.0000 scale=1.0000 norm=1.3694\n",
      "[iter 100] loss=1.3652 val_loss=0.0000 scale=1.0000 norm=1.3705\n",
      "[iter 0] loss=1.4477 val_loss=0.0000 scale=1.0000 norm=1.3696\n",
      "[iter 100] loss=1.3642 val_loss=0.0000 scale=1.0000 norm=1.3700\n",
      "[iter 0] loss=1.4463 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 100] loss=1.3597 val_loss=0.0000 scale=1.0000 norm=1.3727\n",
      "[iter 0] loss=1.4458 val_loss=0.0000 scale=1.0000 norm=1.3742\n",
      "[iter 100] loss=1.3622 val_loss=0.0000 scale=1.0000 norm=1.3730\n",
      "[iter 0] loss=1.4448 val_loss=0.0000 scale=1.0000 norm=1.3766\n",
      "[iter 100] loss=1.3632 val_loss=0.0000 scale=1.0000 norm=1.3764\n",
      "[iter 0] loss=1.4433 val_loss=0.0000 scale=1.0000 norm=1.3804\n",
      "[iter 100] loss=1.3594 val_loss=0.0000 scale=1.0000 norm=1.3790\n",
      "[iter 0] loss=1.4445 val_loss=0.0000 scale=1.0000 norm=1.3785\n",
      "[iter 100] loss=1.3601 val_loss=0.0000 scale=1.0000 norm=1.3762\n",
      "[iter 0] loss=1.4446 val_loss=0.0000 scale=1.0000 norm=1.3790\n",
      "[iter 100] loss=1.3613 val_loss=0.0000 scale=1.0000 norm=1.3767\n",
      "[iter 0] loss=1.4425 val_loss=0.0000 scale=1.0000 norm=1.3831\n",
      "[iter 100] loss=1.3605 val_loss=0.0000 scale=1.0000 norm=1.3819\n",
      "[iter 0] loss=1.4430 val_loss=0.0000 scale=1.0000 norm=1.3817\n",
      "[iter 100] loss=1.3609 val_loss=0.0000 scale=1.0000 norm=1.3806\n",
      "[iter 0] loss=1.4430 val_loss=0.0000 scale=1.0000 norm=1.3816\n",
      "[iter 100] loss=1.3609 val_loss=0.0000 scale=1.0000 norm=1.3812\n",
      "[iter 0] loss=1.4436 val_loss=0.0000 scale=1.0000 norm=1.3806\n",
      "[iter 100] loss=1.3628 val_loss=0.0000 scale=1.0000 norm=1.3802\n",
      "[iter 0] loss=1.4437 val_loss=0.0000 scale=1.0000 norm=1.3801\n",
      "[iter 100] loss=1.3584 val_loss=0.0000 scale=1.0000 norm=1.3789\n",
      "[iter 0] loss=1.4435 val_loss=0.0000 scale=1.0000 norm=1.3805\n",
      "[iter 100] loss=1.3587 val_loss=0.0000 scale=1.0000 norm=1.3786\n",
      "[iter 0] loss=1.4436 val_loss=0.0000 scale=1.0000 norm=1.3814\n",
      "[iter 100] loss=1.3579 val_loss=0.0000 scale=1.0000 norm=1.3787\n",
      "[iter 0] loss=1.4439 val_loss=0.0000 scale=1.0000 norm=1.3808\n",
      "[iter 100] loss=1.3599 val_loss=0.0000 scale=1.0000 norm=1.3785\n",
      "[iter 0] loss=1.4438 val_loss=0.0000 scale=1.0000 norm=1.3812\n",
      "[iter 100] loss=1.3597 val_loss=0.0000 scale=1.0000 norm=1.3793\n",
      "[iter 0] loss=1.4429 val_loss=0.0000 scale=1.0000 norm=1.3831\n",
      "[iter 100] loss=1.3580 val_loss=0.0000 scale=1.0000 norm=1.3804\n",
      "[iter 0] loss=1.4426 val_loss=0.0000 scale=1.0000 norm=1.3834\n",
      "[iter 100] loss=1.3585 val_loss=0.0000 scale=1.0000 norm=1.3820\n",
      "[iter 0] loss=1.4429 val_loss=0.0000 scale=1.0000 norm=1.3834\n",
      "[iter 100] loss=1.3602 val_loss=0.0000 scale=1.0000 norm=1.3821\n",
      "[iter 0] loss=1.4428 val_loss=0.0000 scale=1.0000 norm=1.3836\n",
      "[iter 100] loss=1.3608 val_loss=0.0000 scale=1.0000 norm=1.3817\n",
      "[iter 0] loss=1.4413 val_loss=0.0000 scale=1.0000 norm=1.3871\n",
      "[iter 100] loss=1.3584 val_loss=0.0000 scale=1.0000 norm=1.3847\n",
      "[iter 0] loss=1.4423 val_loss=0.0000 scale=1.0000 norm=1.3847\n",
      "[iter 100] loss=1.3602 val_loss=0.0000 scale=1.0000 norm=1.3822\n",
      "[iter 0] loss=1.4442 val_loss=0.0000 scale=1.0000 norm=1.3809\n",
      "[iter 100] loss=1.3591 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4442 val_loss=0.0000 scale=1.0000 norm=1.3809\n",
      "[iter 100] loss=1.3595 val_loss=0.0000 scale=1.0000 norm=1.3807\n",
      "[iter 0] loss=1.4443 val_loss=0.0000 scale=1.0000 norm=1.3808\n",
      "[iter 100] loss=1.3595 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4444 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 100] loss=1.3582 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4453 val_loss=0.0000 scale=1.0000 norm=1.3783\n",
      "[iter 100] loss=1.3634 val_loss=0.0000 scale=1.0000 norm=1.3777\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3775\n",
      "[iter 100] loss=1.3683 val_loss=0.0000 scale=1.0000 norm=1.3769\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3776\n",
      "[iter 100] loss=1.3663 val_loss=0.0000 scale=1.0000 norm=1.3773\n",
      "[iter 0] loss=1.4462 val_loss=0.0000 scale=1.0000 norm=1.3758\n",
      "[iter 100] loss=1.3680 val_loss=0.0000 scale=1.0000 norm=1.3748\n",
      "[iter 0] loss=1.4475 val_loss=0.0000 scale=1.0000 norm=1.3722\n",
      "[iter 100] loss=1.3713 val_loss=0.0000 scale=1.0000 norm=1.3718\n",
      "[iter 0] loss=1.4469 val_loss=0.0000 scale=1.0000 norm=1.3740\n",
      "[iter 100] loss=1.3685 val_loss=0.0000 scale=1.0000 norm=1.3730\n",
      "[iter 0] loss=1.4470 val_loss=0.0000 scale=1.0000 norm=1.3737\n",
      "[iter 100] loss=1.3665 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 0] loss=1.4457 val_loss=0.0000 scale=1.0000 norm=1.3765\n",
      "[iter 100] loss=1.3641 val_loss=0.0000 scale=1.0000 norm=1.3754\n",
      "[iter 0] loss=1.4471 val_loss=0.0000 scale=1.0000 norm=1.3736\n",
      "[iter 100] loss=1.3638 val_loss=0.0000 scale=1.0000 norm=1.3724\n",
      "[iter 0] loss=1.4458 val_loss=0.0000 scale=1.0000 norm=1.3759\n",
      "[iter 100] loss=1.3622 val_loss=0.0000 scale=1.0000 norm=1.3748\n",
      "[iter 0] loss=1.4471 val_loss=0.0000 scale=1.0000 norm=1.3734\n",
      "[iter 100] loss=1.3662 val_loss=0.0000 scale=1.0000 norm=1.3719\n",
      "[iter 0] loss=1.4474 val_loss=0.0000 scale=1.0000 norm=1.3722\n",
      "[iter 100] loss=1.3640 val_loss=0.0000 scale=1.0000 norm=1.3708\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3757\n",
      "[iter 100] loss=1.3595 val_loss=0.0000 scale=1.0000 norm=1.3756\n",
      "[iter 0] loss=1.4455 val_loss=0.0000 scale=1.0000 norm=1.3761\n",
      "[iter 100] loss=1.3588 val_loss=0.0000 scale=1.0000 norm=1.3752\n",
      "[iter 0] loss=1.4455 val_loss=0.0000 scale=1.0000 norm=1.3756\n",
      "[iter 100] loss=1.3579 val_loss=0.0000 scale=1.0000 norm=1.3758\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3752\n",
      "[iter 100] loss=1.3583 val_loss=0.0000 scale=1.0000 norm=1.3753\n",
      "[iter 0] loss=1.4465 val_loss=0.0000 scale=1.0000 norm=1.3731\n",
      "[iter 100] loss=1.3571 val_loss=0.0000 scale=1.0000 norm=1.3744\n",
      "[iter 0] loss=1.4461 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 100] loss=1.3609 val_loss=0.0000 scale=1.0000 norm=1.3742\n",
      "[iter 0] loss=1.4452 val_loss=0.0000 scale=1.0000 norm=1.3745\n",
      "[iter 100] loss=1.3611 val_loss=0.0000 scale=1.0000 norm=1.3767\n",
      "[iter 0] loss=1.4462 val_loss=0.0000 scale=1.0000 norm=1.3725\n",
      "[iter 100] loss=1.3596 val_loss=0.0000 scale=1.0000 norm=1.3745\n",
      "[iter 0] loss=1.4462 val_loss=0.0000 scale=1.0000 norm=1.3729\n",
      "[iter 100] loss=1.3629 val_loss=0.0000 scale=1.0000 norm=1.3757\n",
      "[iter 0] loss=1.4446 val_loss=0.0000 scale=1.0000 norm=1.3752\n",
      "[iter 100] loss=1.3610 val_loss=0.0000 scale=1.0000 norm=1.3773\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3729\n",
      "[iter 100] loss=1.3615 val_loss=0.0000 scale=1.0000 norm=1.3767\n",
      "[iter 0] loss=1.4464 val_loss=0.0000 scale=1.0000 norm=1.3709\n",
      "[iter 100] loss=1.3606 val_loss=0.0000 scale=1.0000 norm=1.3742\n",
      "[iter 0] loss=1.4463 val_loss=0.0000 scale=1.0000 norm=1.3717\n",
      "[iter 100] loss=1.3575 val_loss=0.0000 scale=1.0000 norm=1.3744\n",
      "[iter 0] loss=1.4448 val_loss=0.0000 scale=1.0000 norm=1.3751\n",
      "[iter 100] loss=1.3551 val_loss=0.0000 scale=1.0000 norm=1.3776\n",
      "[iter 0] loss=1.4443 val_loss=0.0000 scale=1.0000 norm=1.3753\n",
      "[iter 100] loss=1.3543 val_loss=0.0000 scale=1.0000 norm=1.3790\n",
      "[iter 0] loss=1.4424 val_loss=0.0000 scale=1.0000 norm=1.3787\n",
      "[iter 100] loss=1.3539 val_loss=0.0000 scale=1.0000 norm=1.3807\n",
      "[iter 0] loss=1.4424 val_loss=0.0000 scale=1.0000 norm=1.3785\n",
      "[iter 100] loss=1.3540 val_loss=0.0000 scale=1.0000 norm=1.3810\n",
      "[iter 0] loss=1.4420 val_loss=0.0000 scale=1.0000 norm=1.3796\n",
      "[iter 100] loss=1.3549 val_loss=0.0000 scale=1.0000 norm=1.3818\n",
      "[iter 0] loss=1.4421 val_loss=0.0000 scale=1.0000 norm=1.3797\n",
      "[iter 100] loss=1.3543 val_loss=0.0000 scale=1.0000 norm=1.3812\n",
      "[iter 0] loss=1.4420 val_loss=0.0000 scale=1.0000 norm=1.3797\n",
      "[iter 100] loss=1.3531 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4416 val_loss=0.0000 scale=1.0000 norm=1.3807\n",
      "[iter 100] loss=1.3505 val_loss=0.0000 scale=1.0000 norm=1.3808\n",
      "[iter 0] loss=1.4411 val_loss=0.0000 scale=1.0000 norm=1.3815\n",
      "[iter 100] loss=1.3497 val_loss=0.0000 scale=1.0000 norm=1.3828\n",
      "[iter 0] loss=1.4402 val_loss=0.0000 scale=1.0000 norm=1.3828\n",
      "[iter 100] loss=1.3479 val_loss=0.0000 scale=1.0000 norm=1.3837\n",
      "[iter 0] loss=1.4387 val_loss=0.0000 scale=1.0000 norm=1.3862\n",
      "[iter 100] loss=1.3480 val_loss=0.0000 scale=1.0000 norm=1.3870\n",
      "[iter 0] loss=1.4366 val_loss=0.0000 scale=1.0000 norm=1.3903\n",
      "[iter 100] loss=1.3476 val_loss=0.0000 scale=1.0000 norm=1.3905\n",
      "[iter 0] loss=1.4367 val_loss=0.0000 scale=1.0000 norm=1.3898\n",
      "[iter 100] loss=1.3486 val_loss=0.0000 scale=1.0000 norm=1.3906\n",
      "[iter 0] loss=1.4358 val_loss=0.0000 scale=1.0000 norm=1.3910\n",
      "[iter 100] loss=1.3468 val_loss=0.0000 scale=1.0000 norm=1.3909\n",
      "[iter 0] loss=1.4356 val_loss=0.0000 scale=1.0000 norm=1.3911\n",
      "[iter 100] loss=1.3463 val_loss=0.0000 scale=1.0000 norm=1.3924\n",
      "[iter 0] loss=1.4353 val_loss=0.0000 scale=1.0000 norm=1.3917\n",
      "[iter 100] loss=1.3421 val_loss=0.0000 scale=1.0000 norm=1.3932\n",
      "[iter 0] loss=1.4364 val_loss=0.0000 scale=1.0000 norm=1.3898\n",
      "[iter 100] loss=1.3490 val_loss=0.0000 scale=1.0000 norm=1.3910\n",
      "[iter 0] loss=1.4349 val_loss=0.0000 scale=1.0000 norm=1.3917\n",
      "[iter 100] loss=1.3461 val_loss=0.0000 scale=1.0000 norm=1.3940\n",
      "[iter 0] loss=1.4364 val_loss=0.0000 scale=1.0000 norm=1.3894\n",
      "[iter 100] loss=1.3466 val_loss=0.0000 scale=1.0000 norm=1.3915\n",
      "[iter 0] loss=1.4359 val_loss=0.0000 scale=1.0000 norm=1.3898\n",
      "[iter 100] loss=1.3502 val_loss=0.0000 scale=1.0000 norm=1.3903\n",
      "[iter 0] loss=1.4357 val_loss=0.0000 scale=1.0000 norm=1.3902\n",
      "[iter 100] loss=1.3495 val_loss=0.0000 scale=1.0000 norm=1.3912\n",
      "[iter 0] loss=1.4362 val_loss=0.0000 scale=1.0000 norm=1.3889\n",
      "[iter 100] loss=1.3452 val_loss=0.0000 scale=1.0000 norm=1.3907\n",
      "[iter 0] loss=1.4362 val_loss=0.0000 scale=1.0000 norm=1.3891\n",
      "[iter 100] loss=1.3469 val_loss=0.0000 scale=1.0000 norm=1.3917\n",
      "[iter 0] loss=1.4360 val_loss=0.0000 scale=1.0000 norm=1.3891\n",
      "[iter 100] loss=1.3500 val_loss=0.0000 scale=1.0000 norm=1.3907\n",
      "[iter 0] loss=1.4349 val_loss=0.0000 scale=1.0000 norm=1.3907\n",
      "[iter 100] loss=1.3530 val_loss=0.0000 scale=1.0000 norm=1.3922\n",
      "[iter 0] loss=1.4344 val_loss=0.0000 scale=1.0000 norm=1.3916\n",
      "[iter 100] loss=1.3505 val_loss=0.0000 scale=1.0000 norm=1.3919\n",
      "[iter 0] loss=1.4346 val_loss=0.0000 scale=1.0000 norm=1.3914\n",
      "[iter 100] loss=1.3495 val_loss=0.0000 scale=1.0000 norm=1.3919\n",
      "[iter 0] loss=1.4364 val_loss=0.0000 scale=1.0000 norm=1.3878\n",
      "[iter 100] loss=1.3517 val_loss=0.0000 scale=1.0000 norm=1.3892\n",
      "[iter 0] loss=1.4353 val_loss=0.0000 scale=1.0000 norm=1.3905\n",
      "[iter 100] loss=1.3488 val_loss=0.0000 scale=1.0000 norm=1.3914\n",
      "[iter 0] loss=1.4345 val_loss=0.0000 scale=1.0000 norm=1.3919\n",
      "[iter 100] loss=1.3492 val_loss=0.0000 scale=1.0000 norm=1.3952\n",
      "[iter 0] loss=1.4329 val_loss=0.0000 scale=1.0000 norm=1.3955\n",
      "[iter 100] loss=1.3420 val_loss=0.0000 scale=1.0000 norm=1.3980\n",
      "[iter 0] loss=1.4325 val_loss=0.0000 scale=1.0000 norm=1.3966\n",
      "[iter 100] loss=1.3413 val_loss=0.0000 scale=1.0000 norm=1.3979\n",
      "[iter 0] loss=1.4335 val_loss=0.0000 scale=1.0000 norm=1.3947\n",
      "[iter 100] loss=1.3398 val_loss=0.0000 scale=1.0000 norm=1.3960\n",
      "[iter 0] loss=1.4326 val_loss=0.0000 scale=1.0000 norm=1.3967\n",
      "[iter 100] loss=1.3403 val_loss=0.0000 scale=1.0000 norm=1.3981\n",
      "[iter 0] loss=1.4339 val_loss=0.0000 scale=1.0000 norm=1.3939\n",
      "[iter 100] loss=1.3445 val_loss=0.0000 scale=1.0000 norm=1.3949\n",
      "[iter 0] loss=1.4324 val_loss=0.0000 scale=1.0000 norm=1.3967\n",
      "[iter 100] loss=1.3415 val_loss=0.0000 scale=1.0000 norm=1.3972\n",
      "[iter 0] loss=1.4325 val_loss=0.0000 scale=1.0000 norm=1.3964\n",
      "[iter 100] loss=1.3439 val_loss=0.0000 scale=1.0000 norm=1.3972\n",
      "[iter 0] loss=1.4318 val_loss=0.0000 scale=1.0000 norm=1.3985\n",
      "[iter 100] loss=1.3453 val_loss=0.0000 scale=1.0000 norm=1.3971\n",
      "[iter 0] loss=1.4307 val_loss=0.0000 scale=1.0000 norm=1.4007\n",
      "[iter 100] loss=1.3447 val_loss=0.0000 scale=1.0000 norm=1.4002\n",
      "[iter 0] loss=1.4298 val_loss=0.0000 scale=1.0000 norm=1.4026\n",
      "[iter 100] loss=1.3466 val_loss=0.0000 scale=1.0000 norm=1.4012\n",
      "[iter 0] loss=1.4296 val_loss=0.0000 scale=1.0000 norm=1.4037\n",
      "[iter 100] loss=1.3432 val_loss=0.0000 scale=1.0000 norm=1.4010\n",
      "[iter 0] loss=1.4297 val_loss=0.0000 scale=1.0000 norm=1.4032\n",
      "[iter 100] loss=1.3408 val_loss=0.0000 scale=1.0000 norm=1.4001\n",
      "[iter 0] loss=1.4293 val_loss=0.0000 scale=1.0000 norm=1.4032\n",
      "[iter 100] loss=1.3388 val_loss=0.0000 scale=1.0000 norm=1.4000\n",
      "[iter 0] loss=1.4292 val_loss=0.0000 scale=1.0000 norm=1.4036\n",
      "[iter 100] loss=1.3400 val_loss=0.0000 scale=1.0000 norm=1.4005\n",
      "[iter 0] loss=1.4281 val_loss=0.0000 scale=1.0000 norm=1.4061\n",
      "[iter 100] loss=1.3416 val_loss=0.0000 scale=1.0000 norm=1.4032\n",
      "[iter 0] loss=1.4268 val_loss=0.0000 scale=1.0000 norm=1.4089\n",
      "[iter 100] loss=1.3402 val_loss=0.0000 scale=1.0000 norm=1.4064\n",
      "[iter 0] loss=1.4268 val_loss=0.0000 scale=1.0000 norm=1.4090\n",
      "[iter 100] loss=1.3395 val_loss=0.0000 scale=1.0000 norm=1.4064\n",
      "[iter 0] loss=1.4252 val_loss=0.0000 scale=1.0000 norm=1.4121\n",
      "[iter 100] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=1.4090\n",
      "[iter 0] loss=1.4253 val_loss=0.0000 scale=1.0000 norm=1.4120\n",
      "[iter 100] loss=1.3343 val_loss=0.0000 scale=1.0000 norm=1.4081\n",
      "[iter 0] loss=1.4235 val_loss=0.0000 scale=1.0000 norm=1.4155\n",
      "[iter 100] loss=1.3303 val_loss=0.0000 scale=1.0000 norm=1.4109\n",
      "[iter 0] loss=1.4235 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 100] loss=1.3299 val_loss=0.0000 scale=1.0000 norm=1.4099\n",
      "[iter 0] loss=1.4232 val_loss=0.0000 scale=1.0000 norm=1.4154\n",
      "[iter 100] loss=1.3275 val_loss=0.0000 scale=1.0000 norm=1.4111\n",
      "[iter 0] loss=1.4237 val_loss=0.0000 scale=1.0000 norm=1.4134\n",
      "[iter 100] loss=1.3287 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 0] loss=1.4244 val_loss=0.0000 scale=1.0000 norm=1.4114\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4094\n",
      "[iter 0] loss=1.4221 val_loss=0.0000 scale=1.0000 norm=1.4154\n",
      "[iter 100] loss=1.3229 val_loss=0.0000 scale=1.0000 norm=1.4129\n",
      "[iter 0] loss=1.4240 val_loss=0.0000 scale=1.0000 norm=1.4111\n",
      "[iter 100] loss=1.3271 val_loss=0.0000 scale=1.0000 norm=1.4086\n",
      "[iter 0] loss=1.4241 val_loss=0.0000 scale=1.0000 norm=1.4113\n",
      "[iter 100] loss=1.3247 val_loss=0.0000 scale=1.0000 norm=1.4096\n",
      "[iter 0] loss=1.4243 val_loss=0.0000 scale=1.0000 norm=1.4113\n",
      "[iter 100] loss=1.3311 val_loss=0.0000 scale=1.0000 norm=1.4095\n",
      "[iter 0] loss=1.4250 val_loss=0.0000 scale=1.0000 norm=1.4090\n",
      "[iter 100] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.4067\n",
      "[iter 0] loss=1.4257 val_loss=0.0000 scale=1.0000 norm=1.4076\n",
      "[iter 100] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.4042\n",
      "[iter 0] loss=1.4256 val_loss=0.0000 scale=1.0000 norm=1.4078\n",
      "[iter 100] loss=1.3326 val_loss=0.0000 scale=1.0000 norm=1.4036\n",
      "[iter 0] loss=1.4255 val_loss=0.0000 scale=1.0000 norm=1.4068\n",
      "[iter 100] loss=1.3323 val_loss=0.0000 scale=1.0000 norm=1.4006\n",
      "[iter 0] loss=1.4254 val_loss=0.0000 scale=1.0000 norm=1.4072\n",
      "[iter 100] loss=1.3332 val_loss=0.0000 scale=1.0000 norm=1.4031\n",
      "[iter 0] loss=1.4235 val_loss=0.0000 scale=1.0000 norm=1.4107\n",
      "[iter 100] loss=1.3320 val_loss=0.0000 scale=1.0000 norm=1.4065\n",
      "[iter 0] loss=1.4237 val_loss=0.0000 scale=1.0000 norm=1.4102\n",
      "[iter 100] loss=1.3297 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 0] loss=1.4218 val_loss=0.0000 scale=1.0000 norm=1.4131\n",
      "[iter 100] loss=1.3296 val_loss=0.0000 scale=1.0000 norm=1.4085\n",
      "[iter 0] loss=1.4232 val_loss=0.0000 scale=1.0000 norm=1.4098\n",
      "[iter 100] loss=1.3305 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 0] loss=1.4219 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 100] loss=1.3290 val_loss=0.0000 scale=1.0000 norm=1.4066\n",
      "[iter 0] loss=1.4259 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 100] loss=1.3320 val_loss=0.0000 scale=1.0000 norm=1.4011\n",
      "[iter 0] loss=1.4255 val_loss=0.0000 scale=1.0000 norm=1.4063\n",
      "[iter 100] loss=1.3304 val_loss=0.0000 scale=1.0000 norm=1.4012\n",
      "[iter 0] loss=1.4267 val_loss=0.0000 scale=1.0000 norm=1.4036\n",
      "[iter 100] loss=1.3324 val_loss=0.0000 scale=1.0000 norm=1.3992\n",
      "[iter 0] loss=1.4249 val_loss=0.0000 scale=1.0000 norm=1.4067\n",
      "[iter 100] loss=1.3296 val_loss=0.0000 scale=1.0000 norm=1.4031\n",
      "[iter 0] loss=1.4253 val_loss=0.0000 scale=1.0000 norm=1.4056\n",
      "[iter 100] loss=1.3271 val_loss=0.0000 scale=1.0000 norm=1.4015\n",
      "[iter 0] loss=1.4259 val_loss=0.0000 scale=1.0000 norm=1.4047\n",
      "[iter 100] loss=1.3282 val_loss=0.0000 scale=1.0000 norm=1.4022\n",
      "[iter 0] loss=1.4234 val_loss=0.0000 scale=1.0000 norm=1.4091\n",
      "[iter 100] loss=1.3302 val_loss=0.0000 scale=1.0000 norm=1.4089\n",
      "[iter 0] loss=1.4216 val_loss=0.0000 scale=1.0000 norm=1.4116\n",
      "[iter 100] loss=1.3266 val_loss=0.0000 scale=1.0000 norm=1.4124\n",
      "[iter 0] loss=1.4225 val_loss=0.0000 scale=1.0000 norm=1.4095\n",
      "[iter 100] loss=1.3279 val_loss=0.0000 scale=1.0000 norm=1.4093\n",
      "[iter 0] loss=1.4239 val_loss=0.0000 scale=1.0000 norm=1.4067\n",
      "[iter 100] loss=1.3208 val_loss=0.0000 scale=1.0000 norm=1.4039\n",
      "[iter 0] loss=1.4215 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4091\n",
      "[iter 0] loss=1.4196 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 100] loss=1.3211 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 0] loss=1.4196 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 100] loss=1.3175 val_loss=0.0000 scale=1.0000 norm=1.4119\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4160\n",
      "[iter 100] loss=1.3121 val_loss=0.0000 scale=1.0000 norm=1.4102\n",
      "[iter 0] loss=1.4177 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 100] loss=1.3130 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 0] loss=1.4170 val_loss=0.0000 scale=1.0000 norm=1.4183\n",
      "[iter 100] loss=1.3144 val_loss=0.0000 scale=1.0000 norm=1.4149\n",
      "[iter 0] loss=1.4170 val_loss=0.0000 scale=1.0000 norm=1.4167\n",
      "[iter 100] loss=1.3173 val_loss=0.0000 scale=1.0000 norm=1.4129\n",
      "[iter 0] loss=1.4215 val_loss=0.0000 scale=1.0000 norm=1.4100\n",
      "[iter 100] loss=1.3219 val_loss=0.0000 scale=1.0000 norm=1.4049\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=1.0000 norm=1.4065\n",
      "[iter 100] loss=1.3262 val_loss=0.0000 scale=1.0000 norm=1.4009\n",
      "[iter 0] loss=1.4211 val_loss=0.0000 scale=1.0000 norm=1.4105\n",
      "[iter 100] loss=1.3259 val_loss=0.0000 scale=1.0000 norm=1.4039\n",
      "[iter 0] loss=1.4227 val_loss=0.0000 scale=1.0000 norm=1.4072\n",
      "[iter 100] loss=1.3302 val_loss=0.0000 scale=1.0000 norm=1.4011\n",
      "[iter 0] loss=1.4230 val_loss=0.0000 scale=1.0000 norm=1.4067\n",
      "[iter 100] loss=1.3322 val_loss=0.0000 scale=1.0000 norm=1.4014\n",
      "[iter 0] loss=1.4210 val_loss=0.0000 scale=1.0000 norm=1.4092\n",
      "[iter 100] loss=1.3317 val_loss=0.0000 scale=1.0000 norm=1.4041\n",
      "[iter 0] loss=1.4218 val_loss=0.0000 scale=1.0000 norm=1.4074\n",
      "[iter 100] loss=1.3322 val_loss=0.0000 scale=1.0000 norm=1.4019\n",
      "[iter 0] loss=1.4207 val_loss=0.0000 scale=1.0000 norm=1.4101\n",
      "[iter 100] loss=1.3324 val_loss=0.0000 scale=1.0000 norm=1.4048\n",
      "[iter 0] loss=1.4219 val_loss=0.0000 scale=1.0000 norm=1.4065\n",
      "[iter 100] loss=1.3309 val_loss=0.0000 scale=1.0000 norm=1.4000\n",
      "[iter 0] loss=1.4218 val_loss=0.0000 scale=1.0000 norm=1.4068\n",
      "[iter 100] loss=1.3303 val_loss=0.0000 scale=1.0000 norm=1.4012\n",
      "[iter 0] loss=1.4202 val_loss=0.0000 scale=1.0000 norm=1.4086\n",
      "[iter 100] loss=1.3305 val_loss=0.0000 scale=1.0000 norm=1.4032\n",
      "[iter 0] loss=1.4221 val_loss=0.0000 scale=1.0000 norm=1.4043\n",
      "[iter 100] loss=1.3319 val_loss=0.0000 scale=1.0000 norm=1.4011\n",
      "[iter 0] loss=1.4212 val_loss=0.0000 scale=1.0000 norm=1.4064\n",
      "[iter 100] loss=1.3268 val_loss=0.0000 scale=1.0000 norm=1.4054\n",
      "[iter 0] loss=1.4196 val_loss=0.0000 scale=1.0000 norm=1.4101\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4080\n",
      "[iter 0] loss=1.4204 val_loss=0.0000 scale=1.0000 norm=1.4084\n",
      "[iter 100] loss=1.3254 val_loss=0.0000 scale=1.0000 norm=1.4045\n",
      "[iter 0] loss=1.4202 val_loss=0.0000 scale=1.0000 norm=1.4090\n",
      "[iter 100] loss=1.3220 val_loss=0.0000 scale=1.0000 norm=1.4077\n",
      "[iter 0] loss=1.4180 val_loss=0.0000 scale=1.0000 norm=1.4132\n",
      "[iter 100] loss=1.3177 val_loss=0.0000 scale=1.0000 norm=1.4112\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4108\n",
      "[iter 100] loss=1.3193 val_loss=0.0000 scale=1.0000 norm=1.4101\n",
      "[iter 0] loss=1.4184 val_loss=0.0000 scale=1.0000 norm=1.4114\n",
      "[iter 100] loss=1.3200 val_loss=0.0000 scale=1.0000 norm=1.4109\n",
      "[iter 0] loss=1.4184 val_loss=0.0000 scale=1.0000 norm=1.4108\n",
      "[iter 100] loss=1.3234 val_loss=0.0000 scale=1.0000 norm=1.4095\n",
      "[iter 0] loss=1.4181 val_loss=0.0000 scale=1.0000 norm=1.4116\n",
      "[iter 100] loss=1.3241 val_loss=0.0000 scale=1.0000 norm=1.4093\n",
      "[iter 0] loss=1.4182 val_loss=0.0000 scale=1.0000 norm=1.4110\n",
      "[iter 100] loss=1.3224 val_loss=0.0000 scale=1.0000 norm=1.4094\n",
      "[iter 0] loss=1.4165 val_loss=0.0000 scale=1.0000 norm=1.4132\n",
      "[iter 100] loss=1.3213 val_loss=0.0000 scale=1.0000 norm=1.4110\n",
      "[iter 0] loss=1.4156 val_loss=0.0000 scale=1.0000 norm=1.4152\n",
      "[iter 100] loss=1.3167 val_loss=0.0000 scale=1.0000 norm=1.4142\n",
      "[iter 0] loss=1.4150 val_loss=0.0000 scale=1.0000 norm=1.4150\n",
      "[iter 100] loss=1.3146 val_loss=0.0000 scale=1.0000 norm=1.4120\n",
      "[iter 0] loss=1.4143 val_loss=0.0000 scale=1.0000 norm=1.4169\n",
      "[iter 100] loss=1.3152 val_loss=0.0000 scale=1.0000 norm=1.4113\n",
      "[iter 0] loss=1.4132 val_loss=0.0000 scale=1.0000 norm=1.4194\n",
      "[iter 100] loss=1.3084 val_loss=0.0000 scale=1.0000 norm=1.4169\n",
      "[iter 0] loss=1.4121 val_loss=0.0000 scale=1.0000 norm=1.4215\n",
      "[iter 100] loss=1.3068 val_loss=0.0000 scale=1.0000 norm=1.4186\n",
      "[iter 0] loss=1.4121 val_loss=0.0000 scale=1.0000 norm=1.4217\n",
      "[iter 100] loss=1.3031 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 0] loss=1.4111 val_loss=0.0000 scale=1.0000 norm=1.4233\n",
      "[iter 100] loss=1.3039 val_loss=0.0000 scale=1.0000 norm=1.4191\n",
      "[iter 0] loss=1.4109 val_loss=0.0000 scale=1.0000 norm=1.4220\n",
      "[iter 100] loss=1.3012 val_loss=0.0000 scale=1.0000 norm=1.4173\n",
      "[iter 0] loss=1.4090 val_loss=0.0000 scale=1.0000 norm=1.4260\n",
      "[iter 100] loss=1.2998 val_loss=0.0000 scale=1.0000 norm=1.4215\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4302\n",
      "[iter 100] loss=1.2917 val_loss=0.0000 scale=1.0000 norm=1.4224\n",
      "[iter 0] loss=1.4087 val_loss=0.0000 scale=1.0000 norm=1.4268\n",
      "[iter 100] loss=1.2948 val_loss=0.0000 scale=1.0000 norm=1.4184\n",
      "[iter 0] loss=1.4080 val_loss=0.0000 scale=1.0000 norm=1.4283\n",
      "[iter 100] loss=1.2911 val_loss=0.0000 scale=1.0000 norm=1.4191\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4319\n",
      "[iter 100] loss=1.2942 val_loss=0.0000 scale=1.0000 norm=1.4232\n",
      "[iter 0] loss=1.4074 val_loss=0.0000 scale=1.0000 norm=1.4285\n",
      "[iter 100] loss=1.2968 val_loss=0.0000 scale=1.0000 norm=1.4211\n",
      "[iter 0] loss=1.4077 val_loss=0.0000 scale=1.0000 norm=1.4279\n",
      "[iter 100] loss=1.2995 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=1.4259\n",
      "[iter 100] loss=1.3005 val_loss=0.0000 scale=1.0000 norm=1.4212\n",
      "[iter 0] loss=1.4081 val_loss=0.0000 scale=1.0000 norm=1.4267\n",
      "[iter 100] loss=1.2988 val_loss=0.0000 scale=1.0000 norm=1.4221\n",
      "[iter 0] loss=1.4056 val_loss=0.0000 scale=1.0000 norm=1.4305\n",
      "[iter 100] loss=1.2953 val_loss=0.0000 scale=1.0000 norm=1.4248\n",
      "[iter 0] loss=1.4063 val_loss=0.0000 scale=1.0000 norm=1.4286\n",
      "[iter 100] loss=1.2981 val_loss=0.0000 scale=1.0000 norm=1.4239\n",
      "[iter 0] loss=1.4054 val_loss=0.0000 scale=1.0000 norm=1.4298\n",
      "[iter 100] loss=1.2965 val_loss=0.0000 scale=1.0000 norm=1.4245\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4287\n",
      "[iter 100] loss=1.2948 val_loss=0.0000 scale=1.0000 norm=1.4233\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4270\n",
      "[iter 100] loss=1.2964 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 0] loss=1.4059 val_loss=0.0000 scale=1.0000 norm=1.4289\n",
      "[iter 100] loss=1.2948 val_loss=0.0000 scale=1.0000 norm=1.4226\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4269\n",
      "[iter 100] loss=1.2922 val_loss=0.0000 scale=1.0000 norm=1.4220\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4303\n",
      "[iter 100] loss=1.2874 val_loss=0.0000 scale=1.0000 norm=1.4262\n",
      "[iter 0] loss=1.4047 val_loss=0.0000 scale=1.0000 norm=1.4290\n",
      "[iter 100] loss=1.2858 val_loss=0.0000 scale=1.0000 norm=1.4247\n",
      "[iter 0] loss=1.4067 val_loss=0.0000 scale=1.0000 norm=1.4255\n",
      "[iter 100] loss=1.2897 val_loss=0.0000 scale=1.0000 norm=1.4212\n",
      "[iter 0] loss=1.4074 val_loss=0.0000 scale=1.0000 norm=1.4239\n",
      "[iter 100] loss=1.2915 val_loss=0.0000 scale=1.0000 norm=1.4196\n",
      "[iter 0] loss=1.4058 val_loss=0.0000 scale=1.0000 norm=1.4262\n",
      "[iter 100] loss=1.2889 val_loss=0.0000 scale=1.0000 norm=1.4226\n",
      "[iter 0] loss=1.4114 val_loss=0.0000 scale=1.0000 norm=1.4163\n",
      "[iter 100] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 0] loss=1.4110 val_loss=0.0000 scale=1.0000 norm=1.4170\n",
      "[iter 100] loss=1.2976 val_loss=0.0000 scale=1.0000 norm=1.4153\n",
      "[iter 0] loss=1.4094 val_loss=0.0000 scale=1.0000 norm=1.4203\n",
      "[iter 100] loss=1.2923 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=1.4225\n",
      "[iter 100] loss=1.2920 val_loss=0.0000 scale=1.0000 norm=1.4214\n",
      "[iter 0] loss=1.4086 val_loss=0.0000 scale=1.0000 norm=1.4222\n",
      "[iter 100] loss=1.2914 val_loss=0.0000 scale=1.0000 norm=1.4209\n",
      "[iter 0] loss=1.4079 val_loss=0.0000 scale=1.0000 norm=1.4234\n",
      "[iter 100] loss=1.2923 val_loss=0.0000 scale=1.0000 norm=1.4239\n",
      "[iter 0] loss=1.4077 val_loss=0.0000 scale=1.0000 norm=1.4238\n",
      "[iter 100] loss=1.2917 val_loss=0.0000 scale=1.0000 norm=1.4241\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4281\n",
      "[iter 100] loss=1.2896 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 0] loss=1.4051 val_loss=0.0000 scale=1.0000 norm=1.4280\n",
      "[iter 100] loss=1.2898 val_loss=0.0000 scale=1.0000 norm=1.4277\n",
      "[iter 0] loss=1.4032 val_loss=0.0000 scale=1.0000 norm=1.4312\n",
      "[iter 100] loss=1.2874 val_loss=0.0000 scale=1.0000 norm=1.4313\n",
      "[iter 0] loss=1.4013 val_loss=0.0000 scale=1.0000 norm=1.4350\n",
      "[iter 100] loss=1.2835 val_loss=0.0000 scale=1.0000 norm=1.4343\n",
      "[iter 0] loss=1.4160 val_loss=0.0000 scale=1.0000 norm=1.4141\n",
      "[iter 100] loss=1.2968 val_loss=0.0000 scale=1.0000 norm=1.4152\n",
      "[iter 0] loss=1.4175 val_loss=0.0000 scale=1.0000 norm=1.4115\n",
      "[iter 100] loss=1.2987 val_loss=0.0000 scale=1.0000 norm=1.4118\n",
      "[iter 0] loss=1.4159 val_loss=0.0000 scale=1.0000 norm=1.4148\n",
      "[iter 100] loss=1.2943 val_loss=0.0000 scale=1.0000 norm=1.4159\n",
      "[iter 0] loss=1.4151 val_loss=0.0000 scale=1.0000 norm=1.4167\n",
      "[iter 100] loss=1.2957 val_loss=0.0000 scale=1.0000 norm=1.4165\n",
      "[iter 0] loss=1.4153 val_loss=0.0000 scale=1.0000 norm=1.4163\n",
      "[iter 100] loss=1.2980 val_loss=0.0000 scale=1.0000 norm=1.4141\n",
      "[iter 0] loss=1.4158 val_loss=0.0000 scale=1.0000 norm=1.4161\n",
      "[iter 100] loss=1.2965 val_loss=0.0000 scale=1.0000 norm=1.4138\n",
      "[iter 0] loss=1.4138 val_loss=0.0000 scale=1.0000 norm=1.4194\n",
      "[iter 100] loss=1.2959 val_loss=0.0000 scale=1.0000 norm=1.4172\n",
      "[iter 0] loss=1.4131 val_loss=0.0000 scale=1.0000 norm=1.4207\n",
      "[iter 100] loss=1.2938 val_loss=0.0000 scale=1.0000 norm=1.4187\n",
      "[iter 0] loss=1.4131 val_loss=0.0000 scale=1.0000 norm=1.4204\n",
      "[iter 100] loss=1.2948 val_loss=0.0000 scale=1.0000 norm=1.4200\n",
      "[iter 0] loss=1.4115 val_loss=0.0000 scale=1.0000 norm=1.4236\n",
      "[iter 100] loss=1.2904 val_loss=0.0000 scale=1.0000 norm=1.4239\n",
      "[iter 0] loss=1.4122 val_loss=0.0000 scale=1.0000 norm=1.4218\n",
      "[iter 100] loss=1.2890 val_loss=0.0000 scale=1.0000 norm=1.4235\n",
      "[iter 0] loss=1.4098 val_loss=0.0000 scale=1.0000 norm=1.4253\n",
      "[iter 100] loss=1.2883 val_loss=0.0000 scale=1.0000 norm=1.4285\n",
      "[iter 0] loss=1.4082 val_loss=0.0000 scale=1.0000 norm=1.4275\n",
      "[iter 100] loss=1.2847 val_loss=0.0000 scale=1.0000 norm=1.4319\n",
      "[iter 0] loss=1.4081 val_loss=0.0000 scale=1.0000 norm=1.4274\n",
      "[iter 100] loss=1.2865 val_loss=0.0000 scale=1.0000 norm=1.4317\n",
      "[iter 0] loss=1.4084 val_loss=0.0000 scale=1.0000 norm=1.4252\n",
      "[iter 100] loss=1.2899 val_loss=0.0000 scale=1.0000 norm=1.4288\n",
      "[iter 0] loss=1.4099 val_loss=0.0000 scale=1.0000 norm=1.4221\n",
      "[iter 100] loss=1.2959 val_loss=0.0000 scale=1.0000 norm=1.4240\n",
      "[iter 0] loss=1.4121 val_loss=0.0000 scale=1.0000 norm=1.4179\n",
      "[iter 100] loss=1.3047 val_loss=0.0000 scale=1.0000 norm=1.4207\n",
      "[iter 0] loss=1.4113 val_loss=0.0000 scale=1.0000 norm=1.4190\n",
      "[iter 100] loss=1.3037 val_loss=0.0000 scale=1.0000 norm=1.4229\n",
      "[iter 0] loss=1.4139 val_loss=0.0000 scale=1.0000 norm=1.4138\n",
      "[iter 100] loss=1.3074 val_loss=0.0000 scale=1.0000 norm=1.4207\n",
      "[iter 0] loss=1.4132 val_loss=0.0000 scale=1.0000 norm=1.4151\n",
      "[iter 100] loss=1.3023 val_loss=0.0000 scale=1.0000 norm=1.4233\n",
      "[iter 0] loss=1.4131 val_loss=0.0000 scale=1.0000 norm=1.4152\n",
      "[iter 100] loss=1.3006 val_loss=0.0000 scale=1.0000 norm=1.4249\n",
      "[iter 0] loss=1.4134 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 100] loss=1.3000 val_loss=0.0000 scale=1.0000 norm=1.4244\n",
      "[iter 0] loss=1.4149 val_loss=0.0000 scale=1.0000 norm=1.4105\n",
      "[iter 100] loss=1.3039 val_loss=0.0000 scale=1.0000 norm=1.4200\n",
      "[iter 0] loss=1.4149 val_loss=0.0000 scale=1.0000 norm=1.4100\n",
      "[iter 100] loss=1.3049 val_loss=0.0000 scale=1.0000 norm=1.4179\n",
      "[iter 0] loss=1.4170 val_loss=0.0000 scale=1.0000 norm=1.4054\n",
      "[iter 100] loss=1.3080 val_loss=0.0000 scale=1.0000 norm=1.4134\n",
      "[iter 0] loss=1.4163 val_loss=0.0000 scale=1.0000 norm=1.4066\n",
      "[iter 100] loss=1.3084 val_loss=0.0000 scale=1.0000 norm=1.4148\n",
      "[iter 0] loss=1.4174 val_loss=0.0000 scale=1.0000 norm=1.4053\n",
      "[iter 100] loss=1.3103 val_loss=0.0000 scale=1.0000 norm=1.4134\n",
      "[iter 0] loss=1.4162 val_loss=0.0000 scale=1.0000 norm=1.4078\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4151\n",
      "[iter 0] loss=1.4165 val_loss=0.0000 scale=1.0000 norm=1.4057\n",
      "[iter 100] loss=1.3147 val_loss=0.0000 scale=1.0000 norm=1.4131\n",
      "[iter 0] loss=1.4164 val_loss=0.0000 scale=1.0000 norm=1.4063\n",
      "[iter 100] loss=1.3132 val_loss=0.0000 scale=1.0000 norm=1.4135\n",
      "[iter 0] loss=1.4160 val_loss=0.0000 scale=1.0000 norm=1.4065\n",
      "[iter 100] loss=1.3104 val_loss=0.0000 scale=1.0000 norm=1.4126\n",
      "[iter 0] loss=1.4154 val_loss=0.0000 scale=1.0000 norm=1.4077\n",
      "[iter 100] loss=1.3116 val_loss=0.0000 scale=1.0000 norm=1.4133\n",
      "[iter 0] loss=1.4167 val_loss=0.0000 scale=1.0000 norm=1.4056\n",
      "[iter 100] loss=1.3116 val_loss=0.0000 scale=1.0000 norm=1.4113\n",
      "[iter 0] loss=1.4175 val_loss=0.0000 scale=1.0000 norm=1.4028\n",
      "[iter 100] loss=1.3137 val_loss=0.0000 scale=1.0000 norm=1.4080\n",
      "[iter 0] loss=1.4182 val_loss=0.0000 scale=1.0000 norm=1.4018\n",
      "[iter 100] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=1.4065\n",
      "[iter 0] loss=1.4197 val_loss=0.0000 scale=1.0000 norm=1.4001\n",
      "[iter 100] loss=1.3221 val_loss=0.0000 scale=1.0000 norm=1.4041\n",
      "[iter 0] loss=1.4224 val_loss=0.0000 scale=1.0000 norm=1.3945\n",
      "[iter 100] loss=1.3233 val_loss=0.0000 scale=1.0000 norm=1.3981\n",
      "[iter 0] loss=1.4206 val_loss=0.0000 scale=1.0000 norm=1.3979\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4023\n",
      "[iter 0] loss=1.4185 val_loss=0.0000 scale=1.0000 norm=1.4016\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4053\n",
      "[iter 0] loss=1.4169 val_loss=0.0000 scale=1.0000 norm=1.4050\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4080\n",
      "[iter 0] loss=1.4175 val_loss=0.0000 scale=1.0000 norm=1.4038\n",
      "[iter 100] loss=1.3207 val_loss=0.0000 scale=1.0000 norm=1.4069\n",
      "[iter 0] loss=1.4191 val_loss=0.0000 scale=1.0000 norm=1.4008\n",
      "[iter 100] loss=1.3252 val_loss=0.0000 scale=1.0000 norm=1.4041\n",
      "[iter 0] loss=1.4613 val_loss=0.0000 scale=1.0000 norm=1.3550\n",
      "[iter 100] loss=1.3720 val_loss=0.0000 scale=1.0000 norm=1.3565\n",
      "[iter 0] loss=1.4609 val_loss=0.0000 scale=1.0000 norm=1.3565\n",
      "[iter 100] loss=1.3731 val_loss=0.0000 scale=1.0000 norm=1.3581\n",
      "[iter 0] loss=1.4596 val_loss=0.0000 scale=1.0000 norm=1.3595\n",
      "[iter 100] loss=1.3742 val_loss=0.0000 scale=1.0000 norm=1.3595\n",
      "[iter 0] loss=1.4583 val_loss=0.0000 scale=1.0000 norm=1.3628\n",
      "[iter 100] loss=1.3702 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 0] loss=1.4569 val_loss=0.0000 scale=1.0000 norm=1.3659\n",
      "[iter 100] loss=1.3695 val_loss=0.0000 scale=1.0000 norm=1.3675\n",
      "[iter 0] loss=1.4577 val_loss=0.0000 scale=1.0000 norm=1.3639\n",
      "[iter 100] loss=1.3729 val_loss=0.0000 scale=1.0000 norm=1.3636\n",
      "[iter 0] loss=1.4574 val_loss=0.0000 scale=1.0000 norm=1.3650\n",
      "[iter 100] loss=1.3715 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 0] loss=1.4572 val_loss=0.0000 scale=1.0000 norm=1.3652\n",
      "[iter 100] loss=1.3717 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 0] loss=1.4553 val_loss=0.0000 scale=1.0000 norm=1.3690\n",
      "[iter 100] loss=1.3682 val_loss=0.0000 scale=1.0000 norm=1.3669\n",
      "[iter 0] loss=1.4547 val_loss=0.0000 scale=1.0000 norm=1.3708\n",
      "[iter 100] loss=1.3662 val_loss=0.0000 scale=1.0000 norm=1.3681\n",
      "[iter 0] loss=1.4547 val_loss=0.0000 scale=1.0000 norm=1.3706\n",
      "[iter 100] loss=1.3654 val_loss=0.0000 scale=1.0000 norm=1.3682\n",
      "[iter 0] loss=1.4532 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 100] loss=1.3639 val_loss=0.0000 scale=1.0000 norm=1.3725\n",
      "[iter 0] loss=1.4534 val_loss=0.0000 scale=1.0000 norm=1.3726\n",
      "[iter 100] loss=1.3640 val_loss=0.0000 scale=1.0000 norm=1.3718\n",
      "[iter 0] loss=1.4614 val_loss=0.0000 scale=1.0000 norm=1.3618\n",
      "[iter 100] loss=1.3784 val_loss=0.0000 scale=1.0000 norm=1.3602\n",
      "[iter 0] loss=1.4622 val_loss=0.0000 scale=1.0000 norm=1.3597\n",
      "[iter 100] loss=1.3781 val_loss=0.0000 scale=1.0000 norm=1.3588\n",
      "[iter 0] loss=1.4621 val_loss=0.0000 scale=1.0000 norm=1.3599\n",
      "[iter 100] loss=1.3766 val_loss=0.0000 scale=1.0000 norm=1.3583\n",
      "[iter 0] loss=1.4619 val_loss=0.0000 scale=1.0000 norm=1.3597\n",
      "[iter 100] loss=1.3775 val_loss=0.0000 scale=1.0000 norm=1.3590\n",
      "[iter 0] loss=1.4615 val_loss=0.0000 scale=1.0000 norm=1.3594\n",
      "[iter 100] loss=1.3814 val_loss=0.0000 scale=1.0000 norm=1.3584\n",
      "[iter 0] loss=1.4617 val_loss=0.0000 scale=1.0000 norm=1.3588\n",
      "[iter 100] loss=1.3782 val_loss=0.0000 scale=1.0000 norm=1.3574\n",
      "[iter 0] loss=1.4610 val_loss=0.0000 scale=1.0000 norm=1.3607\n",
      "[iter 100] loss=1.3770 val_loss=0.0000 scale=1.0000 norm=1.3583\n",
      "[iter 0] loss=1.4601 val_loss=0.0000 scale=1.0000 norm=1.3623\n",
      "[iter 100] loss=1.3759 val_loss=0.0000 scale=1.0000 norm=1.3606\n",
      "[iter 0] loss=1.4601 val_loss=0.0000 scale=1.0000 norm=1.3622\n",
      "[iter 100] loss=1.3786 val_loss=0.0000 scale=1.0000 norm=1.3605\n",
      "[iter 0] loss=1.4592 val_loss=0.0000 scale=1.0000 norm=1.3646\n",
      "[iter 100] loss=1.3764 val_loss=0.0000 scale=1.0000 norm=1.3631\n",
      "[iter 0] loss=1.4581 val_loss=0.0000 scale=1.0000 norm=1.3672\n",
      "[iter 100] loss=1.3760 val_loss=0.0000 scale=1.0000 norm=1.3646\n",
      "[iter 0] loss=1.4630 val_loss=0.0000 scale=1.0000 norm=1.3604\n",
      "[iter 100] loss=1.3819 val_loss=0.0000 scale=1.0000 norm=1.3586\n",
      "[iter 0] loss=1.4619 val_loss=0.0000 scale=1.0000 norm=1.3619\n",
      "[iter 100] loss=1.3825 val_loss=0.0000 scale=1.0000 norm=1.3593\n",
      "[iter 0] loss=1.4617 val_loss=0.0000 scale=1.0000 norm=1.3628\n",
      "[iter 100] loss=1.3824 val_loss=0.0000 scale=1.0000 norm=1.3612\n",
      "[iter 0] loss=1.4632 val_loss=0.0000 scale=1.0000 norm=1.3593\n",
      "[iter 100] loss=1.3832 val_loss=0.0000 scale=1.0000 norm=1.3570\n",
      "[iter 0] loss=1.4615 val_loss=0.0000 scale=1.0000 norm=1.3625\n",
      "[iter 100] loss=1.3843 val_loss=0.0000 scale=1.0000 norm=1.3606\n",
      "[iter 0] loss=1.4604 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 100] loss=1.3851 val_loss=0.0000 scale=1.0000 norm=1.3635\n",
      "[iter 0] loss=1.4597 val_loss=0.0000 scale=1.0000 norm=1.3660\n",
      "[iter 100] loss=1.3844 val_loss=0.0000 scale=1.0000 norm=1.3642\n",
      "[iter 0] loss=1.4590 val_loss=0.0000 scale=1.0000 norm=1.3679\n",
      "[iter 100] loss=1.3829 val_loss=0.0000 scale=1.0000 norm=1.3665\n",
      "[iter 0] loss=1.4576 val_loss=0.0000 scale=1.0000 norm=1.3701\n",
      "[iter 100] loss=1.3847 val_loss=0.0000 scale=1.0000 norm=1.3691\n",
      "[iter 0] loss=1.4574 val_loss=0.0000 scale=1.0000 norm=1.3698\n",
      "[iter 100] loss=1.3844 val_loss=0.0000 scale=1.0000 norm=1.3685\n",
      "[iter 0] loss=1.4587 val_loss=0.0000 scale=1.0000 norm=1.3673\n",
      "[iter 100] loss=1.3873 val_loss=0.0000 scale=1.0000 norm=1.3670\n",
      "[iter 0] loss=1.4585 val_loss=0.0000 scale=1.0000 norm=1.3677\n",
      "[iter 100] loss=1.3848 val_loss=0.0000 scale=1.0000 norm=1.3673\n",
      "[iter 0] loss=1.4574 val_loss=0.0000 scale=1.0000 norm=1.3704\n",
      "[iter 100] loss=1.3836 val_loss=0.0000 scale=1.0000 norm=1.3705\n",
      "[iter 0] loss=1.4574 val_loss=0.0000 scale=1.0000 norm=1.3705\n",
      "[iter 100] loss=1.3830 val_loss=0.0000 scale=1.0000 norm=1.3706\n",
      "[iter 0] loss=1.4591 val_loss=0.0000 scale=1.0000 norm=1.3682\n",
      "[iter 100] loss=1.3887 val_loss=0.0000 scale=1.0000 norm=1.3693\n",
      "[iter 0] loss=1.4597 val_loss=0.0000 scale=1.0000 norm=1.3664\n",
      "[iter 100] loss=1.3889 val_loss=0.0000 scale=1.0000 norm=1.3680\n",
      "[iter 0] loss=1.4588 val_loss=0.0000 scale=1.0000 norm=1.3689\n",
      "[iter 100] loss=1.3885 val_loss=0.0000 scale=1.0000 norm=1.3703\n",
      "[iter 0] loss=1.4569 val_loss=0.0000 scale=1.0000 norm=1.3726\n",
      "[iter 100] loss=1.3866 val_loss=0.0000 scale=1.0000 norm=1.3731\n",
      "[iter 0] loss=1.4573 val_loss=0.0000 scale=1.0000 norm=1.3716\n",
      "[iter 100] loss=1.3849 val_loss=0.0000 scale=1.0000 norm=1.3736\n",
      "[iter 0] loss=1.4580 val_loss=0.0000 scale=1.0000 norm=1.3701\n",
      "[iter 100] loss=1.3871 val_loss=0.0000 scale=1.0000 norm=1.3709\n",
      "[iter 0] loss=1.4593 val_loss=0.0000 scale=1.0000 norm=1.3674\n",
      "[iter 100] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.3684\n",
      "[iter 0] loss=1.4600 val_loss=0.0000 scale=1.0000 norm=1.3655\n",
      "[iter 100] loss=1.3871 val_loss=0.0000 scale=1.0000 norm=1.3667\n",
      "[iter 0] loss=1.4586 val_loss=0.0000 scale=1.0000 norm=1.3687\n",
      "[iter 100] loss=1.3858 val_loss=0.0000 scale=1.0000 norm=1.3685\n",
      "[iter 0] loss=1.4582 val_loss=0.0000 scale=1.0000 norm=1.3692\n",
      "[iter 100] loss=1.3882 val_loss=0.0000 scale=1.0000 norm=1.3700\n",
      "[iter 0] loss=1.4564 val_loss=0.0000 scale=1.0000 norm=1.3731\n",
      "[iter 100] loss=1.3866 val_loss=0.0000 scale=1.0000 norm=1.3737\n",
      "[iter 0] loss=1.4569 val_loss=0.0000 scale=1.0000 norm=1.3715\n",
      "[iter 100] loss=1.3856 val_loss=0.0000 scale=1.0000 norm=1.3719\n",
      "[iter 0] loss=1.4570 val_loss=0.0000 scale=1.0000 norm=1.3713\n",
      "[iter 100] loss=1.3857 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 0] loss=1.4563 val_loss=0.0000 scale=1.0000 norm=1.3720\n",
      "[iter 100] loss=1.3856 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 0] loss=1.4563 val_loss=0.0000 scale=1.0000 norm=1.3722\n",
      "[iter 100] loss=1.3845 val_loss=0.0000 scale=1.0000 norm=1.3747\n",
      "[iter 0] loss=1.4554 val_loss=0.0000 scale=1.0000 norm=1.3738\n",
      "[iter 100] loss=1.3830 val_loss=0.0000 scale=1.0000 norm=1.3761\n",
      "[iter 0] loss=1.4556 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 100] loss=1.3849 val_loss=0.0000 scale=1.0000 norm=1.3748\n",
      "[iter 0] loss=1.4556 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 100] loss=1.3847 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 0] loss=1.4545 val_loss=0.0000 scale=1.0000 norm=1.3749\n",
      "[iter 100] loss=1.3811 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 0] loss=1.4526 val_loss=0.0000 scale=1.0000 norm=1.3784\n",
      "[iter 100] loss=1.3787 val_loss=0.0000 scale=1.0000 norm=1.3764\n",
      "[iter 0] loss=1.4526 val_loss=0.0000 scale=1.0000 norm=1.3782\n",
      "[iter 100] loss=1.3776 val_loss=0.0000 scale=1.0000 norm=1.3755\n",
      "[iter 0] loss=1.4507 val_loss=0.0000 scale=1.0000 norm=1.3819\n",
      "[iter 100] loss=1.3766 val_loss=0.0000 scale=1.0000 norm=1.3788\n",
      "[iter 0] loss=1.4493 val_loss=0.0000 scale=1.0000 norm=1.3834\n",
      "[iter 100] loss=1.3747 val_loss=0.0000 scale=1.0000 norm=1.3817\n",
      "[iter 0] loss=1.4475 val_loss=0.0000 scale=1.0000 norm=1.3868\n",
      "[iter 100] loss=1.3744 val_loss=0.0000 scale=1.0000 norm=1.3842\n",
      "[iter 0] loss=1.4492 val_loss=0.0000 scale=1.0000 norm=1.3842\n",
      "[iter 100] loss=1.3762 val_loss=0.0000 scale=1.0000 norm=1.3823\n",
      "[iter 0] loss=1.4490 val_loss=0.0000 scale=1.0000 norm=1.3843\n",
      "[iter 100] loss=1.3782 val_loss=0.0000 scale=1.0000 norm=1.3819\n",
      "[iter 0] loss=1.4497 val_loss=0.0000 scale=1.0000 norm=1.3824\n",
      "[iter 100] loss=1.3822 val_loss=0.0000 scale=1.0000 norm=1.3805\n",
      "[iter 0] loss=1.4506 val_loss=0.0000 scale=1.0000 norm=1.3802\n",
      "[iter 100] loss=1.3828 val_loss=0.0000 scale=1.0000 norm=1.3793\n",
      "[iter 0] loss=1.4503 val_loss=0.0000 scale=1.0000 norm=1.3799\n",
      "[iter 100] loss=1.3807 val_loss=0.0000 scale=1.0000 norm=1.3796\n",
      "[iter 0] loss=1.4498 val_loss=0.0000 scale=1.0000 norm=1.3812\n",
      "[iter 100] loss=1.3804 val_loss=0.0000 scale=1.0000 norm=1.3817\n",
      "[iter 0] loss=1.4505 val_loss=0.0000 scale=1.0000 norm=1.3801\n",
      "[iter 100] loss=1.3794 val_loss=0.0000 scale=1.0000 norm=1.3783\n",
      "[iter 0] loss=1.4503 val_loss=0.0000 scale=1.0000 norm=1.3808\n",
      "[iter 100] loss=1.3784 val_loss=0.0000 scale=1.0000 norm=1.3781\n",
      "[iter 0] loss=1.4504 val_loss=0.0000 scale=1.0000 norm=1.3806\n",
      "[iter 100] loss=1.3771 val_loss=0.0000 scale=1.0000 norm=1.3776\n",
      "[iter 0] loss=1.4484 val_loss=0.0000 scale=1.0000 norm=1.3842\n",
      "[iter 100] loss=1.3772 val_loss=0.0000 scale=1.0000 norm=1.3812\n",
      "[iter 0] loss=1.4485 val_loss=0.0000 scale=1.0000 norm=1.3841\n",
      "[iter 100] loss=1.3795 val_loss=0.0000 scale=1.0000 norm=1.3803\n",
      "[iter 0] loss=1.4486 val_loss=0.0000 scale=1.0000 norm=1.3838\n",
      "[iter 100] loss=1.3794 val_loss=0.0000 scale=1.0000 norm=1.3795\n",
      "[iter 0] loss=1.4489 val_loss=0.0000 scale=1.0000 norm=1.3831\n",
      "[iter 100] loss=1.3809 val_loss=0.0000 scale=1.0000 norm=1.3795\n",
      "[iter 0] loss=1.4478 val_loss=0.0000 scale=1.0000 norm=1.3856\n",
      "[iter 100] loss=1.3792 val_loss=0.0000 scale=1.0000 norm=1.3822\n",
      "[iter 0] loss=1.4483 val_loss=0.0000 scale=1.0000 norm=1.3846\n",
      "[iter 100] loss=1.3799 val_loss=0.0000 scale=1.0000 norm=1.3825\n",
      "[iter 0] loss=1.4489 val_loss=0.0000 scale=1.0000 norm=1.3827\n",
      "[iter 100] loss=1.3812 val_loss=0.0000 scale=1.0000 norm=1.3802\n",
      "[iter 0] loss=1.4497 val_loss=0.0000 scale=1.0000 norm=1.3804\n",
      "[iter 100] loss=1.3812 val_loss=0.0000 scale=1.0000 norm=1.3776\n",
      "[iter 0] loss=1.4512 val_loss=0.0000 scale=1.0000 norm=1.3771\n",
      "[iter 100] loss=1.3825 val_loss=0.0000 scale=1.0000 norm=1.3749\n",
      "[iter 0] loss=1.4524 val_loss=0.0000 scale=1.0000 norm=1.3753\n",
      "[iter 100] loss=1.3828 val_loss=0.0000 scale=1.0000 norm=1.3756\n",
      "[iter 0] loss=1.4533 val_loss=0.0000 scale=1.0000 norm=1.3730\n",
      "[iter 100] loss=1.3855 val_loss=0.0000 scale=1.0000 norm=1.3731\n",
      "[iter 0] loss=1.4533 val_loss=0.0000 scale=1.0000 norm=1.3727\n",
      "[iter 100] loss=1.3846 val_loss=0.0000 scale=1.0000 norm=1.3732\n",
      "[iter 0] loss=1.4526 val_loss=0.0000 scale=1.0000 norm=1.3735\n",
      "[iter 100] loss=1.3875 val_loss=0.0000 scale=1.0000 norm=1.3747\n",
      "[iter 0] loss=1.4525 val_loss=0.0000 scale=1.0000 norm=1.3737\n",
      "[iter 100] loss=1.3850 val_loss=0.0000 scale=1.0000 norm=1.3760\n",
      "[iter 0] loss=1.4527 val_loss=0.0000 scale=1.0000 norm=1.3724\n",
      "[iter 100] loss=1.3853 val_loss=0.0000 scale=1.0000 norm=1.3736\n",
      "[iter 0] loss=1.4516 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 100] loss=1.3815 val_loss=0.0000 scale=1.0000 norm=1.3763\n",
      "[iter 0] loss=1.4516 val_loss=0.0000 scale=1.0000 norm=1.3734\n",
      "[iter 100] loss=1.3798 val_loss=0.0000 scale=1.0000 norm=1.3755\n",
      "[iter 0] loss=1.4499 val_loss=0.0000 scale=1.0000 norm=1.3763\n",
      "[iter 100] loss=1.3775 val_loss=0.0000 scale=1.0000 norm=1.3797\n",
      "[iter 0] loss=1.4503 val_loss=0.0000 scale=1.0000 norm=1.3753\n",
      "[iter 100] loss=1.3765 val_loss=0.0000 scale=1.0000 norm=1.3767\n",
      "[iter 0] loss=1.4508 val_loss=0.0000 scale=1.0000 norm=1.3741\n",
      "[iter 100] loss=1.3788 val_loss=0.0000 scale=1.0000 norm=1.3742\n",
      "[iter 0] loss=1.4505 val_loss=0.0000 scale=1.0000 norm=1.3750\n",
      "[iter 100] loss=1.3790 val_loss=0.0000 scale=1.0000 norm=1.3750\n",
      "[iter 0] loss=1.4521 val_loss=0.0000 scale=1.0000 norm=1.3713\n",
      "[iter 100] loss=1.3800 val_loss=0.0000 scale=1.0000 norm=1.3719\n",
      "[iter 0] loss=1.4519 val_loss=0.0000 scale=1.0000 norm=1.3721\n",
      "[iter 100] loss=1.3809 val_loss=0.0000 scale=1.0000 norm=1.3730\n",
      "[iter 0] loss=1.4505 val_loss=0.0000 scale=1.0000 norm=1.3743\n",
      "[iter 100] loss=1.3753 val_loss=0.0000 scale=1.0000 norm=1.3754\n",
      "[iter 0] loss=1.4512 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 100] loss=1.3799 val_loss=0.0000 scale=1.0000 norm=1.3733\n",
      "[iter 0] loss=1.4495 val_loss=0.0000 scale=1.0000 norm=1.3764\n",
      "[iter 100] loss=1.3762 val_loss=0.0000 scale=1.0000 norm=1.3769\n",
      "[iter 0] loss=1.4508 val_loss=0.0000 scale=1.0000 norm=1.3750\n",
      "[iter 100] loss=1.3763 val_loss=0.0000 scale=1.0000 norm=1.3739\n",
      "[iter 0] loss=1.4514 val_loss=0.0000 scale=1.0000 norm=1.3739\n",
      "[iter 100] loss=1.3792 val_loss=0.0000 scale=1.0000 norm=1.3724\n",
      "[iter 0] loss=1.4510 val_loss=0.0000 scale=1.0000 norm=1.3751\n",
      "[iter 100] loss=1.3780 val_loss=0.0000 scale=1.0000 norm=1.3743\n",
      "[iter 0] loss=1.4507 val_loss=0.0000 scale=1.0000 norm=1.3760\n",
      "[iter 100] loss=1.3795 val_loss=0.0000 scale=1.0000 norm=1.3740\n",
      "[iter 0] loss=1.4526 val_loss=0.0000 scale=1.0000 norm=1.3726\n",
      "[iter 100] loss=1.3805 val_loss=0.0000 scale=1.0000 norm=1.3695\n",
      "[iter 0] loss=1.4539 val_loss=0.0000 scale=1.0000 norm=1.3692\n",
      "[iter 100] loss=1.3840 val_loss=0.0000 scale=1.0000 norm=1.3664\n",
      "[iter 0] loss=1.4539 val_loss=0.0000 scale=1.0000 norm=1.3701\n",
      "[iter 100] loss=1.3895 val_loss=0.0000 scale=1.0000 norm=1.3669\n",
      "[iter 0] loss=1.4538 val_loss=0.0000 scale=1.0000 norm=1.3706\n",
      "[iter 100] loss=1.3877 val_loss=0.0000 scale=1.0000 norm=1.3677\n",
      "[iter 0] loss=1.4538 val_loss=0.0000 scale=1.0000 norm=1.3707\n",
      "[iter 100] loss=1.3876 val_loss=0.0000 scale=1.0000 norm=1.3675\n",
      "[iter 0] loss=1.4549 val_loss=0.0000 scale=1.0000 norm=1.3686\n",
      "[iter 100] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.3667\n",
      "[iter 0] loss=1.4552 val_loss=0.0000 scale=1.0000 norm=1.3676\n",
      "[iter 100] loss=1.3899 val_loss=0.0000 scale=1.0000 norm=1.3643\n",
      "[iter 0] loss=1.4570 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 100] loss=1.3913 val_loss=0.0000 scale=1.0000 norm=1.3622\n",
      "[iter 0] loss=1.4573 val_loss=0.0000 scale=1.0000 norm=1.3634\n",
      "[iter 100] loss=1.3928 val_loss=0.0000 scale=1.0000 norm=1.3616\n",
      "[iter 0] loss=1.4591 val_loss=0.0000 scale=1.0000 norm=1.3597\n",
      "[iter 100] loss=1.3906 val_loss=0.0000 scale=1.0000 norm=1.3583\n",
      "[iter 0] loss=1.4591 val_loss=0.0000 scale=1.0000 norm=1.3597\n",
      "[iter 100] loss=1.3888 val_loss=0.0000 scale=1.0000 norm=1.3577\n",
      "[iter 0] loss=1.4598 val_loss=0.0000 scale=1.0000 norm=1.3578\n",
      "[iter 100] loss=1.3915 val_loss=0.0000 scale=1.0000 norm=1.3566\n",
      "[iter 0] loss=1.4608 val_loss=0.0000 scale=1.0000 norm=1.3559\n",
      "[iter 100] loss=1.3903 val_loss=0.0000 scale=1.0000 norm=1.3561\n",
      "[iter 0] loss=1.4619 val_loss=0.0000 scale=1.0000 norm=1.3542\n",
      "[iter 100] loss=1.3937 val_loss=0.0000 scale=1.0000 norm=1.3529\n",
      "[iter 0] loss=1.4607 val_loss=0.0000 scale=1.0000 norm=1.3573\n",
      "[iter 100] loss=1.3944 val_loss=0.0000 scale=1.0000 norm=1.3536\n",
      "[iter 0] loss=1.4613 val_loss=0.0000 scale=1.0000 norm=1.3557\n",
      "[iter 100] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.3531\n",
      "[iter 0] loss=1.4612 val_loss=0.0000 scale=1.0000 norm=1.3550\n",
      "[iter 100] loss=1.3955 val_loss=0.0000 scale=1.0000 norm=1.3519\n",
      "[iter 0] loss=1.4614 val_loss=0.0000 scale=1.0000 norm=1.3547\n",
      "[iter 100] loss=1.3935 val_loss=0.0000 scale=1.0000 norm=1.3547\n",
      "[iter 0] loss=1.4615 val_loss=0.0000 scale=1.0000 norm=1.3543\n",
      "[iter 100] loss=1.3944 val_loss=0.0000 scale=1.0000 norm=1.3548\n",
      "[iter 0] loss=1.4617 val_loss=0.0000 scale=1.0000 norm=1.3543\n",
      "[iter 100] loss=1.3949 val_loss=0.0000 scale=1.0000 norm=1.3533\n",
      "[iter 0] loss=1.4618 val_loss=0.0000 scale=1.0000 norm=1.3541\n",
      "[iter 100] loss=1.3910 val_loss=0.0000 scale=1.0000 norm=1.3535\n",
      "[iter 0] loss=1.4613 val_loss=0.0000 scale=1.0000 norm=1.3549\n",
      "[iter 100] loss=1.3928 val_loss=0.0000 scale=1.0000 norm=1.3532\n",
      "[iter 0] loss=1.4609 val_loss=0.0000 scale=1.0000 norm=1.3563\n",
      "[iter 100] loss=1.3914 val_loss=0.0000 scale=1.0000 norm=1.3549\n",
      "[iter 0] loss=1.4606 val_loss=0.0000 scale=1.0000 norm=1.3569\n",
      "[iter 100] loss=1.3887 val_loss=0.0000 scale=1.0000 norm=1.3557\n",
      "[iter 0] loss=1.4608 val_loss=0.0000 scale=1.0000 norm=1.3561\n",
      "[iter 100] loss=1.3906 val_loss=0.0000 scale=1.0000 norm=1.3544\n",
      "[iter 0] loss=1.4624 val_loss=0.0000 scale=1.0000 norm=1.3531\n",
      "[iter 100] loss=1.3914 val_loss=0.0000 scale=1.0000 norm=1.3515\n",
      "[iter 0] loss=1.4613 val_loss=0.0000 scale=1.0000 norm=1.3552\n",
      "[iter 100] loss=1.3891 val_loss=0.0000 scale=1.0000 norm=1.3556\n",
      "[iter 0] loss=1.4596 val_loss=0.0000 scale=1.0000 norm=1.3588\n",
      "[iter 100] loss=1.3916 val_loss=0.0000 scale=1.0000 norm=1.3565\n",
      "[iter 0] loss=1.4580 val_loss=0.0000 scale=1.0000 norm=1.3625\n",
      "[iter 100] loss=1.3885 val_loss=0.0000 scale=2.0000 norm=2.7206\n",
      "[iter 0] loss=1.4578 val_loss=0.0000 scale=1.0000 norm=1.3629\n",
      "[iter 100] loss=1.3884 val_loss=0.0000 scale=1.0000 norm=1.3602\n",
      "[iter 0] loss=1.4584 val_loss=0.0000 scale=1.0000 norm=1.3615\n",
      "[iter 100] loss=1.3905 val_loss=0.0000 scale=1.0000 norm=1.3594\n",
      "[iter 0] loss=1.4583 val_loss=0.0000 scale=1.0000 norm=1.3611\n",
      "[iter 100] loss=1.3899 val_loss=0.0000 scale=1.0000 norm=1.3591\n",
      "[iter 0] loss=1.4576 val_loss=0.0000 scale=1.0000 norm=1.3615\n",
      "[iter 100] loss=1.3870 val_loss=0.0000 scale=1.0000 norm=1.3612\n",
      "[iter 0] loss=1.4562 val_loss=0.0000 scale=1.0000 norm=1.3651\n",
      "[iter 100] loss=1.3858 val_loss=0.0000 scale=1.0000 norm=1.3663\n",
      "[iter 0] loss=1.4563 val_loss=0.0000 scale=1.0000 norm=1.3648\n",
      "[iter 100] loss=1.3844 val_loss=0.0000 scale=1.0000 norm=1.3677\n",
      "[iter 0] loss=1.4577 val_loss=0.0000 scale=1.0000 norm=1.3617\n",
      "[iter 100] loss=1.3858 val_loss=0.0000 scale=1.0000 norm=1.3640\n",
      "[iter 0] loss=1.4575 val_loss=0.0000 scale=1.0000 norm=1.3616\n",
      "[iter 100] loss=1.3798 val_loss=0.0000 scale=1.0000 norm=1.3655\n",
      "[iter 0] loss=1.4559 val_loss=0.0000 scale=1.0000 norm=1.3653\n",
      "[iter 100] loss=1.3821 val_loss=0.0000 scale=1.0000 norm=1.3671\n",
      "[iter 0] loss=1.4545 val_loss=0.0000 scale=1.0000 norm=1.3679\n",
      "[iter 100] loss=1.3853 val_loss=0.0000 scale=1.0000 norm=1.3684\n",
      "[iter 0] loss=1.4559 val_loss=0.0000 scale=1.0000 norm=1.3654\n",
      "[iter 100] loss=1.3884 val_loss=0.0000 scale=2.0000 norm=2.7333\n",
      "[iter 0] loss=1.4559 val_loss=0.0000 scale=1.0000 norm=1.3656\n",
      "[iter 100] loss=1.3868 val_loss=0.0000 scale=1.0000 norm=1.3664\n",
      "[iter 0] loss=1.4552 val_loss=0.0000 scale=1.0000 norm=1.3672\n",
      "[iter 100] loss=1.3858 val_loss=0.0000 scale=1.0000 norm=1.3654\n",
      "[iter 0] loss=1.4544 val_loss=0.0000 scale=1.0000 norm=1.3695\n",
      "[iter 100] loss=1.3851 val_loss=0.0000 scale=1.0000 norm=1.3672\n",
      "[iter 0] loss=1.4545 val_loss=0.0000 scale=1.0000 norm=1.3689\n",
      "[iter 100] loss=1.3866 val_loss=0.0000 scale=1.0000 norm=1.3664\n",
      "[iter 0] loss=1.4544 val_loss=0.0000 scale=1.0000 norm=1.3692\n",
      "[iter 100] loss=1.3841 val_loss=0.0000 scale=1.0000 norm=1.3679\n",
      "[iter 0] loss=1.4533 val_loss=0.0000 scale=2.0000 norm=2.7436\n",
      "[iter 100] loss=1.3832 val_loss=0.0000 scale=1.0000 norm=1.3682\n",
      "[iter 0] loss=1.4534 val_loss=0.0000 scale=2.0000 norm=2.7429\n",
      "[iter 100] loss=1.3820 val_loss=0.0000 scale=1.0000 norm=1.3675\n",
      "[iter 0] loss=1.4529 val_loss=0.0000 scale=1.0000 norm=1.3718\n",
      "[iter 100] loss=1.3812 val_loss=0.0000 scale=1.0000 norm=1.3665\n",
      "[iter 0] loss=1.4521 val_loss=0.0000 scale=1.0000 norm=1.3737\n",
      "[iter 100] loss=1.3807 val_loss=0.0000 scale=1.0000 norm=1.3674\n",
      "[iter 0] loss=1.4515 val_loss=0.0000 scale=1.0000 norm=1.3751\n",
      "[iter 100] loss=1.3804 val_loss=0.0000 scale=1.0000 norm=1.3696\n",
      "[iter 0] loss=1.4504 val_loss=0.0000 scale=1.0000 norm=1.3772\n",
      "[iter 100] loss=1.3802 val_loss=0.0000 scale=1.0000 norm=1.3715\n",
      "[iter 0] loss=1.4509 val_loss=0.0000 scale=1.0000 norm=1.3763\n",
      "[iter 100] loss=1.3796 val_loss=0.0000 scale=1.0000 norm=1.3705\n",
      "[iter 0] loss=1.4493 val_loss=0.0000 scale=1.0000 norm=1.3789\n",
      "[iter 100] loss=1.3787 val_loss=0.0000 scale=1.0000 norm=1.3735\n",
      "[iter 0] loss=1.4481 val_loss=0.0000 scale=1.0000 norm=1.3818\n",
      "[iter 100] loss=1.3771 val_loss=0.0000 scale=1.0000 norm=1.3759\n",
      "[iter 0] loss=1.4462 val_loss=0.0000 scale=1.0000 norm=1.3856\n",
      "[iter 100] loss=1.3750 val_loss=0.0000 scale=1.0000 norm=1.3799\n",
      "[iter 0] loss=1.4472 val_loss=0.0000 scale=1.0000 norm=1.3832\n",
      "[iter 100] loss=1.3750 val_loss=0.0000 scale=1.0000 norm=1.3782\n",
      "[iter 0] loss=1.4468 val_loss=0.0000 scale=1.0000 norm=1.3843\n",
      "[iter 100] loss=1.3752 val_loss=0.0000 scale=1.0000 norm=1.3788\n",
      "[iter 0] loss=1.4459 val_loss=0.0000 scale=1.0000 norm=1.3852\n",
      "[iter 100] loss=1.3734 val_loss=0.0000 scale=1.0000 norm=1.3809\n",
      "[iter 0] loss=1.4465 val_loss=0.0000 scale=1.0000 norm=1.3832\n",
      "[iter 100] loss=1.3744 val_loss=0.0000 scale=1.0000 norm=1.3785\n",
      "[iter 0] loss=1.4468 val_loss=0.0000 scale=1.0000 norm=1.3825\n",
      "[iter 100] loss=1.3764 val_loss=0.0000 scale=1.0000 norm=1.3782\n",
      "[iter 0] loss=1.4447 val_loss=0.0000 scale=1.0000 norm=1.3864\n",
      "[iter 100] loss=1.3733 val_loss=0.0000 scale=1.0000 norm=1.3819\n",
      "[iter 0] loss=1.4439 val_loss=0.0000 scale=1.0000 norm=1.3884\n",
      "[iter 100] loss=1.3710 val_loss=0.0000 scale=1.0000 norm=1.3834\n",
      "[iter 0] loss=1.4457 val_loss=0.0000 scale=1.0000 norm=1.3848\n",
      "[iter 100] loss=1.3760 val_loss=0.0000 scale=1.0000 norm=1.3789\n",
      "[iter 0] loss=1.4468 val_loss=0.0000 scale=1.0000 norm=1.3837\n",
      "[iter 100] loss=1.3755 val_loss=0.0000 scale=1.0000 norm=1.3787\n",
      "[iter 0] loss=1.4463 val_loss=0.0000 scale=1.0000 norm=1.3847\n",
      "[iter 100] loss=1.3742 val_loss=0.0000 scale=1.0000 norm=1.3800\n",
      "[iter 0] loss=1.4449 val_loss=0.0000 scale=1.0000 norm=1.3878\n",
      "[iter 100] loss=1.3693 val_loss=0.0000 scale=1.0000 norm=1.3827\n",
      "[iter 0] loss=1.4456 val_loss=0.0000 scale=1.0000 norm=1.3858\n",
      "[iter 100] loss=1.3756 val_loss=0.0000 scale=1.0000 norm=1.3802\n",
      "[iter 0] loss=1.4466 val_loss=0.0000 scale=1.0000 norm=1.3850\n",
      "[iter 100] loss=1.3769 val_loss=0.0000 scale=1.0000 norm=1.3794\n",
      "[iter 0] loss=1.4475 val_loss=0.0000 scale=1.0000 norm=1.3840\n",
      "[iter 100] loss=1.3773 val_loss=0.0000 scale=1.0000 norm=1.3789\n",
      "[iter 0] loss=1.4493 val_loss=0.0000 scale=1.0000 norm=1.3804\n",
      "[iter 100] loss=1.3803 val_loss=0.0000 scale=1.0000 norm=1.3763\n",
      "[iter 0] loss=1.4493 val_loss=0.0000 scale=1.0000 norm=1.3804\n",
      "[iter 100] loss=1.3793 val_loss=0.0000 scale=1.0000 norm=1.3741\n",
      "[iter 0] loss=1.4496 val_loss=0.0000 scale=1.0000 norm=1.3795\n",
      "[iter 100] loss=1.3809 val_loss=0.0000 scale=1.0000 norm=1.3725\n",
      "[iter 0] loss=1.4428 val_loss=0.0000 scale=1.0000 norm=1.3891\n",
      "[iter 100] loss=1.3709 val_loss=0.0000 scale=1.0000 norm=1.3822\n",
      "[iter 0] loss=1.4441 val_loss=0.0000 scale=1.0000 norm=1.3864\n",
      "[iter 100] loss=1.3723 val_loss=0.0000 scale=1.0000 norm=1.3791\n",
      "[iter 0] loss=1.4458 val_loss=0.0000 scale=1.0000 norm=1.3828\n",
      "[iter 100] loss=1.3753 val_loss=0.0000 scale=1.0000 norm=1.3755\n",
      "[iter 0] loss=1.4441 val_loss=0.0000 scale=1.0000 norm=1.3866\n",
      "[iter 100] loss=1.3777 val_loss=0.0000 scale=1.0000 norm=1.3793\n",
      "[iter 0] loss=1.4437 val_loss=0.0000 scale=1.0000 norm=1.3872\n",
      "[iter 100] loss=1.3751 val_loss=0.0000 scale=1.0000 norm=1.3793\n",
      "[iter 0] loss=1.4448 val_loss=0.0000 scale=1.0000 norm=1.3851\n",
      "[iter 100] loss=1.3746 val_loss=0.0000 scale=1.0000 norm=1.3772\n",
      "[iter 0] loss=1.4459 val_loss=0.0000 scale=1.0000 norm=1.3843\n",
      "[iter 100] loss=1.3745 val_loss=0.0000 scale=1.0000 norm=1.3770\n",
      "[iter 0] loss=1.4435 val_loss=0.0000 scale=1.0000 norm=1.3880\n",
      "[iter 100] loss=1.3702 val_loss=0.0000 scale=1.0000 norm=1.3820\n",
      "[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.4071\n",
      "[iter 100] loss=1.3530 val_loss=0.0000 scale=1.0000 norm=1.4003\n",
      "[iter 0] loss=1.4287 val_loss=0.0000 scale=1.0000 norm=1.4067\n",
      "[iter 100] loss=1.3538 val_loss=0.0000 scale=1.0000 norm=1.3999\n",
      "[iter 0] loss=1.4293 val_loss=0.0000 scale=1.0000 norm=1.4059\n",
      "[iter 100] loss=1.3550 val_loss=0.0000 scale=1.0000 norm=1.3984\n",
      "[iter 0] loss=1.4258 val_loss=0.0000 scale=1.0000 norm=1.4119\n",
      "[iter 100] loss=1.3488 val_loss=0.0000 scale=1.0000 norm=1.4058\n",
      "[iter 0] loss=1.4242 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 100] loss=1.3480 val_loss=0.0000 scale=1.0000 norm=1.4087\n",
      "[iter 0] loss=1.4242 val_loss=0.0000 scale=1.0000 norm=1.4146\n",
      "[iter 100] loss=1.3448 val_loss=0.0000 scale=1.0000 norm=1.4084\n",
      "[iter 0] loss=1.4254 val_loss=0.0000 scale=1.0000 norm=1.4130\n",
      "[iter 100] loss=1.3424 val_loss=0.0000 scale=1.0000 norm=1.4038\n",
      "[iter 0] loss=1.4250 val_loss=0.0000 scale=1.0000 norm=1.4141\n",
      "[iter 100] loss=1.3406 val_loss=0.0000 scale=1.0000 norm=1.4052\n",
      "[iter 0] loss=1.4241 val_loss=0.0000 scale=1.0000 norm=1.4154\n",
      "[iter 100] loss=1.3402 val_loss=0.0000 scale=1.0000 norm=1.4072\n",
      "[iter 0] loss=1.4219 val_loss=0.0000 scale=1.0000 norm=1.4195\n",
      "[iter 100] loss=1.3377 val_loss=0.0000 scale=1.0000 norm=1.4107\n",
      "[iter 0] loss=1.4214 val_loss=0.0000 scale=1.0000 norm=1.4201\n",
      "[iter 100] loss=1.3395 val_loss=0.0000 scale=1.0000 norm=1.4122\n",
      "[iter 0] loss=1.4218 val_loss=0.0000 scale=1.0000 norm=1.4190\n",
      "[iter 100] loss=1.3371 val_loss=0.0000 scale=1.0000 norm=1.4100\n",
      "[iter 0] loss=1.4211 val_loss=0.0000 scale=1.0000 norm=1.4206\n",
      "[iter 100] loss=1.3360 val_loss=0.0000 scale=1.0000 norm=1.4101\n",
      "[iter 0] loss=1.4228 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 100] loss=1.3393 val_loss=0.0000 scale=1.0000 norm=1.4075\n",
      "[iter 0] loss=1.4229 val_loss=0.0000 scale=1.0000 norm=1.4181\n",
      "[iter 100] loss=1.3370 val_loss=0.0000 scale=1.0000 norm=1.4064\n",
      "[iter 0] loss=1.4214 val_loss=0.0000 scale=1.0000 norm=1.4193\n",
      "[iter 100] loss=1.3335 val_loss=0.0000 scale=1.0000 norm=1.4077\n",
      "[iter 0] loss=1.4204 val_loss=0.0000 scale=1.0000 norm=1.4218\n",
      "[iter 100] loss=1.3327 val_loss=0.0000 scale=1.0000 norm=1.4099\n",
      "[iter 0] loss=1.4213 val_loss=0.0000 scale=1.0000 norm=1.4205\n",
      "[iter 100] loss=1.3370 val_loss=0.0000 scale=1.0000 norm=1.4082\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4238\n",
      "[iter 100] loss=1.3371 val_loss=0.0000 scale=1.0000 norm=1.4116\n",
      "[iter 0] loss=1.4209 val_loss=0.0000 scale=1.0000 norm=1.4208\n",
      "[iter 100] loss=1.3386 val_loss=0.0000 scale=1.0000 norm=1.4115\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4246\n",
      "[iter 100] loss=1.3365 val_loss=0.0000 scale=1.0000 norm=1.4142\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4236\n",
      "[iter 100] loss=1.3378 val_loss=0.0000 scale=1.0000 norm=1.4148\n",
      "[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.4245\n",
      "[iter 100] loss=1.3381 val_loss=0.0000 scale=1.0000 norm=1.4159\n",
      "[iter 0] loss=1.4182 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 100] loss=1.3365 val_loss=0.0000 scale=1.0000 norm=1.4191\n",
      "[iter 0] loss=1.4194 val_loss=0.0000 scale=1.0000 norm=1.4251\n",
      "[iter 100] loss=1.3340 val_loss=0.0000 scale=1.0000 norm=1.4171\n",
      "[iter 0] loss=1.4197 val_loss=0.0000 scale=1.0000 norm=1.4240\n",
      "[iter 100] loss=1.3328 val_loss=0.0000 scale=1.0000 norm=1.4155\n",
      "[iter 0] loss=1.4207 val_loss=0.0000 scale=1.0000 norm=1.4225\n",
      "[iter 100] loss=1.3344 val_loss=0.0000 scale=1.0000 norm=1.4139\n",
      "[iter 0] loss=1.4201 val_loss=0.0000 scale=1.0000 norm=1.4233\n",
      "[iter 100] loss=1.3329 val_loss=0.0000 scale=1.0000 norm=1.4158\n",
      "[iter 0] loss=1.4207 val_loss=0.0000 scale=1.0000 norm=1.4217\n",
      "[iter 100] loss=1.3314 val_loss=0.0000 scale=1.0000 norm=1.4148\n",
      "[iter 0] loss=1.4193 val_loss=0.0000 scale=1.0000 norm=1.4249\n",
      "[iter 100] loss=1.3277 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 0] loss=1.4166 val_loss=0.0000 scale=1.0000 norm=1.4289\n",
      "[iter 100] loss=1.3218 val_loss=0.0000 scale=1.0000 norm=1.4208\n",
      "[iter 0] loss=1.4174 val_loss=0.0000 scale=1.0000 norm=1.4271\n",
      "[iter 100] loss=1.3222 val_loss=0.0000 scale=1.0000 norm=1.4193\n",
      "[iter 0] loss=1.4185 val_loss=0.0000 scale=1.0000 norm=1.4257\n",
      "[iter 100] loss=1.3260 val_loss=0.0000 scale=1.0000 norm=1.4185\n",
      "[iter 0] loss=1.4165 val_loss=0.0000 scale=1.0000 norm=1.4289\n",
      "[iter 100] loss=1.3250 val_loss=0.0000 scale=1.0000 norm=1.4222\n",
      "[iter 0] loss=1.4149 val_loss=0.0000 scale=1.0000 norm=1.4306\n",
      "[iter 100] loss=1.3255 val_loss=0.0000 scale=1.0000 norm=1.4222\n",
      "[iter 0] loss=1.4142 val_loss=0.0000 scale=1.0000 norm=1.4320\n",
      "[iter 100] loss=1.3246 val_loss=0.0000 scale=1.0000 norm=1.4230\n",
      "[iter 0] loss=1.4145 val_loss=0.0000 scale=1.0000 norm=1.4314\n",
      "[iter 100] loss=1.3238 val_loss=0.0000 scale=1.0000 norm=1.4224\n",
      "[iter 0] loss=1.4126 val_loss=0.0000 scale=1.0000 norm=1.4336\n",
      "[iter 100] loss=1.3227 val_loss=0.0000 scale=1.0000 norm=1.4243\n",
      "[iter 0] loss=1.4129 val_loss=0.0000 scale=1.0000 norm=1.4327\n",
      "[iter 100] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=1.4249\n",
      "[iter 0] loss=1.4124 val_loss=0.0000 scale=1.0000 norm=1.4338\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4263\n",
      "[iter 0] loss=1.4143 val_loss=0.0000 scale=1.0000 norm=1.4311\n",
      "[iter 100] loss=1.3254 val_loss=0.0000 scale=1.0000 norm=1.4249\n",
      "[iter 0] loss=1.4143 val_loss=0.0000 scale=1.0000 norm=1.4313\n",
      "[iter 100] loss=1.3244 val_loss=0.0000 scale=1.0000 norm=1.4247\n",
      "[iter 0] loss=1.4124 val_loss=0.0000 scale=1.0000 norm=1.4333\n",
      "[iter 100] loss=1.3224 val_loss=0.0000 scale=1.0000 norm=1.4265\n",
      "[iter 0] loss=1.4126 val_loss=0.0000 scale=1.0000 norm=1.4329\n",
      "[iter 100] loss=1.3220 val_loss=0.0000 scale=1.0000 norm=1.4270\n",
      "[iter 0] loss=1.4149 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 100] loss=1.3256 val_loss=0.0000 scale=1.0000 norm=1.4230\n",
      "[iter 0] loss=1.4145 val_loss=0.0000 scale=1.0000 norm=1.4298\n",
      "[iter 100] loss=1.3255 val_loss=0.0000 scale=1.0000 norm=1.4225\n",
      "[iter 0] loss=1.4147 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 100] loss=1.3234 val_loss=0.0000 scale=1.0000 norm=1.4216\n",
      "[iter 0] loss=1.4148 val_loss=0.0000 scale=1.0000 norm=1.4294\n",
      "[iter 100] loss=1.3243 val_loss=0.0000 scale=1.0000 norm=1.4204\n",
      "[iter 0] loss=1.4153 val_loss=0.0000 scale=1.0000 norm=1.4283\n",
      "[iter 100] loss=1.3249 val_loss=0.0000 scale=1.0000 norm=1.4180\n",
      "[iter 0] loss=1.4145 val_loss=0.0000 scale=1.0000 norm=1.4291\n",
      "[iter 100] loss=1.3239 val_loss=0.0000 scale=1.0000 norm=1.4184\n",
      "[iter 0] loss=1.4153 val_loss=0.0000 scale=1.0000 norm=1.4281\n",
      "[iter 100] loss=1.3279 val_loss=0.0000 scale=1.0000 norm=1.4169\n",
      "[iter 0] loss=1.4137 val_loss=0.0000 scale=1.0000 norm=1.4311\n",
      "[iter 100] loss=1.3225 val_loss=0.0000 scale=1.0000 norm=1.4204\n",
      "[iter 0] loss=1.4137 val_loss=0.0000 scale=1.0000 norm=1.4309\n",
      "[iter 100] loss=1.3232 val_loss=0.0000 scale=1.0000 norm=1.4203\n",
      "[iter 0] loss=1.4115 val_loss=0.0000 scale=1.0000 norm=1.4337\n",
      "[iter 100] loss=1.3272 val_loss=0.0000 scale=1.0000 norm=1.4234\n",
      "[iter 0] loss=1.4095 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3247 val_loss=0.0000 scale=1.0000 norm=1.4265\n",
      "[iter 0] loss=1.4070 val_loss=0.0000 scale=1.0000 norm=1.4400\n",
      "[iter 100] loss=1.3209 val_loss=0.0000 scale=1.0000 norm=1.4302\n",
      "[iter 0] loss=1.4092 val_loss=0.0000 scale=1.0000 norm=1.4367\n",
      "[iter 100] loss=1.3245 val_loss=0.0000 scale=1.0000 norm=1.4267\n",
      "[iter 0] loss=1.4080 val_loss=0.0000 scale=1.0000 norm=1.4396\n",
      "[iter 100] loss=1.3248 val_loss=0.0000 scale=1.0000 norm=1.4282\n",
      "[iter 0] loss=1.4060 val_loss=0.0000 scale=1.0000 norm=1.4420\n",
      "[iter 100] loss=1.3201 val_loss=0.0000 scale=1.0000 norm=1.4289\n",
      "[iter 0] loss=1.4033 val_loss=0.0000 scale=1.0000 norm=1.4457\n",
      "[iter 100] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.4328\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4491\n",
      "[iter 100] loss=1.3105 val_loss=0.0000 scale=1.0000 norm=1.4366\n",
      "[iter 0] loss=1.4003 val_loss=0.0000 scale=1.0000 norm=1.4512\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4392\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 100] loss=1.3092 val_loss=0.0000 scale=1.0000 norm=1.4423\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 100] loss=1.3059 val_loss=0.0000 scale=1.0000 norm=1.4454\n",
      "[iter 0] loss=1.3972 val_loss=0.0000 scale=1.0000 norm=1.4576\n",
      "[iter 100] loss=1.3044 val_loss=0.0000 scale=1.0000 norm=1.4449\n",
      "[iter 0] loss=1.3977 val_loss=0.0000 scale=1.0000 norm=1.4572\n",
      "[iter 100] loss=1.2999 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 0] loss=1.3944 val_loss=0.0000 scale=1.0000 norm=1.4622\n",
      "[iter 100] loss=1.3008 val_loss=0.0000 scale=1.0000 norm=1.4486\n",
      "[iter 0] loss=1.3957 val_loss=0.0000 scale=1.0000 norm=1.4594\n",
      "[iter 100] loss=1.3050 val_loss=0.0000 scale=1.0000 norm=1.4468\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4587\n",
      "[iter 100] loss=1.3053 val_loss=0.0000 scale=1.0000 norm=1.4458\n",
      "[iter 0] loss=1.3932 val_loss=0.0000 scale=1.0000 norm=1.4633\n",
      "[iter 100] loss=1.3015 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 0] loss=1.3917 val_loss=0.0000 scale=1.0000 norm=1.4651\n",
      "[iter 100] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=1.4528\n",
      "[iter 0] loss=1.3915 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 100] loss=1.3015 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.3923 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 100] loss=1.3034 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 100] loss=1.3097 val_loss=0.0000 scale=1.0000 norm=1.4492\n",
      "[iter 0] loss=1.3954 val_loss=0.0000 scale=1.0000 norm=1.4585\n",
      "[iter 100] loss=1.3113 val_loss=0.0000 scale=1.0000 norm=1.4499\n",
      "[iter 0] loss=1.3957 val_loss=0.0000 scale=1.0000 norm=1.4576\n",
      "[iter 100] loss=1.3121 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 0] loss=1.3945 val_loss=0.0000 scale=1.0000 norm=1.4604\n",
      "[iter 100] loss=1.3093 val_loss=0.0000 scale=1.0000 norm=1.4521\n",
      "[iter 0] loss=1.3945 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 100] loss=1.3068 val_loss=0.0000 scale=1.0000 norm=1.4513\n",
      "[iter 0] loss=1.3944 val_loss=0.0000 scale=1.0000 norm=1.4605\n",
      "[iter 100] loss=1.3076 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 0] loss=1.3945 val_loss=0.0000 scale=1.0000 norm=1.4602\n",
      "[iter 100] loss=1.3082 val_loss=0.0000 scale=1.0000 norm=1.4527\n",
      "[iter 0] loss=1.3946 val_loss=0.0000 scale=1.0000 norm=1.4597\n",
      "[iter 100] loss=1.3070 val_loss=0.0000 scale=1.0000 norm=1.4517\n",
      "[iter 0] loss=1.3959 val_loss=0.0000 scale=1.0000 norm=1.4570\n",
      "[iter 100] loss=1.3122 val_loss=0.0000 scale=1.0000 norm=1.4521\n",
      "[iter 0] loss=1.3962 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 100] loss=1.3125 val_loss=0.0000 scale=1.0000 norm=1.4503\n",
      "[iter 0] loss=1.3960 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 100] loss=1.3115 val_loss=0.0000 scale=1.0000 norm=1.4517\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4585\n",
      "[iter 100] loss=1.3097 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 0] loss=1.3942 val_loss=0.0000 scale=1.0000 norm=1.4603\n",
      "[iter 100] loss=1.3058 val_loss=0.0000 scale=1.0000 norm=1.4553\n",
      "[iter 0] loss=1.3946 val_loss=0.0000 scale=1.0000 norm=1.4593\n",
      "[iter 100] loss=1.3069 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 0] loss=1.3932 val_loss=0.0000 scale=1.0000 norm=1.4622\n",
      "[iter 100] loss=1.3029 val_loss=0.0000 scale=1.0000 norm=1.4567\n",
      "[iter 0] loss=1.3940 val_loss=0.0000 scale=1.0000 norm=1.4611\n",
      "[iter 100] loss=1.3072 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 0] loss=1.3947 val_loss=0.0000 scale=1.0000 norm=1.4598\n",
      "[iter 100] loss=1.3090 val_loss=0.0000 scale=1.0000 norm=1.4542\n",
      "[iter 0] loss=1.3960 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 100] loss=1.3088 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 0] loss=1.3974 val_loss=0.0000 scale=1.0000 norm=1.4545\n",
      "[iter 100] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=1.4480\n",
      "[iter 0] loss=1.3951 val_loss=0.0000 scale=1.0000 norm=1.4584\n",
      "[iter 100] loss=1.3070 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 100] loss=1.3091 val_loss=0.0000 scale=1.0000 norm=1.4537\n",
      "[iter 0] loss=1.3964 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.3110 val_loss=0.0000 scale=1.0000 norm=1.4521\n",
      "[iter 0] loss=1.3845 val_loss=0.0000 scale=1.0000 norm=1.4727\n",
      "[iter 100] loss=1.2987 val_loss=0.0000 scale=1.0000 norm=1.4692\n",
      "[iter 0] loss=1.3839 val_loss=0.0000 scale=1.0000 norm=1.4742\n",
      "[iter 100] loss=1.2967 val_loss=0.0000 scale=1.0000 norm=1.4702\n",
      "[iter 0] loss=1.3834 val_loss=0.0000 scale=1.0000 norm=1.4733\n",
      "[iter 100] loss=1.2941 val_loss=0.0000 scale=1.0000 norm=1.4687\n",
      "[iter 0] loss=1.3825 val_loss=0.0000 scale=1.0000 norm=1.4746\n",
      "[iter 100] loss=1.2909 val_loss=0.0000 scale=1.0000 norm=1.4685\n",
      "[iter 0] loss=1.3849 val_loss=0.0000 scale=1.0000 norm=1.4700\n",
      "[iter 100] loss=1.2916 val_loss=0.0000 scale=1.0000 norm=1.4639\n",
      "[iter 0] loss=1.3848 val_loss=0.0000 scale=1.0000 norm=1.4705\n",
      "[iter 100] loss=1.2930 val_loss=0.0000 scale=1.0000 norm=1.4632\n",
      "[iter 0] loss=1.3879 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 100] loss=1.2968 val_loss=0.0000 scale=1.0000 norm=1.4603\n",
      "[iter 0] loss=1.3910 val_loss=0.0000 scale=1.0000 norm=1.4610\n",
      "[iter 100] loss=1.2983 val_loss=0.0000 scale=1.0000 norm=1.4565\n",
      "[iter 0] loss=1.3915 val_loss=0.0000 scale=1.0000 norm=1.4600\n",
      "[iter 100] loss=1.2966 val_loss=0.0000 scale=1.0000 norm=1.4540\n",
      "[iter 0] loss=1.3903 val_loss=0.0000 scale=1.0000 norm=1.4622\n",
      "[iter 100] loss=1.2989 val_loss=0.0000 scale=1.0000 norm=1.4569\n",
      "[iter 0] loss=1.3901 val_loss=0.0000 scale=1.0000 norm=1.4626\n",
      "[iter 100] loss=1.2982 val_loss=0.0000 scale=1.0000 norm=1.4581\n",
      "[iter 0] loss=1.3897 val_loss=0.0000 scale=1.0000 norm=1.4631\n",
      "[iter 100] loss=1.2946 val_loss=0.0000 scale=1.0000 norm=1.4601\n",
      "[iter 0] loss=1.3928 val_loss=0.0000 scale=1.0000 norm=1.4583\n",
      "[iter 100] loss=1.2982 val_loss=0.0000 scale=1.0000 norm=1.4558\n",
      "[iter 0] loss=1.3903 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 100] loss=1.2931 val_loss=0.0000 scale=1.0000 norm=1.4610\n",
      "[iter 0] loss=1.3925 val_loss=0.0000 scale=1.0000 norm=1.4604\n",
      "[iter 100] loss=1.2976 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.3918 val_loss=0.0000 scale=1.0000 norm=1.4623\n",
      "[iter 100] loss=1.2971 val_loss=0.0000 scale=1.0000 norm=1.4623\n",
      "[iter 0] loss=1.3915 val_loss=0.0000 scale=1.0000 norm=1.4629\n",
      "[iter 100] loss=1.2973 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 0] loss=1.3932 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 100] loss=1.2972 val_loss=0.0000 scale=1.0000 norm=1.4630\n",
      "[iter 0] loss=1.3926 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 100] loss=1.2965 val_loss=0.0000 scale=1.0000 norm=1.4634\n",
      "[iter 0] loss=1.3927 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 100] loss=1.2973 val_loss=0.0000 scale=1.0000 norm=1.4636\n",
      "[iter 0] loss=1.3927 val_loss=0.0000 scale=1.0000 norm=1.4615\n",
      "[iter 100] loss=1.2962 val_loss=0.0000 scale=1.0000 norm=1.4643\n",
      "[iter 0] loss=1.3911 val_loss=0.0000 scale=1.0000 norm=1.4638\n",
      "[iter 100] loss=1.2918 val_loss=0.0000 scale=1.0000 norm=1.4660\n",
      "[iter 0] loss=1.3917 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 100] loss=1.2943 val_loss=0.0000 scale=1.0000 norm=1.4657\n",
      "[iter 0] loss=1.3925 val_loss=0.0000 scale=1.0000 norm=1.4606\n",
      "[iter 100] loss=1.2955 val_loss=0.0000 scale=1.0000 norm=1.4633\n",
      "[iter 0] loss=1.3953 val_loss=0.0000 scale=1.0000 norm=1.4569\n",
      "[iter 100] loss=1.2973 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 100] loss=1.2990 val_loss=0.0000 scale=1.0000 norm=1.4553\n",
      "[iter 0] loss=1.3978 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.3026 val_loss=0.0000 scale=1.0000 norm=1.4527\n",
      "[iter 0] loss=1.3971 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.3010 val_loss=0.0000 scale=1.0000 norm=1.4538\n",
      "[iter 0] loss=1.3985 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 100] loss=1.3050 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 0] loss=1.3986 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 100] loss=1.3042 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 0] loss=1.3975 val_loss=0.0000 scale=1.0000 norm=1.4502\n",
      "[iter 100] loss=1.3025 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 0] loss=1.3956 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 100] loss=1.3008 val_loss=0.0000 scale=1.0000 norm=1.4544\n",
      "[iter 0] loss=1.3949 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.2988 val_loss=0.0000 scale=1.0000 norm=1.4568\n",
      "[iter 0] loss=1.3977 val_loss=0.0000 scale=1.0000 norm=1.4503\n",
      "[iter 100] loss=1.3047 val_loss=0.0000 scale=1.0000 norm=1.4510\n",
      "[iter 0] loss=1.3973 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 100] loss=1.3055 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 0] loss=1.3973 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 100] loss=1.3059 val_loss=0.0000 scale=1.0000 norm=1.4531\n",
      "[iter 0] loss=1.3974 val_loss=0.0000 scale=1.0000 norm=1.4511\n",
      "[iter 100] loss=1.3056 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 0] loss=1.3991 val_loss=0.0000 scale=1.0000 norm=1.4478\n",
      "[iter 100] loss=1.3074 val_loss=0.0000 scale=1.0000 norm=1.4493\n",
      "[iter 0] loss=1.3963 val_loss=0.0000 scale=1.0000 norm=1.4516\n",
      "[iter 100] loss=1.3009 val_loss=0.0000 scale=1.0000 norm=1.4549\n",
      "[iter 0] loss=1.3945 val_loss=0.0000 scale=1.0000 norm=1.4552\n",
      "[iter 100] loss=1.2995 val_loss=0.0000 scale=1.0000 norm=1.4585\n",
      "[iter 0] loss=1.3938 val_loss=0.0000 scale=1.0000 norm=1.4571\n",
      "[iter 100] loss=1.2982 val_loss=0.0000 scale=1.0000 norm=1.4607\n",
      "[iter 0] loss=1.3936 val_loss=0.0000 scale=1.0000 norm=1.4574\n",
      "[iter 100] loss=1.2979 val_loss=0.0000 scale=1.0000 norm=1.4613\n",
      "[iter 0] loss=1.3936 val_loss=0.0000 scale=1.0000 norm=1.4592\n",
      "[iter 100] loss=1.2975 val_loss=0.0000 scale=1.0000 norm=1.4624\n",
      "[iter 0] loss=1.3952 val_loss=0.0000 scale=1.0000 norm=1.4559\n",
      "[iter 100] loss=1.2977 val_loss=0.0000 scale=1.0000 norm=1.4592\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4524\n",
      "[iter 100] loss=1.3007 val_loss=0.0000 scale=1.0000 norm=1.4528\n",
      "[iter 0] loss=1.3965 val_loss=0.0000 scale=1.0000 norm=1.4524\n",
      "[iter 100] loss=1.3014 val_loss=0.0000 scale=1.0000 norm=1.4523\n",
      "[iter 0] loss=1.3978 val_loss=0.0000 scale=1.0000 norm=1.4520\n",
      "[iter 100] loss=1.3040 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 0] loss=1.4000 val_loss=0.0000 scale=1.0000 norm=1.4487\n",
      "[iter 100] loss=1.3046 val_loss=0.0000 scale=1.0000 norm=1.4465\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4479\n",
      "[iter 100] loss=1.3057 val_loss=0.0000 scale=1.0000 norm=1.4444\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4445\n",
      "[iter 100] loss=1.3056 val_loss=0.0000 scale=1.0000 norm=1.4419\n",
      "[iter 0] loss=1.4046 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 100] loss=1.3052 val_loss=0.0000 scale=1.0000 norm=1.4393\n",
      "[iter 0] loss=1.4062 val_loss=0.0000 scale=1.0000 norm=1.4423\n",
      "[iter 100] loss=1.3065 val_loss=0.0000 scale=1.0000 norm=1.4397\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4457\n",
      "[iter 100] loss=1.3046 val_loss=0.0000 scale=1.0000 norm=1.4436\n",
      "[iter 0] loss=1.3987 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 100] loss=1.2950 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 0] loss=1.3992 val_loss=0.0000 scale=1.0000 norm=1.4538\n",
      "[iter 100] loss=1.2941 val_loss=0.0000 scale=1.0000 norm=1.4561\n",
      "[iter 0] loss=1.3976 val_loss=0.0000 scale=1.0000 norm=1.4568\n",
      "[iter 100] loss=1.2934 val_loss=0.0000 scale=1.0000 norm=1.4565\n",
      "[iter 0] loss=1.4005 val_loss=0.0000 scale=1.0000 norm=1.4527\n",
      "[iter 100] loss=1.2968 val_loss=0.0000 scale=1.0000 norm=1.4535\n",
      "[iter 0] loss=1.3990 val_loss=0.0000 scale=1.0000 norm=1.4563\n",
      "[iter 100] loss=1.2902 val_loss=0.0000 scale=1.0000 norm=1.4588\n",
      "[iter 0] loss=1.3994 val_loss=0.0000 scale=1.0000 norm=1.4554\n",
      "[iter 100] loss=1.2908 val_loss=0.0000 scale=1.0000 norm=1.4598\n",
      "[iter 0] loss=1.4017 val_loss=0.0000 scale=1.0000 norm=1.4518\n",
      "[iter 100] loss=1.2885 val_loss=0.0000 scale=1.0000 norm=1.4556\n",
      "[iter 0] loss=1.4016 val_loss=0.0000 scale=1.0000 norm=1.4522\n",
      "[iter 100] loss=1.2847 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 0] loss=1.4014 val_loss=0.0000 scale=1.0000 norm=1.4528\n",
      "[iter 100] loss=1.2851 val_loss=0.0000 scale=1.0000 norm=1.4579\n",
      "[iter 0] loss=1.4008 val_loss=0.0000 scale=1.0000 norm=1.4543\n",
      "[iter 100] loss=1.2836 val_loss=0.0000 scale=1.0000 norm=1.4599\n",
      "[iter 0] loss=1.4012 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 100] loss=1.2818 val_loss=0.0000 scale=1.0000 norm=1.4617\n",
      "[iter 0] loss=1.4010 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 100] loss=1.2825 val_loss=0.0000 scale=1.0000 norm=1.4622\n",
      "[iter 0] loss=1.4038 val_loss=0.0000 scale=1.0000 norm=1.4507\n",
      "[iter 100] loss=1.2855 val_loss=0.0000 scale=1.0000 norm=1.4580\n",
      "[iter 0] loss=1.4039 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.2889 val_loss=0.0000 scale=1.0000 norm=1.4557\n",
      "[iter 0] loss=1.4049 val_loss=0.0000 scale=1.0000 norm=1.4504\n",
      "[iter 100] loss=1.2872 val_loss=0.0000 scale=1.0000 norm=1.4550\n",
      "[iter 0] loss=1.4041 val_loss=0.0000 scale=1.0000 norm=1.4519\n",
      "[iter 100] loss=1.2868 val_loss=0.0000 scale=1.0000 norm=1.4566\n",
      "[iter 0] loss=1.4043 val_loss=0.0000 scale=1.0000 norm=1.4513\n",
      "[iter 100] loss=1.2879 val_loss=0.0000 scale=1.0000 norm=1.4551\n",
      "[iter 0] loss=1.4047 val_loss=0.0000 scale=1.0000 norm=1.4505\n",
      "[iter 100] loss=1.2884 val_loss=0.0000 scale=1.0000 norm=1.4547\n",
      "[iter 0] loss=1.3672 val_loss=0.0000 scale=1.0000 norm=1.4968\n",
      "[iter 100] loss=1.2439 val_loss=0.0000 scale=1.0000 norm=1.5025\n",
      "[iter 0] loss=1.3151 val_loss=0.0000 scale=1.0000 norm=1.5602\n",
      "[iter 100] loss=1.1821 val_loss=0.0000 scale=1.0000 norm=1.5666\n",
      "[iter 0] loss=1.3115 val_loss=0.0000 scale=1.0000 norm=1.5657\n",
      "[iter 100] loss=1.1787 val_loss=0.0000 scale=1.0000 norm=1.5738\n",
      "[iter 0] loss=1.3139 val_loss=0.0000 scale=1.0000 norm=1.5632\n",
      "[iter 100] loss=1.1834 val_loss=0.0000 scale=1.0000 norm=1.5736\n",
      "[iter 0] loss=1.2077 val_loss=0.0000 scale=1.0000 norm=1.6968\n",
      "[iter 100] loss=1.0745 val_loss=0.0000 scale=0.5000 norm=0.8498\n",
      "[iter 0] loss=1.2088 val_loss=0.0000 scale=1.0000 norm=1.7007\n",
      "[iter 100] loss=1.0698 val_loss=0.0000 scale=0.5000 norm=0.8544\n",
      "[iter 0] loss=1.2099 val_loss=0.0000 scale=1.0000 norm=1.7003\n",
      "[iter 100] loss=1.0725 val_loss=0.0000 scale=0.5000 norm=0.8531\n",
      "[iter 0] loss=1.2101 val_loss=0.0000 scale=1.0000 norm=1.7000\n",
      "[iter 100] loss=1.0721 val_loss=0.0000 scale=0.5000 norm=0.8532\n",
      "[iter 0] loss=1.2052 val_loss=0.0000 scale=1.0000 norm=1.7079\n",
      "[iter 100] loss=1.0670 val_loss=0.0000 scale=0.5000 norm=0.8563\n",
      "[iter 0] loss=1.2066 val_loss=0.0000 scale=1.0000 norm=1.7089\n",
      "[iter 100] loss=1.0714 val_loss=0.0000 scale=0.5000 norm=0.8549\n",
      "[iter 0] loss=1.2060 val_loss=0.0000 scale=1.0000 norm=1.7098\n",
      "[iter 100] loss=1.0723 val_loss=0.0000 scale=0.5000 norm=0.8551\n",
      "[iter 0] loss=1.2099 val_loss=0.0000 scale=1.0000 norm=1.7071\n",
      "[iter 100] loss=1.0733 val_loss=0.0000 scale=1.0000 norm=1.7076\n",
      "[iter 0] loss=1.2140 val_loss=0.0000 scale=1.0000 norm=1.7024\n",
      "[iter 100] loss=1.0742 val_loss=0.0000 scale=1.0000 norm=1.7023\n",
      "[iter 0] loss=1.2151 val_loss=0.0000 scale=1.0000 norm=1.7016\n",
      "[iter 100] loss=1.0715 val_loss=0.0000 scale=0.5000 norm=0.8538\n",
      "[iter 0] loss=1.2182 val_loss=0.0000 scale=1.0000 norm=1.6975\n",
      "[iter 100] loss=1.0756 val_loss=0.0000 scale=0.5000 norm=0.8522\n",
      "[iter 0] loss=1.2165 val_loss=0.0000 scale=1.0000 norm=1.7008\n",
      "[iter 100] loss=1.0779 val_loss=0.0000 scale=1.0000 norm=1.7024\n",
      "[iter 0] loss=1.2164 val_loss=0.0000 scale=1.0000 norm=1.7008\n",
      "[iter 100] loss=1.0754 val_loss=0.0000 scale=0.5000 norm=0.8521\n",
      "[iter 0] loss=1.2157 val_loss=0.0000 scale=1.0000 norm=1.7026\n",
      "[iter 100] loss=1.0800 val_loss=0.0000 scale=1.0000 norm=1.7042\n",
      "[iter 0] loss=1.2134 val_loss=0.0000 scale=1.0000 norm=1.7069\n",
      "[iter 100] loss=1.0678 val_loss=0.0000 scale=0.5000 norm=0.8539\n",
      "[iter 0] loss=1.2141 val_loss=0.0000 scale=1.0000 norm=1.7055\n",
      "[iter 100] loss=1.0649 val_loss=0.0000 scale=0.5000 norm=0.8561\n",
      "[iter 0] loss=1.2124 val_loss=0.0000 scale=1.0000 norm=1.7077\n",
      "[iter 100] loss=1.0708 val_loss=0.0000 scale=1.0000 norm=1.7100\n",
      "[iter 0] loss=1.2137 val_loss=0.0000 scale=1.0000 norm=1.7054\n",
      "[iter 100] loss=1.0644 val_loss=0.0000 scale=0.5000 norm=0.8545\n",
      "[iter 0] loss=1.2146 val_loss=0.0000 scale=1.0000 norm=1.7033\n",
      "[iter 100] loss=1.0751 val_loss=0.0000 scale=0.5000 norm=0.8550\n",
      "[iter 0] loss=1.2117 val_loss=0.0000 scale=1.0000 norm=1.7087\n",
      "[iter 100] loss=1.0673 val_loss=0.0000 scale=0.5000 norm=0.8568\n",
      "[iter 0] loss=1.2100 val_loss=0.0000 scale=1.0000 norm=1.7120\n",
      "[iter 100] loss=1.0713 val_loss=0.0000 scale=0.5000 norm=0.8561\n",
      "[iter 0] loss=1.2102 val_loss=0.0000 scale=1.0000 norm=1.7117\n",
      "[iter 100] loss=1.0677 val_loss=0.0000 scale=0.5000 norm=0.8559\n",
      "[iter 0] loss=1.2132 val_loss=0.0000 scale=1.0000 norm=1.7106\n",
      "[iter 100] loss=1.0631 val_loss=0.0000 scale=0.5000 norm=0.8572\n",
      "[iter 0] loss=1.2139 val_loss=0.0000 scale=1.0000 norm=1.7091\n",
      "[iter 100] loss=1.0639 val_loss=0.0000 scale=1.0000 norm=1.7135\n",
      "[iter 0] loss=1.2139 val_loss=0.0000 scale=1.0000 norm=1.7091\n",
      "[iter 100] loss=1.0682 val_loss=0.0000 scale=1.0000 norm=1.7139\n",
      "[iter 0] loss=1.2141 val_loss=0.0000 scale=1.0000 norm=1.7092\n",
      "[iter 100] loss=1.0643 val_loss=0.0000 scale=0.5000 norm=0.8566\n",
      "[iter 0] loss=1.2147 val_loss=0.0000 scale=1.0000 norm=1.7078\n",
      "[iter 100] loss=1.0727 val_loss=0.0000 scale=0.5000 norm=0.8523\n",
      "[iter 0] loss=1.2118 val_loss=0.0000 scale=1.0000 norm=1.7132\n",
      "[iter 100] loss=1.0685 val_loss=0.0000 scale=0.5000 norm=0.8535\n",
      "[iter 0] loss=1.2125 val_loss=0.0000 scale=1.0000 norm=1.7118\n",
      "[iter 100] loss=1.0708 val_loss=0.0000 scale=1.0000 norm=1.7048\n",
      "[iter 0] loss=1.2180 val_loss=0.0000 scale=1.0000 norm=1.7027\n",
      "[iter 100] loss=1.0713 val_loss=0.0000 scale=0.5000 norm=0.8519\n",
      "[iter 0] loss=1.2181 val_loss=0.0000 scale=1.0000 norm=1.7026\n",
      "[iter 100] loss=1.0706 val_loss=0.0000 scale=0.5000 norm=0.8529\n",
      "[iter 0] loss=1.2203 val_loss=0.0000 scale=1.0000 norm=1.6981\n",
      "[iter 100] loss=1.0736 val_loss=0.0000 scale=0.5000 norm=0.8530\n",
      "[iter 0] loss=1.2245 val_loss=0.0000 scale=1.0000 norm=1.6920\n",
      "[iter 100] loss=1.0765 val_loss=0.0000 scale=0.5000 norm=0.8515\n",
      "[iter 0] loss=1.2238 val_loss=0.0000 scale=1.0000 norm=1.6934\n",
      "[iter 100] loss=1.0753 val_loss=0.0000 scale=0.5000 norm=0.8514\n",
      "[iter 0] loss=1.2229 val_loss=0.0000 scale=1.0000 norm=1.6951\n",
      "[iter 100] loss=1.0765 val_loss=0.0000 scale=0.5000 norm=0.8530\n",
      "[iter 0] loss=1.2230 val_loss=0.0000 scale=1.0000 norm=1.6951\n",
      "[iter 100] loss=1.0774 val_loss=0.0000 scale=1.0000 norm=1.7016\n",
      "[iter 0] loss=1.2207 val_loss=0.0000 scale=1.0000 norm=1.6996\n",
      "[iter 100] loss=1.0759 val_loss=0.0000 scale=0.5000 norm=0.8505\n",
      "[iter 0] loss=1.2195 val_loss=0.0000 scale=1.0000 norm=1.7025\n",
      "[iter 100] loss=1.0812 val_loss=0.0000 scale=0.5000 norm=0.8489\n",
      "[iter 0] loss=1.2181 val_loss=0.0000 scale=1.0000 norm=1.7048\n",
      "[iter 100] loss=1.0800 val_loss=0.0000 scale=0.5000 norm=0.8486\n",
      "[iter 0] loss=1.2198 val_loss=0.0000 scale=1.0000 norm=1.7015\n",
      "[iter 100] loss=1.0796 val_loss=0.0000 scale=0.5000 norm=0.8476\n",
      "[iter 0] loss=1.2221 val_loss=0.0000 scale=1.0000 norm=1.6972\n",
      "[iter 100] loss=1.0809 val_loss=0.0000 scale=0.5000 norm=0.8458\n",
      "[iter 0] loss=1.2192 val_loss=0.0000 scale=1.0000 norm=1.7026\n",
      "[iter 100] loss=1.0774 val_loss=0.0000 scale=0.5000 norm=0.8483\n",
      "[iter 0] loss=1.2197 val_loss=0.0000 scale=1.0000 norm=1.7020\n",
      "[iter 100] loss=1.0768 val_loss=0.0000 scale=1.0000 norm=1.6977\n",
      "[iter 0] loss=1.2201 val_loss=0.0000 scale=1.0000 norm=1.7011\n",
      "[iter 100] loss=1.0796 val_loss=0.0000 scale=0.5000 norm=0.8490\n",
      "[iter 0] loss=1.2203 val_loss=0.0000 scale=1.0000 norm=1.7009\n",
      "[iter 100] loss=1.0799 val_loss=0.0000 scale=0.5000 norm=0.8481\n",
      "[iter 0] loss=1.2243 val_loss=0.0000 scale=1.0000 norm=1.6933\n",
      "[iter 100] loss=1.0827 val_loss=0.0000 scale=0.5000 norm=0.8465\n",
      "[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=1.6926\n",
      "[iter 100] loss=1.0783 val_loss=0.0000 scale=0.5000 norm=0.8490\n",
      "[iter 0] loss=1.2275 val_loss=0.0000 scale=1.0000 norm=1.6870\n",
      "[iter 100] loss=1.0827 val_loss=0.0000 scale=0.5000 norm=0.8471\n",
      "[iter 0] loss=1.2275 val_loss=0.0000 scale=1.0000 norm=1.6868\n",
      "[iter 100] loss=1.0815 val_loss=0.0000 scale=0.5000 norm=0.8488\n",
      "[iter 0] loss=1.2266 val_loss=0.0000 scale=1.0000 norm=1.6883\n",
      "[iter 100] loss=1.0870 val_loss=0.0000 scale=0.5000 norm=0.8456\n",
      "[iter 0] loss=1.2286 val_loss=0.0000 scale=1.0000 norm=1.6845\n",
      "[iter 100] loss=1.0826 val_loss=0.0000 scale=0.5000 norm=0.8450\n",
      "[iter 0] loss=1.2306 val_loss=0.0000 scale=1.0000 norm=1.6811\n",
      "[iter 100] loss=1.0823 val_loss=0.0000 scale=0.5000 norm=0.8448\n",
      "[iter 0] loss=1.2303 val_loss=0.0000 scale=1.0000 norm=1.6815\n",
      "[iter 100] loss=1.0813 val_loss=0.0000 scale=1.0000 norm=1.6896\n",
      "[iter 0] loss=1.2309 val_loss=0.0000 scale=1.0000 norm=1.6807\n",
      "[iter 100] loss=1.0805 val_loss=0.0000 scale=0.5000 norm=0.8420\n",
      "[iter 0] loss=1.2312 val_loss=0.0000 scale=1.0000 norm=1.6803\n",
      "[iter 100] loss=1.0820 val_loss=0.0000 scale=0.5000 norm=0.8428\n",
      "[iter 0] loss=1.2317 val_loss=0.0000 scale=1.0000 norm=1.6795\n",
      "[iter 100] loss=1.0841 val_loss=0.0000 scale=1.0000 norm=1.6862\n",
      "[iter 0] loss=1.2319 val_loss=0.0000 scale=1.0000 norm=1.6791\n",
      "[iter 100] loss=1.0810 val_loss=0.0000 scale=1.0000 norm=1.6849\n",
      "[iter 0] loss=1.2311 val_loss=0.0000 scale=1.0000 norm=1.6809\n",
      "[iter 100] loss=1.0783 val_loss=0.0000 scale=0.5000 norm=0.8420\n",
      "[iter 0] loss=1.2325 val_loss=0.0000 scale=1.0000 norm=1.6783\n",
      "[iter 100] loss=1.0736 val_loss=0.0000 scale=0.5000 norm=0.8428\n",
      "[iter 0] loss=1.2344 val_loss=0.0000 scale=1.0000 norm=1.6743\n",
      "[iter 100] loss=1.0787 val_loss=0.0000 scale=0.5000 norm=0.8383\n",
      "[iter 0] loss=1.2342 val_loss=0.0000 scale=1.0000 norm=1.6745\n",
      "[iter 100] loss=1.0754 val_loss=0.0000 scale=0.5000 norm=0.8417\n",
      "[iter 0] loss=1.2343 val_loss=0.0000 scale=1.0000 norm=1.6742\n",
      "[iter 100] loss=1.0761 val_loss=0.0000 scale=0.5000 norm=0.8415\n",
      "[iter 0] loss=1.2344 val_loss=0.0000 scale=1.0000 norm=1.6742\n",
      "[iter 100] loss=1.0745 val_loss=0.0000 scale=1.0000 norm=1.6843\n",
      "[iter 0] loss=1.2352 val_loss=0.0000 scale=1.0000 norm=1.6726\n",
      "[iter 100] loss=1.0747 val_loss=0.0000 scale=0.5000 norm=0.8419\n",
      "[iter 0] loss=1.2331 val_loss=0.0000 scale=1.0000 norm=1.6765\n",
      "[iter 100] loss=1.0741 val_loss=0.0000 scale=1.0000 norm=1.6864\n",
      "[iter 0] loss=1.2312 val_loss=0.0000 scale=1.0000 norm=1.6800\n",
      "[iter 100] loss=1.0744 val_loss=0.0000 scale=1.0000 norm=1.6935\n",
      "[iter 0] loss=1.2307 val_loss=0.0000 scale=1.0000 norm=1.6809\n",
      "[iter 100] loss=1.0737 val_loss=0.0000 scale=0.5000 norm=0.8458\n",
      "[iter 0] loss=1.2312 val_loss=0.0000 scale=1.0000 norm=1.6801\n",
      "[iter 100] loss=1.0722 val_loss=0.0000 scale=0.5000 norm=0.8442\n",
      "[iter 0] loss=1.2304 val_loss=0.0000 scale=1.0000 norm=1.6815\n",
      "[iter 100] loss=1.0685 val_loss=0.0000 scale=0.5000 norm=0.8452\n",
      "[iter 0] loss=1.2257 val_loss=0.0000 scale=1.0000 norm=1.6898\n",
      "[iter 100] loss=1.0620 val_loss=0.0000 scale=0.5000 norm=0.8501\n",
      "[iter 0] loss=1.2230 val_loss=0.0000 scale=1.0000 norm=1.6950\n",
      "[iter 100] loss=1.0565 val_loss=0.0000 scale=1.0000 norm=1.7042\n",
      "[iter 0] loss=1.2219 val_loss=0.0000 scale=1.0000 norm=1.6976\n",
      "[iter 100] loss=1.0519 val_loss=0.0000 scale=0.5000 norm=0.8551\n",
      "[iter 0] loss=1.2195 val_loss=0.0000 scale=1.0000 norm=1.7021\n",
      "[iter 100] loss=1.0451 val_loss=0.0000 scale=0.5000 norm=0.8582\n",
      "[iter 0] loss=1.2193 val_loss=0.0000 scale=1.0000 norm=1.7026\n",
      "[iter 100] loss=1.0437 val_loss=0.0000 scale=0.5000 norm=0.8595\n",
      "[iter 0] loss=1.2136 val_loss=0.0000 scale=1.0000 norm=1.7097\n",
      "[iter 100] loss=1.0413 val_loss=0.0000 scale=0.5000 norm=0.8618\n",
      "[iter 0] loss=1.2128 val_loss=0.0000 scale=1.0000 norm=1.7113\n",
      "[iter 100] loss=1.0401 val_loss=0.0000 scale=1.0000 norm=1.7279\n",
      "[iter 0] loss=1.2104 val_loss=0.0000 scale=1.0000 norm=1.7157\n",
      "[iter 100] loss=1.0392 val_loss=0.0000 scale=0.5000 norm=0.8662\n",
      "[iter 0] loss=1.2115 val_loss=0.0000 scale=1.0000 norm=1.7136\n",
      "[iter 100] loss=1.0432 val_loss=0.0000 scale=0.5000 norm=0.8659\n",
      "[iter 0] loss=1.2155 val_loss=0.0000 scale=1.0000 norm=1.7077\n",
      "[iter 100] loss=1.0463 val_loss=0.0000 scale=1.0000 norm=1.7287\n",
      "[iter 0] loss=1.2152 val_loss=0.0000 scale=1.0000 norm=1.7081\n",
      "[iter 100] loss=1.0444 val_loss=0.0000 scale=0.5000 norm=0.8655\n",
      "[iter 0] loss=1.2117 val_loss=0.0000 scale=1.0000 norm=1.7137\n",
      "[iter 100] loss=1.0453 val_loss=0.0000 scale=0.5000 norm=0.8676\n",
      "[iter 0] loss=1.2070 val_loss=0.0000 scale=1.0000 norm=1.7208\n",
      "[iter 100] loss=1.0433 val_loss=0.0000 scale=0.5000 norm=0.8686\n",
      "[iter 0] loss=1.2033 val_loss=0.0000 scale=1.0000 norm=1.7223\n",
      "[iter 100] loss=1.0474 val_loss=0.0000 scale=0.5000 norm=0.8662\n",
      "[iter 0] loss=1.2023 val_loss=0.0000 scale=1.0000 norm=1.7245\n",
      "[iter 100] loss=1.0459 val_loss=0.0000 scale=0.5000 norm=0.8653\n",
      "[iter 0] loss=1.2001 val_loss=0.0000 scale=1.0000 norm=1.7289\n",
      "[iter 100] loss=1.0359 val_loss=0.0000 scale=0.5000 norm=0.8751\n",
      "[iter 0] loss=1.1977 val_loss=0.0000 scale=1.0000 norm=1.7308\n",
      "[iter 100] loss=1.0467 val_loss=0.0000 scale=1.0000 norm=1.7407\n",
      "[iter 0] loss=1.1974 val_loss=0.0000 scale=1.0000 norm=1.7312\n",
      "[iter 100] loss=1.0441 val_loss=0.0000 scale=0.5000 norm=0.8701\n",
      "[iter 0] loss=1.1974 val_loss=0.0000 scale=1.0000 norm=1.7313\n",
      "[iter 100] loss=1.0444 val_loss=0.0000 scale=0.5000 norm=0.8680\n",
      "[iter 0] loss=1.1935 val_loss=0.0000 scale=1.0000 norm=1.7377\n",
      "[iter 100] loss=1.0432 val_loss=0.0000 scale=0.5000 norm=0.8723\n",
      "[iter 0] loss=1.1934 val_loss=0.0000 scale=1.0000 norm=1.7377\n",
      "[iter 100] loss=1.0406 val_loss=0.0000 scale=0.5000 norm=0.8731\n",
      "[iter 0] loss=1.1909 val_loss=0.0000 scale=1.0000 norm=1.7413\n",
      "[iter 100] loss=1.0446 val_loss=0.0000 scale=1.0000 norm=1.7486\n",
      "[iter 0] loss=1.1915 val_loss=0.0000 scale=0.5000 norm=0.8700\n",
      "[iter 100] loss=1.0436 val_loss=0.0000 scale=1.0000 norm=1.7425\n",
      "[iter 0] loss=1.1903 val_loss=0.0000 scale=1.0000 norm=1.7421\n",
      "[iter 100] loss=1.0366 val_loss=0.0000 scale=0.5000 norm=0.8776\n",
      "[iter 0] loss=1.1943 val_loss=0.0000 scale=0.5000 norm=0.8676\n",
      "[iter 100] loss=1.0520 val_loss=0.0000 scale=0.5000 norm=0.8691\n",
      "[iter 0] loss=1.1952 val_loss=0.0000 scale=0.5000 norm=0.8666\n",
      "[iter 100] loss=1.0487 val_loss=0.0000 scale=0.5000 norm=0.8675\n",
      "[iter 0] loss=1.1921 val_loss=0.0000 scale=0.5000 norm=0.8697\n",
      "[iter 100] loss=1.0424 val_loss=0.0000 scale=1.0000 norm=1.7437\n",
      "[iter 0] loss=1.1890 val_loss=0.0000 scale=0.5000 norm=0.8728\n",
      "[iter 100] loss=1.0490 val_loss=0.0000 scale=1.0000 norm=1.7405\n",
      "[iter 0] loss=1.1882 val_loss=0.0000 scale=0.5000 norm=0.8736\n",
      "[iter 100] loss=1.0491 val_loss=0.0000 scale=1.0000 norm=1.7393\n",
      "[iter 0] loss=1.1906 val_loss=0.0000 scale=0.5000 norm=0.8712\n",
      "[iter 100] loss=1.0438 val_loss=0.0000 scale=0.5000 norm=0.8735\n",
      "[iter 0] loss=1.1910 val_loss=0.0000 scale=0.5000 norm=0.8707\n",
      "[iter 100] loss=1.0461 val_loss=0.0000 scale=0.5000 norm=0.8751\n",
      "[iter 0] loss=1.1915 val_loss=0.0000 scale=0.5000 norm=0.8704\n",
      "[iter 100] loss=1.0448 val_loss=0.0000 scale=0.5000 norm=0.8737\n",
      "[iter 0] loss=1.1908 val_loss=0.0000 scale=1.0000 norm=1.7422\n",
      "[iter 100] loss=1.0393 val_loss=0.0000 scale=0.5000 norm=0.8777\n",
      "[iter 0] loss=1.1856 val_loss=0.0000 scale=1.0000 norm=1.7488\n",
      "[iter 100] loss=1.0302 val_loss=0.0000 scale=1.0000 norm=1.7683\n",
      "[iter 0] loss=1.1852 val_loss=0.0000 scale=1.0000 norm=1.7495\n",
      "[iter 100] loss=1.0274 val_loss=0.0000 scale=0.5000 norm=0.8847\n",
      "[iter 0] loss=1.1843 val_loss=0.0000 scale=1.0000 norm=1.7512\n",
      "[iter 100] loss=1.0216 val_loss=0.0000 scale=1.0000 norm=1.7755\n",
      "[iter 0] loss=1.1782 val_loss=0.0000 scale=0.5000 norm=0.8804\n",
      "[iter 100] loss=1.0221 val_loss=0.0000 scale=0.5000 norm=0.8883\n",
      "[iter 0] loss=1.1769 val_loss=0.0000 scale=0.5000 norm=0.8817\n",
      "[iter 100] loss=1.0269 val_loss=0.0000 scale=0.5000 norm=0.8868\n",
      "[iter 0] loss=1.1766 val_loss=0.0000 scale=0.5000 norm=0.8819\n",
      "[iter 100] loss=1.0217 val_loss=0.0000 scale=1.0000 norm=1.7696\n",
      "[iter 0] loss=1.1741 val_loss=0.0000 scale=0.5000 norm=0.8844\n",
      "[iter 100] loss=1.0198 val_loss=0.0000 scale=0.5000 norm=0.8867\n",
      "[iter 0] loss=1.1729 val_loss=0.0000 scale=0.5000 norm=0.8855\n",
      "[iter 100] loss=1.0188 val_loss=0.0000 scale=0.5000 norm=0.8896\n",
      "[iter 0] loss=1.1724 val_loss=0.0000 scale=0.5000 norm=0.8858\n",
      "[iter 100] loss=1.0191 val_loss=0.0000 scale=0.5000 norm=0.8919\n",
      "[iter 0] loss=1.1730 val_loss=0.0000 scale=0.5000 norm=0.8855\n",
      "[iter 100] loss=1.0203 val_loss=0.0000 scale=0.5000 norm=0.8889\n",
      "[iter 0] loss=1.1735 val_loss=0.0000 scale=0.5000 norm=0.8851\n",
      "[iter 100] loss=1.0177 val_loss=0.0000 scale=0.5000 norm=0.8925\n",
      "[iter 0] loss=1.1730 val_loss=0.0000 scale=0.5000 norm=0.8857\n",
      "[iter 100] loss=1.0154 val_loss=0.0000 scale=0.5000 norm=0.8957\n",
      "[iter 0] loss=1.1699 val_loss=0.0000 scale=0.5000 norm=0.8879\n",
      "[iter 100] loss=1.0141 val_loss=0.0000 scale=0.5000 norm=0.8950\n",
      "[iter 0] loss=1.1703 val_loss=0.0000 scale=0.5000 norm=0.8877\n",
      "[iter 100] loss=1.0160 val_loss=0.0000 scale=0.5000 norm=0.8938\n",
      "[iter 0] loss=1.1720 val_loss=0.0000 scale=1.0000 norm=1.7716\n",
      "[iter 100] loss=1.0183 val_loss=0.0000 scale=1.0000 norm=1.7860\n",
      "[iter 0] loss=1.1729 val_loss=0.0000 scale=1.0000 norm=1.7702\n",
      "[iter 100] loss=1.0125 val_loss=0.0000 scale=0.5000 norm=0.8958\n",
      "[iter 0] loss=1.1725 val_loss=0.0000 scale=1.0000 norm=1.7711\n",
      "[iter 100] loss=1.0255 val_loss=0.0000 scale=0.5000 norm=0.8899\n",
      "[iter 0] loss=1.1725 val_loss=0.0000 scale=0.5000 norm=0.8856\n",
      "[iter 100] loss=1.0224 val_loss=0.0000 scale=1.0000 norm=1.7818\n",
      "[iter 0] loss=1.1723 val_loss=0.0000 scale=0.5000 norm=0.8858\n",
      "[iter 100] loss=1.0187 val_loss=0.0000 scale=0.5000 norm=0.8941\n",
      "[iter 0] loss=1.1720 val_loss=0.0000 scale=0.5000 norm=0.8860\n",
      "[iter 100] loss=1.0244 val_loss=0.0000 scale=0.5000 norm=0.8911\n",
      "[iter 0] loss=1.1680 val_loss=0.0000 scale=0.5000 norm=0.8898\n",
      "[iter 100] loss=1.0179 val_loss=0.0000 scale=0.5000 norm=0.8971\n",
      "[iter 0] loss=1.1682 val_loss=0.0000 scale=0.5000 norm=0.8898\n",
      "[iter 100] loss=1.0140 val_loss=0.0000 scale=0.5000 norm=0.9018\n",
      "[iter 0] loss=1.1690 val_loss=0.0000 scale=0.5000 norm=0.8889\n",
      "[iter 100] loss=1.0211 val_loss=0.0000 scale=1.0000 norm=1.7868\n",
      "[iter 0] loss=1.1677 val_loss=0.0000 scale=0.5000 norm=0.8904\n",
      "[iter 100] loss=1.0184 val_loss=0.0000 scale=0.5000 norm=0.8955\n",
      "[iter 0] loss=1.1644 val_loss=0.0000 scale=0.5000 norm=0.8936\n",
      "[iter 100] loss=1.0121 val_loss=0.0000 scale=0.5000 norm=0.8991\n",
      "[iter 0] loss=1.1671 val_loss=0.0000 scale=0.5000 norm=0.8940\n",
      "[iter 100] loss=1.0199 val_loss=0.0000 scale=0.5000 norm=0.9001\n",
      "[iter 0] loss=1.1706 val_loss=0.0000 scale=0.5000 norm=0.8909\n",
      "[iter 100] loss=1.0278 val_loss=0.0000 scale=0.5000 norm=0.8945\n",
      "[iter 0] loss=1.1704 val_loss=0.0000 scale=0.5000 norm=0.8910\n",
      "[iter 100] loss=1.0239 val_loss=0.0000 scale=0.5000 norm=0.8941\n",
      "[iter 0] loss=1.1697 val_loss=0.0000 scale=0.5000 norm=0.8916\n",
      "[iter 100] loss=1.0302 val_loss=0.0000 scale=0.5000 norm=0.8920\n",
      "[iter 0] loss=1.1705 val_loss=0.0000 scale=0.5000 norm=0.8907\n",
      "[iter 100] loss=1.0333 val_loss=0.0000 scale=0.5000 norm=0.8904\n",
      "[iter 0] loss=1.1681 val_loss=0.0000 scale=0.5000 norm=0.8934\n",
      "[iter 100] loss=1.0310 val_loss=0.0000 scale=0.5000 norm=0.8927\n",
      "[iter 0] loss=1.1706 val_loss=0.0000 scale=0.5000 norm=0.8908\n",
      "[iter 100] loss=1.0327 val_loss=0.0000 scale=0.5000 norm=0.8892\n",
      "[iter 0] loss=1.1706 val_loss=0.0000 scale=0.5000 norm=0.8908\n",
      "[iter 100] loss=1.0301 val_loss=0.0000 scale=1.0000 norm=1.7749\n",
      "[iter 0] loss=1.1723 val_loss=0.0000 scale=0.5000 norm=0.8890\n",
      "[iter 100] loss=1.0267 val_loss=0.0000 scale=1.0000 norm=1.7843\n",
      "[iter 0] loss=1.1727 val_loss=0.0000 scale=0.5000 norm=0.8886\n",
      "[iter 100] loss=1.0314 val_loss=0.0000 scale=0.5000 norm=0.8916\n",
      "[iter 0] loss=1.1743 val_loss=0.0000 scale=0.5000 norm=0.8871\n",
      "[iter 100] loss=1.0334 val_loss=0.0000 scale=0.5000 norm=0.8916\n",
      "[iter 0] loss=1.1740 val_loss=0.0000 scale=0.5000 norm=0.8875\n",
      "[iter 100] loss=1.0317 val_loss=0.0000 scale=0.5000 norm=0.8909\n",
      "[iter 0] loss=1.1740 val_loss=0.0000 scale=0.5000 norm=0.8875\n",
      "[iter 100] loss=1.0286 val_loss=0.0000 scale=0.5000 norm=0.8908\n",
      "[iter 0] loss=1.1740 val_loss=0.0000 scale=0.5000 norm=0.8876\n",
      "[iter 100] loss=1.0279 val_loss=0.0000 scale=0.5000 norm=0.8916\n",
      "[iter 0] loss=1.1781 val_loss=0.0000 scale=0.5000 norm=0.8834\n",
      "[iter 100] loss=1.0375 val_loss=0.0000 scale=1.0000 norm=1.7707\n",
      "[iter 0] loss=1.1782 val_loss=0.0000 scale=0.5000 norm=0.8834\n",
      "[iter 100] loss=1.0386 val_loss=0.0000 scale=0.5000 norm=0.8856\n",
      "[iter 0] loss=1.1771 val_loss=0.0000 scale=0.5000 norm=0.8845\n",
      "[iter 100] loss=1.0378 val_loss=0.0000 scale=1.0000 norm=1.7744\n",
      "[iter 0] loss=1.1756 val_loss=0.0000 scale=0.5000 norm=0.8847\n",
      "[iter 100] loss=1.0333 val_loss=0.0000 scale=1.0000 norm=1.7719\n",
      "[iter 0] loss=1.1755 val_loss=0.0000 scale=0.5000 norm=0.8848\n",
      "[iter 100] loss=1.0331 val_loss=0.0000 scale=0.5000 norm=0.8838\n",
      "[iter 0] loss=1.1793 val_loss=0.0000 scale=0.5000 norm=0.8811\n",
      "[iter 100] loss=1.0357 val_loss=0.0000 scale=0.5000 norm=0.8820\n",
      "[iter 0] loss=1.1757 val_loss=0.0000 scale=0.5000 norm=0.8846\n",
      "[iter 100] loss=1.0332 val_loss=0.0000 scale=1.0000 norm=1.7670\n",
      "[iter 0] loss=1.1750 val_loss=0.0000 scale=0.5000 norm=0.8836\n",
      "[iter 100] loss=1.0194 val_loss=0.0000 scale=0.5000 norm=0.8864\n",
      "[iter 0] loss=1.1735 val_loss=0.0000 scale=0.5000 norm=0.8850\n",
      "[iter 100] loss=1.0220 val_loss=0.0000 scale=0.5000 norm=0.8869\n",
      "[iter 0] loss=1.1732 val_loss=0.0000 scale=0.5000 norm=0.8851\n",
      "[iter 100] loss=1.0212 val_loss=0.0000 scale=0.5000 norm=0.8884\n",
      "[iter 0] loss=1.1794 val_loss=0.0000 scale=0.5000 norm=0.8801\n",
      "[iter 100] loss=1.0274 val_loss=0.0000 scale=0.5000 norm=0.8843\n",
      "[iter 0] loss=1.1787 val_loss=0.0000 scale=0.5000 norm=0.8807\n",
      "[iter 100] loss=1.0320 val_loss=0.0000 scale=1.0000 norm=1.7654\n",
      "[iter 0] loss=1.1825 val_loss=0.0000 scale=0.5000 norm=0.8771\n",
      "[iter 100] loss=1.0334 val_loss=0.0000 scale=1.0000 norm=1.7641\n",
      "[iter 0] loss=1.1835 val_loss=0.0000 scale=1.0000 norm=1.7521\n",
      "[iter 100] loss=1.0327 val_loss=0.0000 scale=0.5000 norm=0.8813\n",
      "[iter 0] loss=1.1784 val_loss=0.0000 scale=0.5000 norm=0.8807\n",
      "[iter 100] loss=1.0329 val_loss=0.0000 scale=1.0000 norm=1.7722\n",
      "[iter 0] loss=1.1841 val_loss=0.0000 scale=0.5000 norm=0.8760\n",
      "[iter 100] loss=1.0403 val_loss=0.0000 scale=0.5000 norm=0.8786\n",
      "[iter 0] loss=1.1863 val_loss=0.0000 scale=0.5000 norm=0.8736\n",
      "[iter 100] loss=1.0412 val_loss=0.0000 scale=1.0000 norm=1.7526\n",
      "[iter 0] loss=1.1908 val_loss=0.0000 scale=1.0000 norm=1.7397\n",
      "[iter 100] loss=1.0533 val_loss=0.0000 scale=0.5000 norm=0.8715\n",
      "[iter 0] loss=1.1922 val_loss=0.0000 scale=1.0000 norm=1.7369\n",
      "[iter 100] loss=1.0517 val_loss=0.0000 scale=1.0000 norm=1.7390\n",
      "[iter 0] loss=1.1932 val_loss=0.0000 scale=1.0000 norm=1.7348\n",
      "[iter 100] loss=1.0488 val_loss=0.0000 scale=0.5000 norm=0.8709\n",
      "[iter 0] loss=1.1938 val_loss=0.0000 scale=1.0000 norm=1.7339\n",
      "[iter 100] loss=1.0553 val_loss=0.0000 scale=0.5000 norm=0.8690\n",
      "[iter 0] loss=1.1949 val_loss=0.0000 scale=1.0000 norm=1.7317\n",
      "[iter 100] loss=1.0543 val_loss=0.0000 scale=0.5000 norm=0.8677\n",
      "[iter 0] loss=1.1937 val_loss=0.0000 scale=1.0000 norm=1.7338\n",
      "[iter 100] loss=1.0517 val_loss=0.0000 scale=0.5000 norm=0.8693\n",
      "[iter 0] loss=1.1931 val_loss=0.0000 scale=1.0000 norm=1.7348\n",
      "[iter 100] loss=1.0528 val_loss=0.0000 scale=1.0000 norm=1.7401\n",
      "[iter 0] loss=1.1930 val_loss=0.0000 scale=1.0000 norm=1.7349\n",
      "[iter 100] loss=1.0544 val_loss=0.0000 scale=0.5000 norm=0.8687\n",
      "[iter 0] loss=1.1930 val_loss=0.0000 scale=1.0000 norm=1.7350\n",
      "[iter 100] loss=1.0514 val_loss=0.0000 scale=0.5000 norm=0.8709\n",
      "[iter 0] loss=1.1931 val_loss=0.0000 scale=1.0000 norm=1.7345\n",
      "[iter 100] loss=1.0462 val_loss=0.0000 scale=0.5000 norm=0.8700\n",
      "[iter 0] loss=1.1929 val_loss=0.0000 scale=1.0000 norm=1.7348\n",
      "[iter 100] loss=1.0497 val_loss=0.0000 scale=0.5000 norm=0.8669\n",
      "[iter 0] loss=1.1940 val_loss=0.0000 scale=1.0000 norm=1.7330\n",
      "[iter 100] loss=1.0546 val_loss=0.0000 scale=1.0000 norm=1.7226\n",
      "[iter 0] loss=1.1918 val_loss=0.0000 scale=1.0000 norm=1.7373\n",
      "[iter 100] loss=1.0564 val_loss=0.0000 scale=0.5000 norm=0.8627\n",
      "[iter 0] loss=1.1952 val_loss=0.0000 scale=1.0000 norm=1.7310\n",
      "[iter 100] loss=1.0583 val_loss=0.0000 scale=1.0000 norm=1.7218\n",
      "[iter 0] loss=1.1938 val_loss=0.0000 scale=1.0000 norm=1.7332\n",
      "[iter 100] loss=1.0553 val_loss=0.0000 scale=0.5000 norm=0.8633\n",
      "[iter 0] loss=1.1926 val_loss=0.0000 scale=1.0000 norm=1.7359\n",
      "[iter 100] loss=1.0544 val_loss=0.0000 scale=1.0000 norm=1.7268\n",
      "[iter 0] loss=1.1971 val_loss=0.0000 scale=1.0000 norm=1.7277\n",
      "[iter 100] loss=1.0589 val_loss=0.0000 scale=1.0000 norm=1.7224\n",
      "[iter 0] loss=1.1985 val_loss=0.0000 scale=1.0000 norm=1.7251\n",
      "[iter 100] loss=1.0631 val_loss=0.0000 scale=0.5000 norm=0.8581\n",
      "[iter 0] loss=1.1992 val_loss=0.0000 scale=1.0000 norm=1.7238\n",
      "[iter 100] loss=1.0649 val_loss=0.0000 scale=1.0000 norm=1.7156\n",
      "[iter 0] loss=1.1963 val_loss=0.0000 scale=1.0000 norm=1.7237\n",
      "[iter 100] loss=1.0455 val_loss=0.0000 scale=1.0000 norm=1.7198\n",
      "[iter 0] loss=1.1964 val_loss=0.0000 scale=1.0000 norm=1.7239\n",
      "[iter 100] loss=1.0462 val_loss=0.0000 scale=1.0000 norm=1.7178\n",
      "[iter 0] loss=1.1938 val_loss=0.0000 scale=1.0000 norm=1.7289\n",
      "[iter 100] loss=1.0379 val_loss=0.0000 scale=0.5000 norm=0.8649\n",
      "[iter 0] loss=1.1925 val_loss=0.0000 scale=1.0000 norm=1.7308\n",
      "[iter 100] loss=1.0351 val_loss=0.0000 scale=0.5000 norm=0.8635\n",
      "[iter 0] loss=1.1907 val_loss=0.0000 scale=1.0000 norm=1.7344\n",
      "[iter 100] loss=1.0384 val_loss=0.0000 scale=0.5000 norm=0.8639\n",
      "[iter 0] loss=1.1909 val_loss=0.0000 scale=1.0000 norm=1.7339\n",
      "[iter 100] loss=1.0377 val_loss=0.0000 scale=0.5000 norm=0.8644\n",
      "[iter 0] loss=1.1893 val_loss=0.0000 scale=1.0000 norm=1.7366\n",
      "[iter 100] loss=1.0353 val_loss=0.0000 scale=0.5000 norm=0.8666\n",
      "[iter 0] loss=1.1849 val_loss=0.0000 scale=1.0000 norm=1.7447\n",
      "[iter 100] loss=1.0281 val_loss=0.0000 scale=0.5000 norm=0.8733\n",
      "[iter 0] loss=1.1845 val_loss=0.0000 scale=1.0000 norm=1.7460\n",
      "[iter 100] loss=1.0323 val_loss=0.0000 scale=0.5000 norm=0.8742\n",
      "[iter 0] loss=1.1848 val_loss=0.0000 scale=1.0000 norm=1.7457\n",
      "[iter 100] loss=1.0314 val_loss=0.0000 scale=0.5000 norm=0.8751\n",
      "[iter 0] loss=1.1823 val_loss=0.0000 scale=1.0000 norm=1.7503\n",
      "[iter 100] loss=1.0254 val_loss=0.0000 scale=0.5000 norm=0.8740\n",
      "[iter 0] loss=1.1816 val_loss=0.0000 scale=1.0000 norm=1.7511\n",
      "[iter 100] loss=1.0235 val_loss=0.0000 scale=0.5000 norm=0.8753\n",
      "[iter 0] loss=1.1781 val_loss=0.0000 scale=1.0000 norm=1.7579\n",
      "[iter 100] loss=1.0256 val_loss=0.0000 scale=0.5000 norm=0.8778\n",
      "[iter 0] loss=1.1781 val_loss=0.0000 scale=1.0000 norm=1.7579\n",
      "[iter 100] loss=1.0258 val_loss=0.0000 scale=0.5000 norm=0.8779\n",
      "[iter 0] loss=1.1726 val_loss=0.0000 scale=1.0000 norm=1.7668\n",
      "[iter 100] loss=1.0237 val_loss=0.0000 scale=1.0000 norm=1.7625\n",
      "[iter 0] loss=1.1747 val_loss=0.0000 scale=1.0000 norm=1.7626\n",
      "[iter 100] loss=1.0257 val_loss=0.0000 scale=0.5000 norm=0.8769\n",
      "[iter 0] loss=1.1748 val_loss=0.0000 scale=1.0000 norm=1.7628\n",
      "[iter 100] loss=1.0243 val_loss=0.0000 scale=0.5000 norm=0.8774\n",
      "[iter 0] loss=1.1696 val_loss=0.0000 scale=1.0000 norm=1.7710\n",
      "[iter 100] loss=1.0225 val_loss=0.0000 scale=0.5000 norm=0.8791\n",
      "[iter 0] loss=1.1754 val_loss=0.0000 scale=1.0000 norm=1.7634\n",
      "[iter 100] loss=1.0213 val_loss=0.0000 scale=0.5000 norm=0.8775\n",
      "[iter 0] loss=1.1761 val_loss=0.0000 scale=1.0000 norm=1.7622\n",
      "[iter 100] loss=1.0232 val_loss=0.0000 scale=0.5000 norm=0.8769\n",
      "[iter 0] loss=1.1747 val_loss=0.0000 scale=1.0000 norm=1.7650\n",
      "[iter 100] loss=1.0198 val_loss=0.0000 scale=0.5000 norm=0.8789\n",
      "[iter 0] loss=1.1738 val_loss=0.0000 scale=1.0000 norm=1.7666\n",
      "[iter 100] loss=1.0253 val_loss=0.0000 scale=1.0000 norm=1.7561\n",
      "[iter 0] loss=1.1737 val_loss=0.0000 scale=1.0000 norm=1.7667\n",
      "[iter 100] loss=1.0254 val_loss=0.0000 scale=0.5000 norm=0.8790\n",
      "[iter 0] loss=1.1731 val_loss=0.0000 scale=1.0000 norm=1.7682\n",
      "[iter 100] loss=1.0234 val_loss=0.0000 scale=0.5000 norm=0.8815\n",
      "[iter 0] loss=1.1673 val_loss=0.0000 scale=1.0000 norm=1.7781\n",
      "[iter 100] loss=1.0260 val_loss=0.0000 scale=0.5000 norm=0.8839\n",
      "[iter 0] loss=1.1652 val_loss=0.0000 scale=1.0000 norm=1.7818\n",
      "[iter 100] loss=1.0222 val_loss=0.0000 scale=0.5000 norm=0.8851\n",
      "[iter 0] loss=1.1650 val_loss=0.0000 scale=1.0000 norm=1.7824\n",
      "[iter 100] loss=1.0188 val_loss=0.0000 scale=0.5000 norm=0.8866\n",
      "[iter 0] loss=1.1651 val_loss=0.0000 scale=1.0000 norm=1.7822\n",
      "[iter 100] loss=1.0235 val_loss=0.0000 scale=0.5000 norm=0.8859\n",
      "[iter 0] loss=1.1668 val_loss=0.0000 scale=1.0000 norm=1.7788\n",
      "[iter 100] loss=1.0205 val_loss=0.0000 scale=0.5000 norm=0.8863\n",
      "[iter 0] loss=1.1655 val_loss=0.0000 scale=1.0000 norm=1.7815\n",
      "[iter 100] loss=1.0207 val_loss=0.0000 scale=1.0000 norm=1.7717\n",
      "[iter 0] loss=1.1692 val_loss=0.0000 scale=1.0000 norm=1.7745\n",
      "[iter 100] loss=1.0221 val_loss=0.0000 scale=0.5000 norm=0.8827\n",
      "[iter 0] loss=1.1745 val_loss=0.0000 scale=1.0000 norm=1.7661\n",
      "[iter 100] loss=1.0208 val_loss=0.0000 scale=1.0000 norm=1.7591\n",
      "[iter 0] loss=1.1748 val_loss=0.0000 scale=1.0000 norm=1.7656\n",
      "[iter 100] loss=1.0254 val_loss=0.0000 scale=0.5000 norm=0.8769\n",
      "[iter 0] loss=1.1718 val_loss=0.0000 scale=1.0000 norm=1.7716\n",
      "[iter 100] loss=1.0248 val_loss=0.0000 scale=1.0000 norm=1.7602\n",
      "[iter 0] loss=1.1706 val_loss=0.0000 scale=1.0000 norm=1.7738\n",
      "[iter 100] loss=1.0219 val_loss=0.0000 scale=0.5000 norm=0.8819\n",
      "[iter 0] loss=1.1767 val_loss=0.0000 scale=1.0000 norm=1.7637\n",
      "[iter 100] loss=1.0259 val_loss=0.0000 scale=0.5000 norm=0.8774\n",
      "[iter 0] loss=1.1759 val_loss=0.0000 scale=1.0000 norm=1.7651\n",
      "[iter 100] loss=1.0273 val_loss=0.0000 scale=1.0000 norm=1.7555\n",
      "[iter 0] loss=1.1729 val_loss=0.0000 scale=1.0000 norm=1.7708\n",
      "[iter 100] loss=1.0278 val_loss=0.0000 scale=0.5000 norm=0.8786\n",
      "[iter 0] loss=1.1736 val_loss=0.0000 scale=1.0000 norm=1.7694\n",
      "[iter 100] loss=1.0299 val_loss=0.0000 scale=0.5000 norm=0.8791\n",
      "[iter 0] loss=1.1744 val_loss=0.0000 scale=1.0000 norm=1.7679\n",
      "[iter 100] loss=1.0258 val_loss=0.0000 scale=0.5000 norm=0.8793\n"
     ]
    }
   ],
   "source": [
    "start_test_idx = 252 * 3\n",
    "\n",
    "res_sent = run_expanding_backtest_ngboost(\n",
    "    X,\n",
    "    y,\n",
    "    start_test_idx=start_test_idx,\n",
    "    var_levels=[0.95, 0.99],\n",
    "    es_mc_samples=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "204d6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sent.to_csv(\"../data/processed/sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a7f46e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation rate 95: 0.07827788649706457\n",
      "Violation rate 99: 0.012393998695368558\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(res_sent[\"y_real\"], label = \"Return\")\n",
    "plt.plot(res_sent[\"VaR_95\"], label = \"VaR_95\")\n",
    "\n",
    "res_sent[\"violation_95\"] = (res_sent[\"y_real\"] < res_sent[\"VaR_95\"]).astype(int)\n",
    "res_sent[\"violation_99\"] = (res_sent[\"y_real\"] < res_sent[\"VaR_99\"]).astype(int)\n",
    "\n",
    "violation_rate_95 = res_sent[\"violation_95\"].mean()\n",
    "violation_rate_99 = res_sent[\"violation_99\"].mean()\n",
    "\n",
    "print(\"Violation rate 95:\", violation_rate_95)\n",
    "print(\"Violation rate 99:\", violation_rate_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84d7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
