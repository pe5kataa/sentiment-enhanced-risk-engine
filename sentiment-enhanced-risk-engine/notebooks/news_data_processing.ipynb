{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "088bf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f303a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03fde13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 - Pulled 100 articles...\n",
      "2021 - Pulled 100 articles...\n",
      "2021 - Pulled 200 articles...\n",
      "2021 - Pulled 300 articles...\n",
      "2021 - Pulled 400 articles...\n",
      "2021 - Pulled 500 articles...\n",
      "2021 - Pulled 600 articles...\n",
      "2021 - Pulled 700 articles...\n",
      "2022 - Pulled 100 articles...\n",
      "2022 - Pulled 200 articles...\n",
      "2022 - Pulled 300 articles...\n",
      "2022 - Pulled 400 articles...\n",
      "2022 - Pulled 500 articles...\n",
      "2022 - Pulled 600 articles...\n",
      "2022 - Pulled 700 articles...\n",
      "2022 - Pulled 800 articles...\n",
      "2022 - Pulled 900 articles...\n",
      "2022 - Pulled 1000 articles...\n",
      "2023 - Pulled 100 articles...\n",
      "2024 - Pulled 100 articles...\n",
      "2024 - Pulled 200 articles...\n",
      "2025 - Pulled 100 articles...\n",
      "2025 - Pulled 200 articles...\n",
      "2025 - Pulled 300 articles...\n",
      "2025 - Pulled 400 articles...\n",
      "2025 - Pulled 500 articles...\n",
      "2025 - Pulled 600 articles...\n",
      "2025 - Pulled 700 articles...\n",
      "2025 - Pulled 800 articles...\n",
      "2025 - Pulled 900 articles...\n",
      "2025 - Pulled 1000 articles...\n",
      "2025 - Pulled 1100 articles...\n",
      "2025 - Pulled 1200 articles...\n",
      "2025 - Pulled 1300 articles...\n",
      "2025 - Pulled 1400 articles...\n",
      "2025 - Pulled 1500 articles...\n",
      "2025 - Pulled 1600 articles...\n",
      "2025 - Pulled 1700 articles...\n",
      "2025 - Pulled 1800 articles...\n",
      "2025 - Pulled 1900 articles...\n",
      "2025 - Pulled 2000 articles...\n",
      "2025 - Pulled 2100 articles...\n",
      "2025 - Pulled 2200 articles...\n",
      "2025 - Pulled 2300 articles...\n",
      "2025 - Pulled 2400 articles...\n",
      "2025 - Pulled 2500 articles...\n",
      "2025 - Pulled 2600 articles...\n",
      "2025 - Pulled 2700 articles...\n",
      "2025 - Pulled 2800 articles...\n",
      "2025 - Pulled 2900 articles...\n",
      "2025 - Pulled 3000 articles...\n",
      "2025 - Pulled 3100 articles...\n",
      "2025 - Pulled 3200 articles...\n",
      "2025 - Pulled 3300 articles...\n",
      "2025 - Pulled 3400 articles...\n",
      "2025 - Pulled 3500 articles...\n",
      "2025 - Pulled 3600 articles...\n",
      "2025 - Pulled 3700 articles...\n",
      "2025 - Pulled 3800 articles...\n",
      "2025 - Pulled 3900 articles...\n",
      "2025 - Pulled 4000 articles...\n",
      "2025 - Pulled 4100 articles...\n",
      "2025 - Pulled 4200 articles...\n",
      "2025 - Pulled 4300 articles...\n",
      "2025 - Pulled 4400 articles...\n",
      "Total articles collected: 6198\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "API_KEY = os.getenv('api_key')\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "\n",
    "years = range(2017, 2026)\n",
    "all_articles = []\n",
    "\n",
    "for year in years:\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"s\": \"SPY.US\",\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        all_articles.extend(data)\n",
    "        offset += 100\n",
    "        \n",
    "        print(f\"{year} - Pulled {offset} articles...\")\n",
    "        time.sleep(0.5)  # throttle safety\n",
    "\n",
    "print(f\"Total articles collected: {len(all_articles)}\")\n",
    "\n",
    "with open(\"SP500_news_2017_2025.json\", \"w\") as f:\n",
    "    json.dump(all_articles, f)\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "# Largest S&P 500 companies by market cap\n",
    "tickers = [\n",
    "    \"AAPL.US\", \"MSFT.US\", \"AMZN.US\", \"GOOGL.US\", \"META.US\",\n",
    "    \"TSLA.US\", \"NVDA.US\", \"JPM.US\", \"JNJ.US\"\n",
    "]\n",
    "\n",
    "years = range(2017, 2026)  # 2017 through 2025\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "all_articles = []\n",
    "article_ids = set()  # for deduplication\n",
    "MAX_ARTICLES_PER_YEAR = 300  # cap per ticker per year\n",
    "\n",
    "# -------------------------\n",
    "# FUNCTION TO PULL NEWS\n",
    "# -------------------------\n",
    "def pull_news_for_ticker_year(ticker, year):\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    year_articles = []\n",
    "\n",
    "    while len(year_articles) < MAX_ARTICLES_PER_YEAR:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"s\": ticker,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except requests.JSONDecodeError:\n",
    "            print(f\"Warning: JSON decode error for {ticker} {year} offset {offset}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "            continue  # Retry this request\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        for article in data:\n",
    "            unique_id = article.get(\"link\") or article.get(\"title\")\n",
    "            if unique_id and unique_id not in article_ids:\n",
    "                article_ids.add(unique_id)\n",
    "                year_articles.append(article)\n",
    "                if len(year_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "                    break\n",
    "\n",
    "        offset += len(data)\n",
    "        print(f\"{ticker} {year} - collected {len(year_articles)} articles so far...\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return year_articles\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN LOOP\n",
    "# -------------------------\n",
    "for ticker in tickers:\n",
    "    print(f\"Starting ticker: {ticker}\")\n",
    "    for year in years:\n",
    "        articles = pull_news_for_ticker_year(ticker, year)\n",
    "        all_articles.extend(articles)\n",
    "    print(f\"Finished ticker: {ticker}, total articles collected: {len(all_articles)}\\n\")\n",
    "\n",
    "print(f\"TOTAL articles collected for all tickers: {len(all_articles)}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE JSON\n",
    "# -------------------------\n",
    "json_filename = \"Stocks_biggest_news_2017_2025_capped.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"JSON saved as {json_filename}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE CSV\n",
    "# -------------------------\n",
    "csv_filename = \"Stocks_biggest_news_2017_2025_capped.csv\"\n",
    "\n",
    "df = pd.read_csv(json_filename)\n",
    "df.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56fbf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_news = pd.read_csv(\"../data/SP500_news_2017_2025.csv\")\n",
    "Stocks_news = pd.read_csv(\"../data/Stocks_biggest_news_2017_2025.csv\")\n",
    "out_path = \"../data/news_2017-2025.csv\"\n",
    "\n",
    "news =  pd.concat([SP500_news, Stocks_news], ignore_index=True)\n",
    "\n",
    "\n",
    "news = news.sort_values(by=\"date\", ascending=True)\n",
    "news.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07671995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
