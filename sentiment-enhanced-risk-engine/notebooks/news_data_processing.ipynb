{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fde13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "API_KEY = \"6993505f971534.72691327\"\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "\n",
    "years = range(2017, 2026)\n",
    "all_articles = []\n",
    "\n",
    "for year in years:\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"t\": \"DAX\",\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        all_articles.extend(data)\n",
    "        offset += 100\n",
    "        \n",
    "        print(f\"{year} - Pulled {offset} articles...\")\n",
    "        time.sleep(0.5)  # throttle safety\n",
    "\n",
    "print(f\"Total articles collected: {len(all_articles)}\")\n",
    "\n",
    "with open(\"SP500_news_2017_2025.json\", \"w\") as f:\n",
    "    json.dump(all_articles, f)\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f984c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "API_KEY = \"6993505f971534.72691327\"\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "\n",
    "years = range(2017, 2026)\n",
    "all_articles = []\n",
    "\n",
    "for year in years:\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"t\": \"DAX\",\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        all_articles.extend(data)\n",
    "        offset += 100\n",
    "        \n",
    "        print(f\"{year} - Pulled {offset} articles...\")\n",
    "        time.sleep(0.5)  # throttle safety\n",
    "\n",
    "print(f\"Total articles collected: {len(all_articles)}\")\n",
    "\n",
    "with open(\"SP500_news_2017_2025.json\", \"w\") as f:\n",
    "    json.dump(all_articles, f)\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceef3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ticker: SAP\n",
      "SAP 2017 - collected 10 articles so far...\n",
      "SAP 2018 - collected 15 articles so far...\n",
      "SAP 2019 - collected 7 articles so far...\n",
      "SAP 2020 - collected 19 articles so far...\n",
      "SAP 2021 - collected 100 articles so far...\n",
      "SAP 2021 - collected 200 articles so far...\n",
      "SAP 2021 - collected 264 articles so far...\n",
      "SAP 2022 - collected 100 articles so far...\n",
      "SAP 2022 - collected 200 articles so far...\n",
      "SAP 2022 - collected 257 articles so far...\n",
      "SAP 2023 - collected 100 articles so far...\n",
      "SAP 2023 - collected 200 articles so far...\n",
      "SAP 2023 - collected 300 articles so far...\n",
      "SAP 2023 - collected 320 articles so far...\n",
      "SAP 2024 - collected 100 articles so far...\n",
      "SAP 2024 - collected 200 articles so far...\n",
      "SAP 2024 - collected 276 articles so far...\n",
      "SAP 2025 - collected 100 articles so far...\n",
      "SAP 2025 - collected 200 articles so far...\n",
      "SAP 2025 - collected 300 articles so far...\n",
      "SAP 2025 - collected 400 articles so far...\n",
      "SAP 2025 - collected 500 articles so far...\n",
      "Finished ticker: SAP, total articles collected: 1668\n",
      "\n",
      "Starting ticker: SIE\n",
      "SIE 2021 - collected 2 articles so far...\n",
      "SIE 2023 - collected 1 articles so far...\n",
      "SIE 2025 - collected 44 articles so far...\n",
      "Finished ticker: SIE, total articles collected: 1715\n",
      "\n",
      "Starting ticker: ALV\n",
      "ALV 2017 - collected 2 articles so far...\n",
      "ALV 2018 - collected 2 articles so far...\n",
      "ALV 2019 - collected 2 articles so far...\n",
      "ALV 2020 - collected 7 articles so far...\n",
      "ALV 2021 - collected 99 articles so far...\n",
      "ALV 2021 - collected 121 articles so far...\n",
      "ALV 2022 - collected 98 articles so far...\n",
      "ALV 2023 - collected 100 articles so far...\n",
      "ALV 2023 - collected 143 articles so far...\n",
      "ALV 2024 - collected 100 articles so far...\n",
      "ALV 2024 - collected 119 articles so far...\n",
      "ALV 2025 - collected 99 articles so far...\n",
      "ALV 2025 - collected 198 articles so far...\n",
      "ALV 2025 - collected 259 articles so far...\n",
      "Finished ticker: ALV, total articles collected: 2468\n",
      "\n",
      "Starting ticker: AIR\n",
      "AIR 2017 - collected 1 articles so far...\n",
      "AIR 2018 - collected 2 articles so far...\n",
      "AIR 2019 - collected 2 articles so far...\n",
      "AIR 2020 - collected 57 articles so far...\n",
      "AIR 2021 - collected 100 articles so far...\n",
      "AIR 2021 - collected 155 articles so far...\n",
      "AIR 2022 - collected 100 articles so far...\n",
      "AIR 2022 - collected 129 articles so far...\n",
      "AIR 2023 - collected 100 articles so far...\n",
      "AIR 2023 - collected 152 articles so far...\n",
      "AIR 2024 - collected 100 articles so far...\n",
      "AIR 2024 - collected 200 articles so far...\n",
      "AIR 2024 - collected 300 articles so far...\n",
      "AIR 2024 - collected 400 articles so far...\n",
      "AIR 2024 - collected 456 articles so far...\n",
      "AIR 2025 - collected 99 articles so far...\n",
      "AIR 2025 - collected 199 articles so far...\n",
      "AIR 2025 - collected 299 articles so far...\n",
      "AIR 2025 - collected 399 articles so far...\n",
      "AIR 2025 - collected 499 articles so far...\n",
      "Finished ticker: AIR, total articles collected: 3921\n",
      "\n",
      "Starting ticker: BAS\n",
      "BAS 2018 - collected 8 articles so far...\n",
      "BAS 2019 - collected 1 articles so far...\n",
      "BAS 2022 - collected 2 articles so far...\n",
      "BAS 2025 - collected 24 articles so far...\n",
      "Finished ticker: BAS, total articles collected: 3956\n",
      "\n",
      "Starting ticker: BAYN\n",
      "BAYN 2020 - collected 1 articles so far...\n",
      "BAYN 2021 - collected 2 articles so far...\n",
      "BAYN 2022 - collected 6 articles so far...\n",
      "BAYN 2025 - collected 24 articles so far...\n",
      "Finished ticker: BAYN, total articles collected: 3989\n",
      "\n",
      "Starting ticker: MBG\n",
      "MBG 2022 - collected 4 articles so far...\n",
      "MBG 2024 - collected 0 articles so far...\n",
      "MBG 2025 - collected 31 articles so far...\n",
      "Finished ticker: MBG, total articles collected: 4024\n",
      "\n",
      "Starting ticker: BMW\n",
      "BMW 2022 - collected 4 articles so far...\n",
      "BMW 2023 - collected 2 articles so far...\n",
      "BMW 2025 - collected 56 articles so far...\n",
      "Finished ticker: BMW, total articles collected: 4086\n",
      "\n",
      "Starting ticker: DTE\n",
      "DTE 2017 - collected 2 articles so far...\n",
      "DTE 2018 - collected 7 articles so far...\n",
      "DTE 2019 - collected 4 articles so far...\n",
      "DTE 2020 - collected 48 articles so far...\n",
      "DTE 2021 - collected 97 articles so far...\n",
      "DTE 2021 - collected 197 articles so far...\n",
      "DTE 2021 - collected 297 articles so far...\n",
      "DTE 2021 - collected 303 articles so far...\n",
      "DTE 2022 - collected 100 articles so far...\n",
      "DTE 2022 - collected 199 articles so far...\n",
      "DTE 2022 - collected 240 articles so far...\n",
      "DTE 2023 - collected 100 articles so far...\n",
      "DTE 2023 - collected 175 articles so far...\n",
      "DTE 2024 - collected 100 articles so far...\n",
      "DTE 2024 - collected 161 articles so far...\n",
      "DTE 2025 - collected 97 articles so far...\n",
      "DTE 2025 - collected 196 articles so far...\n",
      "DTE 2025 - collected 295 articles so far...\n",
      "DTE 2025 - collected 364 articles so far...\n",
      "Finished ticker: DTE, total articles collected: 5390\n",
      "\n",
      "Starting ticker: DBK\n",
      "DBK 2021 - collected 1 articles so far...\n",
      "DBK 2022 - collected 2 articles so far...\n",
      "DBK 2023 - collected 1 articles so far...\n",
      "DBK 2024 - collected 0 articles so far...\n",
      "DBK 2025 - collected 30 articles so far...\n",
      "Finished ticker: DBK, total articles collected: 5424\n",
      "\n",
      "TOTAL articles collected for all tickers: 5424\n",
      "JSON saved as Stocks_DAX_biggest_news_2017_2025_capped.json\n",
      "CSV saved as Stocks_DAX_biggest_news_2017_2025_capped.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "API_KEY = \"6993505f971534.72691327\"\n",
    "\n",
    "# Largest S&P 500 companies by market cap\n",
    "\n",
    "tickers = [\n",
    "    \"SAP\",\n",
    "    \"SIE\",\n",
    "    \"ALV\",\n",
    "    \"AIR\",\n",
    "    \"BAS\",\n",
    "    \"BAYN\",\n",
    "    \"MBG\",\n",
    "    \"BMW\",\n",
    "    \"DTE\",\n",
    "    \"DBK\"\n",
    "]\n",
    "\n",
    "years = range(2017, 2026)  # 2017 through 2025\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "all_articles = []\n",
    "article_ids = set()  # for deduplication\n",
    "MAX_ARTICLES_PER_YEAR = 500  # cap per ticker per year\n",
    "\n",
    "# -------------------------\n",
    "# FUNCTION TO PULL NEWS\n",
    "# -------------------------\n",
    "def pull_news_for_ticker_year(ticker, year):\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    year_articles = []\n",
    "\n",
    "    while len(year_articles) < MAX_ARTICLES_PER_YEAR:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"s\": ticker,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except requests.JSONDecodeError:\n",
    "            print(f\"Warning: JSON decode error for {ticker} {year} offset {offset}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "            continue  # Retry this request\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        for article in data:\n",
    "            unique_id = article.get(\"link\") or article.get(\"title\")\n",
    "            if unique_id and unique_id not in article_ids:\n",
    "                article_ids.add(unique_id)\n",
    "                year_articles.append(article)\n",
    "                if len(year_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "                    break\n",
    "\n",
    "        offset += len(data)\n",
    "        print(f\"{ticker} {year} - collected {len(year_articles)} articles so far...\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return year_articles\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN LOOP\n",
    "# -------------------------\n",
    "for ticker in tickers:\n",
    "    print(f\"Starting ticker: {ticker}\")\n",
    "    for year in years:\n",
    "        articles = pull_news_for_ticker_year(ticker, year)\n",
    "        all_articles.extend(articles)\n",
    "    print(f\"Finished ticker: {ticker}, total articles collected: {len(all_articles)}\\n\")\n",
    "\n",
    "print(f\"TOTAL articles collected for all tickers: {len(all_articles)}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE JSON\n",
    "# -------------------------\n",
    "json_filename = \"Stocks_DAX_biggest_news_2017_2025_capped.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"JSON saved as {json_filename}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE CSV\n",
    "# -------------------------\n",
    "csv_filename = \"Stocks_DAX_biggest_news_2017_2025_capped.csv\"\n",
    "\n",
    "if all_articles:\n",
    "    keys = all_articles[0].keys()\n",
    "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "    print(f\"CSV saved as {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a78d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{\"date\":\"2023-12-29T15:11:42+00:00\",\"title\":\"The Next Big Thing: 3 Tech Stocks Ready for a 500% Leap by 2027\",\"content\":\"In an era where technological advancements wield unprecedented influence, the hunt for the following game-changing tech stocks intensifies. Imagine a landscape where three tech juggernauts illuminate the path to a staggering 5X surge by 2027. Operating at the forefront of semiconductor materials, systems, and application software, these companies aren\\u2019t just names on a s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "API_KEY = \"6993505f971534.72691327\"\n",
    "params = {\n",
    "    \"api_token\": API_KEY,\n",
    "    \"fmt\": \"json\",\n",
    "    \"s\": \"SAP\",\n",
    "    \"from\": \"2023-01-01\",\n",
    "    \"to\": \"2023-12-31\",\n",
    "    \"limit\": 100\n",
    "}\n",
    "response = requests.get(\"https://eodhd.com/api/news\", params=params)\n",
    "print(response.status_code)\n",
    "print(response.text[:500])  # see first 500 chars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "API_KEY = \"6993505f971534.72691327\"\n",
    "\n",
    "# Largest S&P 500 companies by market cap\n",
    "tickers = [\n",
    "    \"AAPL.US\", \"MSFT.US\", \"AMZN.US\", \"GOOGL.US\", \"META.US\",\n",
    "    \"TSLA.US\", \"NVDA.US\", \"JPM.US\", \"JNJ.US\"\n",
    "]\n",
    "\n",
    "years = range(2017, 2026)  # 2017 through 2025\n",
    "BASE_URL = \"https://eodhd.com/api/news\"\n",
    "all_articles = []\n",
    "article_ids = set()  # for deduplication\n",
    "MAX_ARTICLES_PER_YEAR = 300  # cap per ticker per year\n",
    "\n",
    "# -------------------------\n",
    "# FUNCTION TO PULL NEWS\n",
    "# -------------------------\n",
    "def pull_news_for_ticker_year(ticker, year):\n",
    "    from_date = f\"{year}-01-01\"\n",
    "    to_date = f\"{year}-12-31\"\n",
    "    offset = 0\n",
    "    year_articles = []\n",
    "\n",
    "    while len(year_articles) < MAX_ARTICLES_PER_YEAR:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"fmt\": \"json\",\n",
    "            \"s\": ticker,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": 100,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except requests.JSONDecodeError:\n",
    "            print(f\"Warning: JSON decode error for {ticker} {year} offset {offset}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "            continue  # Retry this request\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        for article in data:\n",
    "            unique_id = article.get(\"link\") or article.get(\"title\")\n",
    "            if unique_id and unique_id not in article_ids:\n",
    "                article_ids.add(unique_id)\n",
    "                year_articles.append(article)\n",
    "                if len(year_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "                    break\n",
    "\n",
    "        offset += len(data)\n",
    "        print(f\"{ticker} {year} - collected {len(year_articles)} articles so far...\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return year_articles\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN LOOP\n",
    "# -------------------------\n",
    "for ticker in tickers:\n",
    "    print(f\"Starting ticker: {ticker}\")\n",
    "    for year in years:\n",
    "        articles = pull_news_for_ticker_year(ticker, year)\n",
    "        all_articles.extend(articles)\n",
    "    print(f\"Finished ticker: {ticker}, total articles collected: {len(all_articles)}\\n\")\n",
    "\n",
    "print(f\"TOTAL articles collected for all tickers: {len(all_articles)}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE JSON\n",
    "# -------------------------\n",
    "json_filename = \"Stocks_biggest_news_2017_2025_capped.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"JSON saved as {json_filename}\")\n",
    "\n",
    "# -------------------------\n",
    "# SAVE CSV\n",
    "# -------------------------\n",
    "csv_filename = \"Stocks_biggest_news_2017_2025_capped.csv\"\n",
    "\n",
    "if all_articles:\n",
    "    keys = all_articles[0].keys()\n",
    "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "    print(f\"CSV saved as {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_news = pd.read_csv(\"../data/SP500_news_2017_2025\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
