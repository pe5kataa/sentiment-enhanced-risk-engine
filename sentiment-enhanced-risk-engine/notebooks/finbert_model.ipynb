{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved as SP500_news_2017_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8c0c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows to score: 21159 | date range: 2017-01-03 -> 2025-12-31\n",
      "Columns: ['date', 'title', 'content', 'link', 'symbols', 'tags', 'sentiment']\n",
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e1cf17e25644b9a57fa67d14d1737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: ProsusAI/finbert\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf3e273066842798081c090db11674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FinBERT scoring (mps):   0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scored articles -> finbert_scored_news_2017_2025.csv (rows=21159)\n",
      "After per-day cap: 12063 rows | avg/day: 6.788407428249859\n",
      "Saved daily sentiment -> daily_sentiment_news_2017_2025.csv (days=1777)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sent_mean</th>\n",
       "      <th>sent_median</th>\n",
       "      <th>sent_std</th>\n",
       "      <th>news_count</th>\n",
       "      <th>frac_neg</th>\n",
       "      <th>frac_pos</th>\n",
       "      <th>frac_neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>0.457251</td>\n",
       "      <td>0.167108</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-19</td>\n",
       "      <td>0.291051</td>\n",
       "      <td>0.291051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sent_mean  sent_median  sent_std  news_count  frac_neg  \\\n",
       "0  2017-01-03   0.042987     0.042987  0.000000           1       0.0   \n",
       "1  2017-01-04   0.289485     0.289485  0.000000           1       0.0   \n",
       "2  2017-01-05   0.394566     0.457251  0.167108           6       0.0   \n",
       "3  2017-01-18   0.052208     0.052208  0.000000           1       0.0   \n",
       "4  2017-01-19   0.291051     0.291051  0.000000           1       0.0   \n",
       "\n",
       "   frac_pos  frac_neu  \n",
       "0  1.000000  0.000000  \n",
       "1  0.000000  1.000000  \n",
       "2  0.333333  0.666667  \n",
       "3  0.000000  1.000000  \n",
       "4  0.000000  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- FinBERT scoring \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "IN_CSV     = \"../data/news_2017-2025.csv\"          # <-- your file\n",
    "OUT_SCORED = \"finbert_scored_news_2017_2025.csv\"   # article-level outputs (CSV)\n",
    "OUT_DAILY  = \"daily_sentiment_news_2017_2025.csv\"  # daily aggregated outputs (CSV)\n",
    "\n",
    "TEXT_COL = \"content\"   # adjust if needed\n",
    "DATE_COL = \"date\"      # adjust if needed\n",
    "\n",
    "MAX_ARTICLES_PER_DAY = 20\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 256\n",
    "\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected '{TEXT_COL}' column. Found: {df.columns.tolist()}\")\n",
    "if DATE_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected '{DATE_COL}' column. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# Parse datetime -> daily key (kept as YYYY-MM-DD string for clean CSV)\n",
    "dt = pd.to_datetime(df[DATE_COL], errors=\"coerce\", utc=True)\n",
    "df[\"date\"] = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df = df.dropna(subset=[\"date\", TEXT_COL]).reset_index(drop=True)\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "print(\"Rows to score:\", len(df), \"| date range:\", df[\"date\"].min(), \"->\", df[\"date\"].max())\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# -----------------------\n",
    "# Device\n",
    "# -----------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -----------------------\n",
    "# Load FinBERT\n",
    "# -----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Robust label mapping\n",
    "id2label = model.config.id2label\n",
    "label2id = {v.lower(): k for k, v in id2label.items()}\n",
    "\n",
    "def get_label_id(name: str) -> int:\n",
    "    name = name.lower()\n",
    "    if name not in label2id:\n",
    "        raise ValueError(f\"Label '{name}' not found. Available: {id2label}\")\n",
    "    return label2id[name]\n",
    "\n",
    "pos_id = get_label_id(\"positive\")\n",
    "neg_id = get_label_id(\"negative\")\n",
    "neu_id = get_label_id(\"neutral\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_batch(texts):\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKENS,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    logits = model(**enc).logits\n",
    "    probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "    p_pos = probs[:, pos_id]\n",
    "    p_neg = probs[:, neg_id]\n",
    "    p_neu = probs[:, neu_id]\n",
    "    score = p_pos - p_neg\n",
    "    return p_pos, p_neg, p_neu, score\n",
    "\n",
    "# -----------------------\n",
    "# Score all articles\n",
    "# -----------------------\n",
    "p_pos_all, p_neg_all, p_neu_all, score_all = [], [], [], []\n",
    "\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=f\"FinBERT scoring ({device})\"):\n",
    "    batch_texts = df[TEXT_COL].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    ppos, pneg, pneu, sc = finbert_batch(batch_texts)\n",
    "    p_pos_all.append(ppos)\n",
    "    p_neg_all.append(pneg)\n",
    "    p_neu_all.append(pneu)\n",
    "    score_all.append(sc)\n",
    "\n",
    "df[\"p_pos\"] = np.concatenate(p_pos_all)\n",
    "df[\"p_neg\"] = np.concatenate(p_neg_all)\n",
    "df[\"p_neu\"] = np.concatenate(p_neu_all)\n",
    "df[\"sent_score\"] = np.concatenate(score_all)\n",
    "\n",
    "# Discrete label from max prob\n",
    "prob_mat = np.vstack([df[\"p_pos\"].values, df[\"p_neu\"].values, df[\"p_neg\"].values]).T\n",
    "label_idx = prob_mat.argmax(axis=1)\n",
    "df[\"sent_label\"] = np.where(label_idx == 0, \"positive\",\n",
    "                     np.where(label_idx == 1, \"neutral\", \"negative\"))\n",
    "\n",
    "# Save article-level scores (CSV)\n",
    "df.to_csv(OUT_SCORED, index=False)\n",
    "print(f\"Saved scored articles -> {OUT_SCORED} (rows={len(df)})\")\n",
    "\n",
    "# -----------------------\n",
    "# Keep max N articles/day (strongest signal by |sent_score|)\n",
    "# -----------------------\n",
    "df_day = (df.assign(abs_score=np.abs(df[\"sent_score\"]))\n",
    "            .sort_values([\"date\", \"abs_score\"], ascending=[True, False])\n",
    "            .groupby(\"date\", as_index=False)\n",
    "            .head(MAX_ARTICLES_PER_DAY)\n",
    "            .drop(columns=[\"abs_score\"])\n",
    "         )\n",
    "\n",
    "print(\"After per-day cap:\", len(df_day),\n",
    "      \"rows | avg/day:\", len(df_day) / df_day[\"date\"].nunique())\n",
    "\n",
    "# -----------------------\n",
    "# Daily aggregation\n",
    "# -----------------------\n",
    "daily = (df_day.groupby(\"date\")\n",
    "         .agg(\n",
    "             sent_mean=(\"sent_score\", \"mean\"),\n",
    "             sent_median=(\"sent_score\", \"median\"),\n",
    "             sent_std=(\"sent_score\", \"std\"),\n",
    "             news_count=(\"sent_score\", \"size\"),\n",
    "             frac_neg=(\"sent_label\", lambda s: float(np.mean(s == \"negative\"))),\n",
    "             frac_pos=(\"sent_label\", lambda s: float(np.mean(s == \"positive\"))),\n",
    "             frac_neu=(\"sent_label\", lambda s: float(np.mean(s == \"neutral\"))),\n",
    "         )\n",
    "         .reset_index()\n",
    "        )\n",
    "\n",
    "# Optional: fill std NaNs (happens when news_count==1)\n",
    "daily[\"sent_std\"] = daily[\"sent_std\"].fillna(0.0)\n",
    "\n",
    "# Save daily aggregated (CSV)\n",
    "daily.to_csv(OUT_DAILY, index=False)\n",
    "print(f\"Saved daily sentiment -> {OUT_DAILY} (days={len(daily)})\")\n",
    "\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bdd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a45f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
