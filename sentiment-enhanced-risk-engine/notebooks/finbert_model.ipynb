{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Sentiment Pipeline (Updated)\n",
    "\n",
    "This notebook fixes the earlier critical issues and marks every major change inline with `# CHANGED (Issue X)` comments.\n",
    "\n",
    "## Fixed Issues\n",
    "- Issue 1: NY market-day alignment (instead of UTC date grouping)\n",
    "- Issue 2: Long-text truncation reduced via 512-token + multi-chunk scoring\n",
    "- Issue 3: Removed extreme-score daily capping bias\n",
    "- Issue 4: Added validation diagnostics\n",
    "- Issue 5: Reproducibility improvements (seed, stable outputs, metadata)\n",
    "- Issue 6: Duplicate/syndicated news deduplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b67c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FinBERT scoring + robust daily sentiment features (S&P 500 news)\n",
    "# CHANGED (Issue 1): Align article timestamps to NY market day instead of raw UTC date.\n",
    "# CHANGED (Issue 2): Reduce truncation error with 512-token window + multi-chunk scoring.\n",
    "# CHANGED (Issue 3): Remove extreme-score daily capping bias (use all articles by default).\n",
    "# CHANGED (Issue 4): Add validation diagnostics (provider-label agreement + optional returns check).\n",
    "# CHANGED (Issue 5): Improve reproducibility with deterministic seed, fixed output dirs, run metadata.\n",
    "# CHANGED (Issue 6): Deduplicate repeated/syndicated news to avoid overweighting duplicate stories.\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "# CHANGED (Issue 5): Use explicit project root to avoid path ambiguity.\n",
    "PROJECT_ROOT = Path('/Users/petarnikodimov/Documents/diploma/sentiment-enhanced-risk-engine').resolve()\n",
    "DATA_DIR = (PROJECT_ROOT / 'data').resolve()\n",
    "PROCESSED_DIR = (DATA_DIR / 'processed').resolve()\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IN_CSV = DATA_DIR / 'news_2017-2025.csv'\n",
    "OUT_SCORED = PROCESSED_DIR / 'finbert_scored_news_2017_2025.csv'\n",
    "OUT_DAILY = PROCESSED_DIR / 'daily_sentiment_news_2017_2025.csv'\n",
    "OUT_RUN_META = PROCESSED_DIR / 'finbert_run_metadata.json'\n",
    "\n",
    "TEXT_COL = 'content'\n",
    "TITLE_COL = 'title'\n",
    "DATE_COL = 'date'\n",
    "LINK_COL = 'link'\n",
    "PROVIDER_LABEL_COL = 'sentiment'\n",
    "\n",
    "# CHANGED (Issue 1): S&P 500 market-day alignment timezone.\n",
    "MARKET_TZ = 'America/New_York'\n",
    "\n",
    "# CHANGED (Issue 2): Increase context and add chunking for long articles.\n",
    "MAX_TOKENS = 512\n",
    "MAX_CHUNKS_PER_ARTICLE = 3\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "# CHANGED (Issue 3): Keep all articles by default to avoid selection bias.\n",
    "USE_ALL_ARTICLES_PER_DAY = True\n",
    "MAX_ARTICLES_PER_DAY = None\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MODEL_NAME = 'ProsusAI/finbert'\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "if not IN_CSV.exists():\n",
    "    raise FileNotFoundError(f'Input CSV not found: {IN_CSV}')\n",
    "\n",
    "raw_df = pd.read_csv(IN_CSV)\n",
    "for col in [TEXT_COL, DATE_COL]:\n",
    "    if col not in raw_df.columns:\n",
    "        raise ValueError(f\"Expected '{col}' column. Found: {raw_df.columns.tolist()}\")\n",
    "\n",
    "raw_rows = len(raw_df)\n",
    "\n",
    "# -----------------------\n",
    "# Preprocess + deduplicate\n",
    "# -----------------------\n",
    "df = raw_df.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "if TITLE_COL in df.columns:\n",
    "    df[TITLE_COL] = df[TITLE_COL].fillna('').astype(str)\n",
    "\n",
    "# CHANGED (Issue 1): Parse timestamps as UTC and convert to NY market date.\n",
    "dt_utc = pd.to_datetime(df[DATE_COL], errors='coerce', utc=True)\n",
    "df['timestamp_utc'] = dt_utc\n",
    "df['timestamp_ny'] = dt_utc.dt.tz_convert(MARKET_TZ)\n",
    "df['market_date'] = df['timestamp_ny'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df = df.dropna(subset=['market_date', TEXT_COL]).copy()\n",
    "df = df.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "# CHANGED (Issue 6): Deduplicate by normalized link first (when available).\n",
    "dropped_by_link = 0\n",
    "if LINK_COL in df.columns:\n",
    "    df['_link_norm'] = df[LINK_COL].fillna('').astype(str).str.strip().str.lower()\n",
    "    before = len(df)\n",
    "    keep_mask = (df['_link_norm'] == '') | (~df['_link_norm'].duplicated(keep='first'))\n",
    "    df = df.loc[keep_mask].copy()\n",
    "    dropped_by_link = before - len(df)\n",
    "\n",
    "# CHANGED (Issue 6): Secondary near-duplicate removal on (market_date, title, content).\n",
    "dropped_by_text = 0\n",
    "dedup_cols = ['market_date', TEXT_COL]\n",
    "if TITLE_COL in df.columns:\n",
    "    dedup_cols.insert(1, TITLE_COL)\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=dedup_cols, keep='first').copy()\n",
    "dropped_by_text = before - len(df)\n",
    "\n",
    "# Build model text using title + content.\n",
    "if TITLE_COL in df.columns:\n",
    "    df['model_text'] = (df[TITLE_COL].str.strip() + '\\\\n\\\\n' + df[TEXT_COL].str.strip()).str.strip()\n",
    "else:\n",
    "    df['model_text'] = df[TEXT_COL].str.strip()\n",
    "\n",
    "df = df[df['model_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print('Input file:', IN_CSV)\n",
    "print('Rows loaded:', raw_rows)\n",
    "print('Rows after preprocessing:', len(df))\n",
    "print('Dropped duplicate links:', dropped_by_link)\n",
    "print('Dropped duplicate text/title/date rows:', dropped_by_text)\n",
    "print('Market date range:', df['market_date'].min(), '->', df['market_date'].max())\n",
    "print('Output scored file:', OUT_SCORED)\n",
    "print('Output daily file:', OUT_DAILY)\n",
    "\n",
    "# -----------------------\n",
    "# Device\n",
    "# -----------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# -----------------------\n",
    "# Load FinBERT\n",
    "# -----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "id2label = {int(k): v.lower() for k, v in model.config.id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "for needed in ['positive', 'negative', 'neutral']:\n",
    "    if needed not in label2id:\n",
    "        raise ValueError(f\"Missing label '{needed}' in model labels: {id2label}\")\n",
    "\n",
    "pos_id = label2id['positive']\n",
    "neg_id = label2id['negative']\n",
    "neu_id = label2id['neutral']\n",
    "\n",
    "\n",
    "def choose_chunk_windows(token_count, max_body_tokens, max_chunks):\n",
    "    \"\"\"Select up to max_chunks windows spread across long articles.\"\"\"\n",
    "    if token_count <= max_body_tokens:\n",
    "        return [(0, token_count)]\n",
    "    if max_chunks <= 1:\n",
    "        return [(0, max_body_tokens)]\n",
    "\n",
    "    starts = np.linspace(0, token_count - max_body_tokens, num=max_chunks, dtype=int).tolist()\n",
    "    starts = sorted(dict.fromkeys(starts))\n",
    "    return [(s, s + max_body_tokens) for s in starts]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_batch_chunked(texts):\n",
    "    \"\"\"\n",
    "    CHANGED (Issue 2): For long texts, score multiple windows and average probabilities.\n",
    "    This keeps runtime manageable while reducing single-window truncation bias.\n",
    "\n",
    "    CHANGED (Runtime fix): Avoid tokenizer.prepare_for_model for compatibility with older\n",
    "    tokenizer classes that do not expose this method.\n",
    "    \"\"\"\n",
    "    max_body_tokens = MAX_TOKENS - 2  # reserve [CLS]/[SEP]\n",
    "\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    if cls_id is None or sep_id is None:\n",
    "        raise ValueError('Tokenizer must define cls_token_id and sep_token_id')\n",
    "\n",
    "    flat_input_ids = []\n",
    "    flat_attention_masks = []\n",
    "    article_index = []\n",
    "    token_lengths = []\n",
    "    chunked_articles = 0\n",
    "\n",
    "    for article_i, text in enumerate(texts):\n",
    "        token_ids = tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "        token_lengths.append(len(token_ids))\n",
    "\n",
    "        windows = choose_chunk_windows(len(token_ids), max_body_tokens, MAX_CHUNKS_PER_ARTICLE)\n",
    "        if len(windows) > 1:\n",
    "            chunked_articles += 1\n",
    "\n",
    "        for start, end in windows:\n",
    "            chunk_ids = token_ids[start:end]\n",
    "            input_ids = [cls_id] + chunk_ids + [sep_id]\n",
    "            attn_mask = [1] * len(input_ids)\n",
    "\n",
    "            flat_input_ids.append(input_ids)\n",
    "            flat_attention_masks.append(attn_mask)\n",
    "            article_index.append(article_i)\n",
    "\n",
    "    max_len = max(len(x) for x in flat_input_ids)\n",
    "    input_tensor = torch.full((len(flat_input_ids), max_len), pad_id, dtype=torch.long)\n",
    "    attention_tensor = torch.zeros((len(flat_input_ids), max_len), dtype=torch.long)\n",
    "\n",
    "    for row_i, (ids, attn) in enumerate(zip(flat_input_ids, flat_attention_masks)):\n",
    "        seq_len = len(ids)\n",
    "        input_tensor[row_i, :seq_len] = torch.tensor(ids, dtype=torch.long)\n",
    "        attention_tensor[row_i, :seq_len] = torch.tensor(attn, dtype=torch.long)\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=input_tensor.to(device),\n",
    "        attention_mask=attention_tensor.to(device),\n",
    "    ).logits\n",
    "    probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "    p_pos = np.zeros(len(texts), dtype=np.float32)\n",
    "    p_neg = np.zeros(len(texts), dtype=np.float32)\n",
    "    p_neu = np.zeros(len(texts), dtype=np.float32)\n",
    "    counts = np.zeros(len(texts), dtype=np.float32)\n",
    "\n",
    "    for row_i, article_i in enumerate(article_index):\n",
    "        p_pos[article_i] += probs[row_i, pos_id]\n",
    "        p_neg[article_i] += probs[row_i, neg_id]\n",
    "        p_neu[article_i] += probs[row_i, neu_id]\n",
    "        counts[article_i] += 1.0\n",
    "\n",
    "    counts[counts == 0.0] = 1.0\n",
    "    p_pos /= counts\n",
    "    p_neg /= counts\n",
    "    p_neu /= counts\n",
    "\n",
    "    sent_score = p_pos - p_neg\n",
    "    return p_pos, p_neg, p_neu, sent_score, np.array(token_lengths), chunked_articles, len(flat_input_ids)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Score all articles\n",
    "# -----------------------\n",
    "p_pos_all, p_neg_all, p_neu_all, score_all = [], [], [], []\n",
    "token_lengths_all = []\n",
    "chunked_articles_total = 0\n",
    "total_chunks = 0\n",
    "\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=f'FinBERT scoring ({device})'):\n",
    "    batch_texts = df['model_text'].iloc[i : i + BATCH_SIZE].tolist()\n",
    "    ppos, pneg, pneu, sc, tok_lens, chunked_n, chunk_rows = finbert_batch_chunked(batch_texts)\n",
    "\n",
    "    p_pos_all.append(ppos)\n",
    "    p_neg_all.append(pneg)\n",
    "    p_neu_all.append(pneu)\n",
    "    score_all.append(sc)\n",
    "\n",
    "    token_lengths_all.extend(tok_lens.tolist())\n",
    "    chunked_articles_total += chunked_n\n",
    "    total_chunks += chunk_rows\n",
    "\n",
    "df['p_pos'] = np.concatenate(p_pos_all)\n",
    "df['p_neg'] = np.concatenate(p_neg_all)\n",
    "df['p_neu'] = np.concatenate(p_neu_all)\n",
    "df['sent_score'] = np.concatenate(score_all)\n",
    "\n",
    "# Robust label assignment from max probability among named classes.\n",
    "df['sent_label'] = np.select(\n",
    "    [\n",
    "        (df['p_pos'] >= df[['p_neg', 'p_neu']].max(axis=1)),\n",
    "        (df['p_neg'] >= df[['p_pos', 'p_neu']].max(axis=1)),\n",
    "    ],\n",
    "    ['positive', 'negative'],\n",
    "    default='neutral',\n",
    ")\n",
    "\n",
    "# Save article-level output\n",
    "article_cols = [\n",
    "    'market_date',\n",
    "    'timestamp_utc',\n",
    "    'timestamp_ny',\n",
    "    TITLE_COL if TITLE_COL in df.columns else None,\n",
    "    TEXT_COL,\n",
    "    LINK_COL if LINK_COL in df.columns else None,\n",
    "    PROVIDER_LABEL_COL if PROVIDER_LABEL_COL in df.columns else None,\n",
    "    'p_pos',\n",
    "    'p_neg',\n",
    "    'p_neu',\n",
    "    'sent_score',\n",
    "    'sent_label',\n",
    "]\n",
    "article_cols = [c for c in article_cols if c is not None and c in df.columns]\n",
    "df[article_cols].to_csv(OUT_SCORED, index=False)\n",
    "print(f'Saved scored articles -> {OUT_SCORED} (rows={len(df)})')\n",
    "\n",
    "# -----------------------\n",
    "# Optional per-day cap (unbiased)\n",
    "# -----------------------\n",
    "# CHANGED (Issue 3): No default cap. If enabled, use random sampling, not extreme |score| selection.\n",
    "if USE_ALL_ARTICLES_PER_DAY or MAX_ARTICLES_PER_DAY is None:\n",
    "    df_day = df.copy()\n",
    "else:\n",
    "    df_day = (\n",
    "        df.groupby('market_date', group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=min(len(g), MAX_ARTICLES_PER_DAY), random_state=RANDOM_SEED))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(\n",
    "    'Rows used in daily aggregation:',\n",
    "    len(df_day),\n",
    "    '| avg/day:',\n",
    "    len(df_day) / df_day['market_date'].nunique(),\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Daily aggregation\n",
    "# -----------------------\n",
    "daily = (\n",
    "    df_day.groupby('market_date')\n",
    "    .agg(\n",
    "        sent_mean=('sent_score', 'mean'),\n",
    "        sent_median=('sent_score', 'median'),\n",
    "        sent_std=('sent_score', 'std'),\n",
    "        news_count=('sent_score', 'size'),\n",
    "        frac_neg=('sent_label', lambda s: float(np.mean(s == 'negative'))),\n",
    "        frac_pos=('sent_label', lambda s: float(np.mean(s == 'positive'))),\n",
    "        frac_neu=('sent_label', lambda s: float(np.mean(s == 'neutral'))),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add sampling uncertainty feature for downstream risk models.\n",
    "daily['sent_std'] = daily['sent_std'].fillna(0.0)\n",
    "daily['sent_se'] = daily['sent_std'] / np.sqrt(daily['news_count'].clip(lower=1))\n",
    "\n",
    "daily.to_csv(OUT_DAILY, index=False)\n",
    "print(f'Saved daily sentiment -> {OUT_DAILY} (days={len(daily)})')\n",
    "\n",
    "# -----------------------\n",
    "# Validation diagnostics\n",
    "# -----------------------\n",
    "# CHANGED (Issue 4): Add weak-label validation against provider sentiment if available.\n",
    "\n",
    "\n",
    "def normalize_provider_label(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if 'pos' in s:\n",
    "        return 'positive'\n",
    "    if 'neg' in s:\n",
    "        return 'negative'\n",
    "    if 'neu' in s:\n",
    "        return 'neutral'\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "if PROVIDER_LABEL_COL in df.columns:\n",
    "    df['provider_label_norm'] = df[PROVIDER_LABEL_COL].apply(normalize_provider_label)\n",
    "    val = df.dropna(subset=['provider_label_norm', 'sent_label']).copy()\n",
    "\n",
    "    if len(val) > 0:\n",
    "        agreement = float((val['provider_label_norm'] == val['sent_label']).mean())\n",
    "        conf = pd.crosstab(val['provider_label_norm'], val['sent_label'], rownames=['provider'], colnames=['finbert'])\n",
    "\n",
    "        print('\\\\nValidation vs provider labels')\n",
    "        print('rows:', len(val), '| agreement:', f'{agreement:.4f}')\n",
    "        print(conf)\n",
    "\n",
    "        provider_num = val['provider_label_norm'].map({'negative': -1, 'neutral': 0, 'positive': 1}).values\n",
    "        corr = np.corrcoef(val['sent_score'].values, provider_num)[0, 1]\n",
    "        print('score-provider ordinal correlation:', round(float(corr), 4))\n",
    "\n",
    "        try:\n",
    "            from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "            f1_macro = f1_score(val['provider_label_norm'], val['sent_label'], average='macro')\n",
    "            print('macro F1:', round(float(f1_macro), 4))\n",
    "            print(classification_report(val['provider_label_norm'], val['sent_label'], digits=4))\n",
    "        except Exception as e:\n",
    "            print('sklearn report skipped:', e)\n",
    "    else:\n",
    "        print('No overlapping rows with normalized provider labels for validation.')\n",
    "else:\n",
    "    print(f\"Provider label column '{PROVIDER_LABEL_COL}' not found; skipping weak-label validation.\")\n",
    "\n",
    "# CHANGED (Issue 4): Optional market-response check if returns file exists.\n",
    "RETURNS_CSV = DATA_DIR / 'sp500_returns_daily.csv'\n",
    "if RETURNS_CSV.exists():\n",
    "    r = pd.read_csv(RETURNS_CSV)\n",
    "    date_col = 'date' if 'date' in r.columns else ('Date' if 'Date' in r.columns else None)\n",
    "\n",
    "    if date_col is not None:\n",
    "        r['market_date'] = pd.to_datetime(r[date_col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        if 'return' in r.columns:\n",
    "            r['ret_t'] = pd.to_numeric(r['return'], errors='coerce')\n",
    "        elif 'ret' in r.columns:\n",
    "            r['ret_t'] = pd.to_numeric(r['ret'], errors='coerce')\n",
    "        elif 'close' in r.columns:\n",
    "            r['ret_t'] = pd.to_numeric(r['close'], errors='coerce').pct_change()\n",
    "        else:\n",
    "            r['ret_t'] = np.nan\n",
    "\n",
    "        r['ret_t_plus_1'] = r['ret_t'].shift(-1)\n",
    "        merged = daily.merge(r[['market_date', 'ret_t_plus_1']], on='market_date', how='inner').dropna()\n",
    "\n",
    "        if len(merged) > 10:\n",
    "            ic = np.corrcoef(merged['sent_mean'], merged['ret_t_plus_1'])[0, 1]\n",
    "            print('\\\\nOptional next-day return validation')\n",
    "            print('rows:', len(merged), '| corr(sent_mean, next_day_return):', round(float(ic), 4))\n",
    "        else:\n",
    "            print('Optional returns validation skipped: not enough merged rows.')\n",
    "    else:\n",
    "        print('Optional returns validation skipped: no date/Date column in returns file.')\n",
    "else:\n",
    "    print(f'Optional returns file not found ({RETURNS_CSV}); skipped market-response check.')\n",
    "\n",
    "# -----------------------\n",
    "# Run metadata for reproducibility\n",
    "# -----------------------\n",
    "# CHANGED (Issue 5): Save run metadata with versions/config/diagnostics.\n",
    "meta = {\n",
    "    'run_utc': datetime.now(timezone.utc).isoformat(),\n",
    "    'model_name': MODEL_NAME,\n",
    "    'device': device,\n",
    "    'market_tz': MARKET_TZ,\n",
    "    'config': {\n",
    "        'max_tokens': MAX_TOKENS,\n",
    "        'max_chunks_per_article': MAX_CHUNKS_PER_ARTICLE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'use_all_articles_per_day': USE_ALL_ARTICLES_PER_DAY,\n",
    "        'max_articles_per_day': MAX_ARTICLES_PER_DAY,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "    },\n",
    "    'library_versions': {\n",
    "        'torch': torch.__version__,\n",
    "        'transformers': __import__('transformers').__version__,\n",
    "        'pandas': pd.__version__,\n",
    "        'numpy': np.__version__,\n",
    "    },\n",
    "    'row_counts': {\n",
    "        'raw_rows': int(raw_rows),\n",
    "        'scored_rows': int(len(df)),\n",
    "        'daily_rows': int(len(daily)),\n",
    "        'dropped_duplicate_links': int(dropped_by_link),\n",
    "        'dropped_duplicate_text_rows': int(dropped_by_text),\n",
    "    },\n",
    "    'token_diagnostics': {\n",
    "        'mean_tokens': float(np.mean(token_lengths_all)) if token_lengths_all else 0.0,\n",
    "        'p50_tokens': float(np.quantile(token_lengths_all, 0.50)) if token_lengths_all else 0.0,\n",
    "        'p90_tokens': float(np.quantile(token_lengths_all, 0.90)) if token_lengths_all else 0.0,\n",
    "        'p99_tokens': float(np.quantile(token_lengths_all, 0.99)) if token_lengths_all else 0.0,\n",
    "        'articles_with_multiple_chunks': int(chunked_articles_total),\n",
    "        'pct_articles_with_multiple_chunks': float(chunked_articles_total / max(len(df), 1)),\n",
    "        'total_chunk_inferences': int(total_chunks),\n",
    "    },\n",
    "    'outputs': {\n",
    "        'scored_csv': str(OUT_SCORED),\n",
    "        'daily_csv': str(OUT_DAILY),\n",
    "        'run_metadata_json': str(OUT_RUN_META),\n",
    "    },\n",
    "}\n",
    "\n",
    "OUT_RUN_META.write_text(json.dumps(meta, indent=2))\n",
    "print(f'Saved run metadata -> {OUT_RUN_META}')\n",
    "\n",
    "daily.head()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
